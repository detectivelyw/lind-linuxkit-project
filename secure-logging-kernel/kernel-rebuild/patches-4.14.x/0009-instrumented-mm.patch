From b2687fdfe18e17850017d6660d4696cc10366d11 Mon Sep 17 00:00:00 2001
From: Yiwen Li <detectivelyw@gmail.com>
Date: Tue, 4 Jun 2019 15:23:11 -0400
Subject: [PATCH 09/11] instrumented mm.

---
 mm/backing-dev.c     |  31 ++++
 mm/compaction.c      |  71 +++++++++
 mm/early_ioremap.c   |  22 +++
 mm/fadvise.c         |  17 +++
 mm/filemap.c         | 250 ++++++++++++++++++++++++++++++++
 mm/gup.c             | 243 +++++++++++++++++++++++++++++++
 mm/huge_memory.c     |   2 +
 mm/hugetlb.c         | 203 ++++++++++++++++++++++++++
 mm/hugetlb_cgroup.c  |   2 +
 mm/internal.h        |   2 +
 mm/interval_tree.c   |   7 +
 mm/khugepaged.c      |   2 +
 mm/ksm.c             |   2 +
 mm/list_lru.c        |  22 +++
 mm/maccess.c         |   3 +
 mm/madvise.c         | 104 ++++++++++++++
 mm/memblock.c        | 133 +++++++++++++++++
 mm/memcontrol.c      |   2 +
 mm/memory.c          | 399 +++++++++++++++++++++++++++++++++++++++++++++++++++
 mm/memory_hotplug.c  |   2 +
 mm/mempool.c         |  29 ++++
 mm/mlock.c           |  67 +++++++++
 mm/mm_init.c         |  22 +++
 mm/mmap.c            | 339 +++++++++++++++++++++++++++++++++++++++++++
 mm/mmzone.c          |   9 ++
 mm/mprotect.c        |  58 ++++++++
 mm/mremap.c          |  84 +++++++++++
 mm/nobootmem.c       |  34 +++++
 mm/oom_kill.c        |  42 ++++++
 mm/page-writeback.c  | 140 ++++++++++++++++++
 mm/page_alloc.c      | 375 +++++++++++++++++++++++++++++++++++++++++++++++
 mm/page_counter.c    |   2 +
 mm/page_ext.c        |   2 +
 mm/page_poison.c     |   2 +
 mm/pagewalk.c        |  44 ++++++
 mm/percpu-internal.h |   2 +
 mm/percpu-vm.c       |   2 +
 mm/percpu.c          | 117 +++++++++++++++
 mm/pgtable-generic.c |   6 +
 mm/readahead.c       |  41 ++++++
 mm/rmap.c            |  98 +++++++++++++
 mm/shmem.c           | 377 ++++++++++++++++++++++++++++++++++++++++++++++++
 mm/slab.c            |   2 +
 mm/slab.h            |   2 +
 mm/slab_common.c     |  80 +++++++++++
 mm/sparse-vmemmap.c  |  35 +++++
 mm/sparse.c          |  28 ++++
 mm/swap.c            |  50 +++++++
 mm/swap_state.c      |   5 +
 mm/swapfile.c        | 138 ++++++++++++++++++
 mm/truncate.c        |  42 ++++++
 mm/usercopy.c        |   2 +
 mm/util.c            |  88 ++++++++++++
 mm/vmacache.c        |  13 ++
 mm/vmalloc.c         | 179 +++++++++++++++++++++++
 mm/vmpressure.c      |   2 +
 mm/vmscan.c          | 100 +++++++++++++
 mm/vmstat.c          |  94 ++++++++++++
 mm/workingset.c      |  17 +++
 59 files changed, 4288 insertions(+)

diff --git a/mm/backing-dev.c b/mm/backing-dev.c
index e19606b..fc19c6c 100644
--- a/mm/backing-dev.c
+++ b/mm/backing-dev.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 
 #include <linux/wait.h>
 #include <linux/backing-dev.h>
@@ -53,6 +55,7 @@ static int bdi_debug_stats_show(struct seq_file *m, void *v)
 
 	nr_dirty = nr_io = nr_more_io = nr_dirty_time = 0;
 	spin_lock(&wb->list_lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry(inode, &wb->b_dirty, i_io_list)
 		nr_dirty++;
 	list_for_each_entry(inode, &wb->b_io, i_io_list)
@@ -103,6 +106,7 @@ static int bdi_debug_stats_show(struct seq_file *m, void *v)
 
 static int bdi_debug_stats_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return single_open(file, bdi_debug_stats_show, inode->i_private);
 }
 
@@ -122,6 +126,7 @@ static void bdi_debug_register(struct backing_dev_info *bdi, const char *name)
 
 static void bdi_debug_unregister(struct backing_dev_info *bdi)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	debugfs_remove(bdi->debug_stats);
 	debugfs_remove(bdi->debug_dir);
 }
@@ -142,6 +147,7 @@ static ssize_t read_ahead_kb_store(struct device *dev,
 				  struct device_attribute *attr,
 				  const char *buf, size_t count)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct backing_dev_info *bdi = dev_get_drvdata(dev);
 	unsigned long read_ahead_kb;
 	ssize_t ret;
@@ -172,6 +178,7 @@ BDI_SHOW(read_ahead_kb, K(bdi->ra_pages))
 static ssize_t min_ratio_store(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct backing_dev_info *bdi = dev_get_drvdata(dev);
 	unsigned int ratio;
 	ssize_t ret;
@@ -191,6 +198,7 @@ BDI_SHOW(min_ratio, bdi->min_ratio)
 static ssize_t max_ratio_store(struct device *dev,
 		struct device_attribute *attr, const char *buf, size_t count)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct backing_dev_info *bdi = dev_get_drvdata(dev);
 	unsigned int ratio;
 	ssize_t ret;
@@ -211,6 +219,7 @@ static ssize_t stable_pages_required_show(struct device *dev,
 					  struct device_attribute *attr,
 					  char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct backing_dev_info *bdi = dev_get_drvdata(dev);
 
 	return snprintf(page, PAGE_SIZE-1, "%d\n",
@@ -231,7 +240,9 @@ static __init int bdi_class_init(void)
 {
 	bdi_class = class_create(THIS_MODULE, "bdi");
 	if (IS_ERR(bdi_class))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PTR_ERR(bdi_class);
+}
 
 	bdi_class->dev_groups = bdi_dev_groups;
 	bdi_debug_init();
@@ -249,7 +260,9 @@ static int __init default_bdi_init(void)
 	bdi_wq = alloc_workqueue("writeback", WQ_MEM_RECLAIM | WQ_FREEZABLE |
 					      WQ_UNBOUND | WQ_SYSFS, 0);
 	if (!bdi_wq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	err = bdi_init(&noop_backing_dev_info);
 
@@ -277,6 +290,7 @@ void wb_wakeup_delayed(struct bdi_writeback *wb)
 
 	timeout = msecs_to_jiffies(dirty_writeback_interval * 10);
 	spin_lock_bh(&wb->work_lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(WB_registered, &wb->state))
 		queue_delayed_work(bdi_wq, &wb->dwork, timeout);
 	spin_unlock_bh(&wb->work_lock);
@@ -295,7 +309,9 @@ static int wb_init(struct bdi_writeback *wb, struct backing_dev_info *bdi,
 	memset(wb, 0, sizeof(*wb));
 
 	if (wb != &bdi->wb)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bdi_get(bdi);
+}
 	wb->bdi = bdi;
 	wb->last_old_flush = jiffies;
 	INIT_LIST_HEAD(&wb->b_dirty);
@@ -317,6 +333,7 @@ static int wb_init(struct bdi_writeback *wb, struct backing_dev_info *bdi,
 
 	wb->congested = wb_congested_get_create(bdi, blkcg_id, gfp);
 	if (!wb->congested) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = -ENOMEM;
 		goto out_put_bdi;
 	}
@@ -331,17 +348,22 @@ static int wb_init(struct bdi_writeback *wb, struct backing_dev_info *bdi,
 			goto out_destroy_stat;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 out_destroy_stat:
 	while (i--)
 		percpu_counter_destroy(&wb->stat[i]);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	fprop_local_destroy_percpu(&wb->completions);
 out_put_cong:
 	wb_congested_put(wb->congested);
 out_put_bdi:
 	if (wb != &bdi->wb)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bdi_put(bdi);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return err;
 }
 
@@ -388,6 +410,7 @@ static void wb_exit(struct bdi_writeback *wb)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON(delayed_work_pending(&wb->dwork));
 
 	for (i = 0; i < NR_WB_STAT_ITEMS; i++)
@@ -791,7 +814,9 @@ static int cgwb_bdi_init(struct backing_dev_info *bdi)
 
 	bdi->wb_congested = kzalloc(sizeof(*bdi->wb_congested), GFP_KERNEL);
 	if (!bdi->wb_congested)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	atomic_set(&bdi->wb_congested->refcnt, 1);
 
@@ -974,17 +999,22 @@ void clear_wb_congested(struct bdi_writeback_congested *congested, int sync)
 
 	bit = sync ? WB_sync_congested : WB_async_congested;
 	if (test_and_clear_bit(bit, &congested->state))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		atomic_dec(&nr_wb_congested[sync]);
+}
 	smp_mb__after_atomic();
 	if (waitqueue_active(wqh))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		wake_up(wqh);
 }
+}
 EXPORT_SYMBOL(clear_wb_congested);
 
 void set_wb_congested(struct bdi_writeback_congested *congested, int sync)
 {
 	enum wb_congested_state bit;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bit = sync ? WB_sync_congested : WB_async_congested;
 	if (!test_and_set_bit(bit, &congested->state))
 		atomic_inc(&nr_wb_congested[sync]);
@@ -1078,6 +1108,7 @@ int pdflush_proc_obsolete(struct ctl_table *table, int write,
 {
 	char kbuf[] = "0\n";
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (*ppos || *lenp < sizeof(kbuf)) {
 		*lenp = 0;
 		return 0;
diff --git a/mm/compaction.c b/mm/compaction.c
index 85395dc..d5dc0d6 100644
--- a/mm/compaction.c
+++ b/mm/compaction.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * linux/mm/compaction.c
@@ -27,11 +29,13 @@
 #ifdef CONFIG_COMPACTION
 static inline void count_compact_event(enum vm_event_item item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	count_vm_event(item);
 }
 
 static inline void count_compact_events(enum vm_event_item item, long delta)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	count_vm_events(item, delta);
 }
 #else
@@ -54,6 +58,7 @@ static unsigned long release_freepages(struct list_head *freelist)
 	struct page *page, *next;
 	unsigned long high_pfn = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry_safe(page, next, freelist, lru) {
 		unsigned long pfn = page_to_pfn(page);
 		list_del(&page->lru);
@@ -71,6 +76,7 @@ static void map_pages(struct list_head *list)
 	struct page *page, *next;
 	LIST_HEAD(tmp_list);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry_safe(page, next, list, lru) {
 		list_del(&page->lru);
 
@@ -98,7 +104,9 @@ int PageMovable(struct page *page)
 
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	if (!__PageMovable(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	mapping = page_mapping(page);
 	if (mapping && mapping->a_ops && mapping->a_ops->isolate_page)
@@ -110,6 +118,7 @@ EXPORT_SYMBOL(PageMovable);
 
 void __SetPageMovable(struct page *page, struct address_space *mapping)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	VM_BUG_ON_PAGE((unsigned long)mapping & PAGE_MAPPING_MOVABLE, page);
 	page->mapping = (void *)((unsigned long)mapping | PAGE_MAPPING_MOVABLE);
@@ -118,6 +127,7 @@ EXPORT_SYMBOL(__SetPageMovable);
 
 void __ClearPageMovable(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	VM_BUG_ON_PAGE(!PageMovable(page), page);
 	/*
@@ -140,6 +150,7 @@ EXPORT_SYMBOL(__ClearPageMovable);
  */
 void defer_compaction(struct zone *zone, int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	zone->compact_considered = 0;
 	zone->compact_defer_shift++;
 
@@ -158,7 +169,9 @@ bool compaction_deferred(struct zone *zone, int order)
 	unsigned long defer_limit = 1UL << zone->compact_defer_shift;
 
 	if (order < zone->compact_order_failed)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	/* Avoid possible overflow */
 	if (++zone->compact_considered > defer_limit)
@@ -180,6 +193,7 @@ bool compaction_deferred(struct zone *zone, int order)
 void compaction_defer_reset(struct zone *zone, int order,
 		bool alloc_success)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (alloc_success) {
 		zone->compact_considered = 0;
 		zone->compact_defer_shift = 0;
@@ -193,6 +207,7 @@ void compaction_defer_reset(struct zone *zone, int order,
 /* Returns true if restarting compaction after many failures */
 bool compaction_restarting(struct zone *zone, int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (order < zone->compact_order_failed)
 		return false;
 
@@ -204,6 +219,7 @@ bool compaction_restarting(struct zone *zone, int order)
 static inline bool isolation_suitable(struct compact_control *cc,
 					struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (cc->ignore_skip_hint)
 		return true;
 
@@ -212,6 +228,7 @@ static inline bool isolation_suitable(struct compact_control *cc,
 
 static void reset_cached_positions(struct zone *zone)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	zone->compact_cached_migrate_pfn[0] = zone->zone_start_pfn;
 	zone->compact_cached_migrate_pfn[1] = zone->zone_start_pfn;
 	zone->compact_cached_free_pfn =
@@ -260,7 +277,9 @@ void reset_isolation_suitable(pg_data_t *pgdat)
 
 		/* Only flush if a full compaction finished recently */
 		if (zone->compact_blockskip_flush)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__reset_isolation_suitable(zone);
+}
 	}
 }
 
@@ -276,7 +295,9 @@ static void update_pageblock_skip(struct compact_control *cc,
 	unsigned long pfn;
 
 	if (cc->ignore_skip_hint)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (!page)
 		return;
@@ -325,6 +346,7 @@ static void update_pageblock_skip(struct compact_control *cc,
 static bool compact_trylock_irqsave(spinlock_t *lock, unsigned long *flags,
 						struct compact_control *cc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (cc->mode == MIGRATE_ASYNC) {
 		if (!spin_trylock_irqsave(lock, *flags)) {
 			cc->contended = true;
@@ -355,6 +377,7 @@ static bool compact_trylock_irqsave(spinlock_t *lock, unsigned long *flags,
 static bool compact_unlock_should_abort(spinlock_t *lock,
 		unsigned long flags, bool *locked, struct compact_control *cc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (*locked) {
 		spin_unlock_irqrestore(lock, flags);
 		*locked = false;
@@ -573,7 +596,9 @@ isolate_freepages_range(struct compact_control *cc,
 	pfn = start_pfn;
 	block_start_pfn = pageblock_start_pfn(pfn);
 	if (block_start_pfn < cc->zone->zone_start_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		block_start_pfn = cc->zone->zone_start_pfn;
+}
 	block_end_pfn = pageblock_end_pfn(pfn);
 
 	for (; pfn < end_pfn; pfn += isolated,
@@ -954,7 +979,9 @@ isolate_migratepages_range(struct compact_control *cc, unsigned long start_pfn,
 	pfn = start_pfn;
 	block_start_pfn = pageblock_start_pfn(pfn);
 	if (block_start_pfn < cc->zone->zone_start_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		block_start_pfn = cc->zone->zone_start_pfn;
+}
 	block_end_pfn = pageblock_end_pfn(pfn);
 
 	for (; pfn < end_pfn; pfn = block_end_pfn,
@@ -988,6 +1015,7 @@ static bool suitable_migration_source(struct compact_control *cc,
 {
 	int block_mt;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((cc->mode != MIGRATE_ASYNC) || !cc->direct_compaction)
 		return true;
 
@@ -1031,6 +1059,7 @@ static bool suitable_migration_target(struct compact_control *cc,
  */
 static inline bool compact_scanners_met(struct compact_control *cc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (cc->free_pfn >> pageblock_order)
 		<= (cc->migrate_pfn >> pageblock_order);
 }
@@ -1062,6 +1091,7 @@ static void isolate_freepages(struct compact_control *cc)
 	 */
 	isolate_start_pfn = cc->free_pfn;
 	block_start_pfn = pageblock_start_pfn(cc->free_pfn);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	block_end_pfn = min(block_start_pfn + pageblock_nr_pages,
 						zone_end_pfn(zone));
 	low_pfn = pageblock_end_pfn(cc->migrate_pfn);
@@ -1153,6 +1183,7 @@ static struct page *compaction_alloc(struct page *migratepage,
 	 * contention.
 	 */
 	if (list_empty(&cc->freepages)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!cc->contended)
 			isolate_freepages(cc);
 
@@ -1283,6 +1314,7 @@ static isolate_migrate_t isolate_migratepages(struct zone *zone,
  */
 static inline bool is_via_compact_memory(int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return order == -1;
 }
 
@@ -1292,6 +1324,7 @@ static enum compact_result __compact_finished(struct zone *zone,
 	unsigned int order;
 	const int migratetype = cc->migratetype;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (cc->contended || fatal_signal_pending(current))
 		return COMPACT_CONTENDED;
 
@@ -1385,7 +1418,9 @@ static enum compact_result compact_finished(struct zone *zone,
 	ret = __compact_finished(zone, cc);
 	trace_mm_compaction_finished(zone, cc->order, ret);
 	if (ret == COMPACT_NO_SUITABLE_PAGE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = COMPACT_CONTINUE;
+}
 
 	return ret;
 }
@@ -1405,7 +1440,9 @@ static enum compact_result __compaction_suitable(struct zone *zone, int order,
 	unsigned long watermark;
 
 	if (is_via_compact_memory(order))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return COMPACT_CONTINUE;
+}
 
 	watermark = zone->watermark[alloc_flags & ALLOC_WMARK_MASK];
 	/*
@@ -1733,7 +1770,9 @@ enum compact_result try_to_compact_pages(gfp_t gfp_mask, unsigned int order,
 	 * tricky context because the migration might require IO
 	 */
 	if (!may_perform_io)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return COMPACT_SKIPPED;
+}
 
 	trace_mm_compaction_try_to_compact_pages(order, gfp_mask, prio);
 
@@ -1805,6 +1844,7 @@ static void compact_node(int nid)
 	};
 
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zoneid = 0; zoneid < MAX_NR_ZONES; zoneid++) {
 
 		zone = &pgdat->node_zones[zoneid];
@@ -1832,6 +1872,7 @@ static void compact_nodes(void)
 	/* Flush pending updates to the LRU lists */
 	lru_add_drain_all();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_online_node(nid)
 		compact_node(nid);
 }
@@ -1846,6 +1887,7 @@ int sysctl_compact_memory;
 int sysctl_compaction_handler(struct ctl_table *table, int write,
 			void __user *buffer, size_t *length, loff_t *ppos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (write)
 		compact_nodes();
 
@@ -1855,6 +1897,7 @@ int sysctl_compaction_handler(struct ctl_table *table, int write,
 int sysctl_extfrag_handler(struct ctl_table *table, int write,
 			void __user *buffer, size_t *length, loff_t *ppos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	proc_dointvec_minmax(table, write, buffer, length, ppos);
 
 	return 0;
@@ -1867,6 +1910,7 @@ static ssize_t sysfs_compact_node(struct device *dev,
 {
 	int nid = dev->id;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (nid >= 0 && nid < nr_node_ids && node_online(nid)) {
 		/* Flush pending updates to the LRU lists */
 		lru_add_drain_all();
@@ -1880,11 +1924,13 @@ static DEVICE_ATTR(compact, S_IWUSR, NULL, sysfs_compact_node);
 
 int compaction_register_node(struct node *node)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return device_create_file(&node->dev, &dev_attr_compact);
 }
 
 void compaction_unregister_node(struct node *node)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return device_remove_file(&node->dev, &dev_attr_compact);
 }
 #endif /* CONFIG_SYSFS && CONFIG_NUMA */
@@ -1900,6 +1946,7 @@ static bool kcompactd_node_suitable(pg_data_t *pgdat)
 	struct zone *zone;
 	enum zone_type classzone_idx = pgdat->kcompactd_classzone_idx;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zoneid = 0; zoneid <= classzone_idx; zoneid++) {
 		zone = &pgdat->node_zones[zoneid];
 
@@ -1936,6 +1983,7 @@ static void kcompactd_do_work(pg_data_t *pgdat)
 							cc.classzone_idx);
 	count_compact_event(KCOMPACTD_WAKE);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zoneid = 0; zoneid <= cc.classzone_idx; zoneid++) {
 		int status;
 
@@ -1995,24 +2043,38 @@ static void kcompactd_do_work(pg_data_t *pgdat)
 void wakeup_kcompactd(pg_data_t *pgdat, int order, int classzone_idx)
 {
 	if (!order)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pgdat->kcompactd_max_order < order)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pgdat->kcompactd_max_order = order;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pgdat->kcompactd_classzone_idx > classzone_idx)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pgdat->kcompactd_classzone_idx = classzone_idx;
+}
 
 	/*
 	 * Pairs with implicit barrier in wait_event_freezable()
 	 * such that wakeups are not missed.
 	 */
 	if (!wq_has_sleeper(&pgdat->kcompactd_wait))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!kcompactd_node_suitable(pgdat))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	trace_mm_compaction_wakeup_kcompactd(pgdat->node_id, order,
 							classzone_idx);
 	wake_up_interruptible(&pgdat->kcompactd_wait);
@@ -2045,6 +2107,7 @@ static int kcompactd(void *p)
 		kcompactd_do_work(pgdat);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -2058,14 +2121,18 @@ int kcompactd_run(int nid)
 	int ret = 0;
 
 	if (pgdat->kcompactd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	pgdat->kcompactd = kthread_run(kcompactd, pgdat, "kcompactd%d", nid);
 	if (IS_ERR(pgdat->kcompactd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("Failed to start kcompactd on node %d\n", nid);
 		ret = PTR_ERR(pgdat->kcompactd);
 		pgdat->kcompactd = NULL;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -2078,6 +2145,7 @@ void kcompactd_stop(int nid)
 	struct task_struct *kcompactd = NODE_DATA(nid)->kcompactd;
 
 	if (kcompactd) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kthread_stop(kcompactd);
 		NODE_DATA(nid)->kcompactd = NULL;
 	}
@@ -2093,6 +2161,7 @@ static int kcompactd_cpu_online(unsigned int cpu)
 {
 	int nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node_state(nid, N_MEMORY) {
 		pg_data_t *pgdat = NODE_DATA(nid);
 		const struct cpumask *mask;
@@ -2115,12 +2184,14 @@ static int __init kcompactd_init(void)
 					"mm/compaction:online",
 					kcompactd_cpu_online, NULL);
 	if (ret < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("kcompactd: failed to register hotplug callbacks.\n");
 		return ret;
 	}
 
 	for_each_node_state(nid, N_MEMORY)
 		kcompactd_run(nid);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 subsys_initcall(kcompactd_init)
diff --git a/mm/early_ioremap.c b/mm/early_ioremap.c
index 1826f19..7867047 100644
--- a/mm/early_ioremap.c
+++ b/mm/early_ioremap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Provide common bits of early_ioremap() support for architectures needing
@@ -23,6 +25,7 @@ static int early_ioremap_debug __initdata;
 
 static int __init early_ioremap_debug_setup(char *str)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	early_ioremap_debug = 1;
 
 	return 0;
@@ -35,6 +38,7 @@ pgprot_t __init __weak early_memremap_pgprot_adjust(resource_size_t phys_addr,
 						    unsigned long size,
 						    pgprot_t prot)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return prot;
 }
 
@@ -44,6 +48,7 @@ void __init __weak early_ioremap_shutdown(void)
 
 void __init early_ioremap_reset(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	early_ioremap_shutdown();
 	after_paging_init = 1;
 }
@@ -91,7 +96,9 @@ static int __init check_early_ioremap_leak(void)
 
 	for (i = 0; i < FIX_BTMAPS_SLOTS; i++)
 		if (prev_map[i])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			count++;
+}
 
 	if (WARN(count, KERN_WARNING
 		 "Debug warning: early ioremap leak of %d areas detected.\n"
@@ -116,6 +123,7 @@ __early_ioremap(resource_size_t phys_addr, unsigned long size, pgprot_t prot)
 	slot = -1;
 	for (i = 0; i < FIX_BTMAPS_SLOTS; i++) {
 		if (!prev_map[i]) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			slot = i;
 			break;
 		}
@@ -128,7 +136,9 @@ __early_ioremap(resource_size_t phys_addr, unsigned long size, pgprot_t prot)
 	/* Don't allow wraparound or zero size */
 	last_addr = phys_addr + size - 1;
 	if (WARN_ON(!size || last_addr < phys_addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	prev_size[slot] = size;
 	/*
@@ -143,7 +153,9 @@ __early_ioremap(resource_size_t phys_addr, unsigned long size, pgprot_t prot)
 	 */
 	nrpages = size >> PAGE_SHIFT;
 	if (WARN_ON(nrpages > NR_FIX_BTMAPS))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	/*
 	 * Ok, go for it..
@@ -151,7 +163,9 @@ __early_ioremap(resource_size_t phys_addr, unsigned long size, pgprot_t prot)
 	idx = FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*slot;
 	while (nrpages > 0) {
 		if (after_paging_init)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__late_set_fixmap(idx, phys_addr, prot);
+}
 		else
 			__early_set_fixmap(idx, phys_addr, prot);
 		phys_addr += PAGE_SIZE;
@@ -176,6 +190,7 @@ void __init early_iounmap(void __iomem *addr, unsigned long size)
 	slot = -1;
 	for (i = 0; i < FIX_BTMAPS_SLOTS; i++) {
 		if (prev_map[i] == addr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			slot = i;
 			break;
 		}
@@ -195,7 +210,9 @@ void __init early_iounmap(void __iomem *addr, unsigned long size)
 
 	virt_addr = (unsigned long)addr;
 	if (WARN_ON(virt_addr < fix_to_virt(FIX_BTMAP_BEGIN)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	offset = offset_in_page(virt_addr);
 	nrpages = PAGE_ALIGN(offset + size) >> PAGE_SHIFT;
@@ -203,7 +220,9 @@ void __init early_iounmap(void __iomem *addr, unsigned long size)
 	idx = FIX_BTMAP_BEGIN - NR_FIX_BTMAPS*slot;
 	while (nrpages > 0) {
 		if (after_paging_init)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__late_clear_fixmap(idx);
+}
 		else
 			__early_set_fixmap(idx, 0, FIXMAP_PAGE_CLEAR);
 		--idx;
@@ -216,6 +235,7 @@ void __init early_iounmap(void __iomem *addr, unsigned long size)
 void __init __iomem *
 early_ioremap(resource_size_t phys_addr, unsigned long size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __early_ioremap(phys_addr, size, FIXMAP_PAGE_IO);
 }
 
@@ -232,6 +252,7 @@ early_memremap(resource_size_t phys_addr, unsigned long size)
 void __init *
 early_memremap_ro(resource_size_t phys_addr, unsigned long size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pgprot_t prot = early_memremap_pgprot_adjust(phys_addr, size,
 						     FIXMAP_PAGE_RO);
 
@@ -256,6 +277,7 @@ void __init copy_from_early_mem(void *dest, phys_addr_t src, unsigned long size)
 	unsigned long slop, clen;
 	char *p;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (size) {
 		slop = offset_in_page(src);
 		clen = size;
diff --git a/mm/fadvise.c b/mm/fadvise.c
index ec70d6e..4e5c50e 100644
--- a/mm/fadvise.c
+++ b/mm/fadvise.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * mm/fadvise.c
@@ -39,16 +41,21 @@ SYSCALL_DEFINE4(fadvise64_64, int, fd, loff_t, offset, loff_t, len, int, advice)
 	int ret = 0;
 
 	if (!f.file)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EBADF;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inode = file_inode(f.file);
 	if (S_ISFIFO(inode->i_mode)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = -ESPIPE;
 		goto out;
 	}
 
 	mapping = f.file->f_mapping;
 	if (!mapping || len < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = -EINVAL;
 		goto out;
 	}
@@ -74,7 +81,9 @@ SYSCALL_DEFINE4(fadvise64_64, int, fd, loff_t, offset, loff_t, len, int, advice)
 	/* Careful about overflows. Len == 0 means "as much as possible" */
 	endbyte = offset + len;
 	if (!len || endbyte < len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		endbyte = -1;
+}
 	else
 		endbyte--;		/* inclusive */
 
@@ -104,7 +113,9 @@ SYSCALL_DEFINE4(fadvise64_64, int, fd, loff_t, offset, loff_t, len, int, advice)
 		/* Careful about overflow on the "+1" */
 		nrpages = end_index - start_index + 1;
 		if (!nrpages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nrpages = ~0UL;
+}
 
 		/*
 		 * Ignore return value because fadvise() shall return
@@ -117,8 +128,10 @@ SYSCALL_DEFINE4(fadvise64_64, int, fd, loff_t, offset, loff_t, len, int, advice)
 		break;
 	case POSIX_FADV_DONTNEED:
 		if (!inode_write_congested(mapping->host))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__filemap_fdatawrite_range(mapping, offset, endbyte,
 						   WB_SYNC_NONE);
+}
 
 		/*
 		 * First and last FULL page! Partial pages are deliberately
@@ -136,9 +149,11 @@ SYSCALL_DEFINE4(fadvise64_64, int, fd, loff_t, offset, loff_t, len, int, advice)
 			if (end_index == 0)
 				break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			end_index--;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (end_index >= start_index) {
 			unsigned long count;
 
@@ -163,6 +178,7 @@ SYSCALL_DEFINE4(fadvise64_64, int, fd, loff_t, offset, loff_t, len, int, advice)
 			 * pagevecs and try again.
 			 */
 			if (count < (end_index - start_index + 1)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				lru_add_drain_all();
 				invalidate_mapping_pages(mapping, start_index,
 						end_index);
@@ -181,6 +197,7 @@ SYSCALL_DEFINE4(fadvise64_64, int, fd, loff_t, offset, loff_t, len, int, advice)
 
 SYSCALL_DEFINE4(fadvise64, int, fd, loff_t, offset, size_t, len, int, advice)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sys_fadvise64_64(fd, offset, len, advice);
 }
 
diff --git a/mm/filemap.c b/mm/filemap.c
index 594d73f..ce2be386 100644
--- a/mm/filemap.c
+++ b/mm/filemap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *	linux/mm/filemap.c
  *
@@ -121,17 +123,24 @@ static int page_cache_tree_insert(struct address_space *mapping,
 	error = __radix_tree_create(&mapping->page_tree, page->index, 0,
 				    &node, &slot);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 	if (*slot) {
 		void *p;
 
 		p = radix_tree_deref_slot_protected(slot, &mapping->tree_lock);
 		if (!radix_tree_exceptional_entry(p))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EEXIST;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mapping->nrexceptional--;
 		if (shadowp)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*shadowp = p;
+}
 	}
 	__radix_tree_replace(&mapping->page_tree, node, slot, page,
 			     workingset_update_node, mapping);
@@ -166,6 +175,7 @@ static void page_cache_tree_delete(struct address_space *mapping,
 	}
 
 	if (shadow) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mapping->nrexceptional += nr;
 		/*
 		 * Make sure the nrexceptional update is committed before
@@ -195,10 +205,13 @@ void __delete_from_page_cache(struct page *page, void *shadow)
 	 * stale data around in the cleancache once our page is gone
 	 */
 	if (PageUptodate(page) && PageMappedToDisk(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		cleancache_put_page(page);
+}
 	else
 		cleancache_invalidate_page(mapping, page);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(PageTail(page), page);
 	VM_BUG_ON_PAGE(page_mapped(page), page);
 	if (!IS_ENABLED(CONFIG_DEBUG_VM) && unlikely(page_mapped(page))) {
@@ -231,14 +244,19 @@ void __delete_from_page_cache(struct page *page, void *shadow)
 
 	/* hugetlb pages do not participate in page cache accounting. */
 	if (PageHuge(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	__mod_node_page_state(page_pgdat(page), NR_FILE_PAGES, -nr);
 	if (PageSwapBacked(page)) {
 		__mod_node_page_state(page_pgdat(page), NR_SHMEM, -nr);
 		if (PageTransHuge(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__dec_node_page_state(page, NR_SHMEM_THPS);
+}
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(PageTransHuge(page), page);
 	}
 
@@ -251,8 +269,10 @@ void __delete_from_page_cache(struct page *page, void *shadow)
 	 * anyway will be cleared before returning page into buddy allocator.
 	 */
 	if (WARN_ON_ONCE(PageDirty(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		account_page_cleaned(page, mapping, inode_to_wb(mapping->host));
 }
+}
 
 /**
  * delete_from_page_cache - delete page from page cache
@@ -277,9 +297,12 @@ void delete_from_page_cache(struct page *page)
 	spin_unlock_irqrestore(&mapping->tree_lock, flags);
 
 	if (freepage)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		freepage(page);
+}
 
 	if (PageTransHuge(page) && !PageHuge(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page_ref_sub(page, HPAGE_PMD_NR);
 		VM_BUG_ON_PAGE(page_count(page) <= 0, page);
 	} else {
@@ -339,7 +362,9 @@ int __filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
 	};
 
 	if (!mapping_cap_writeback_dirty(mapping))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	wbc_attach_fdatawrite_inode(&wbc, mapping->host);
 	ret = do_writepages(mapping, &wbc);
@@ -362,6 +387,7 @@ EXPORT_SYMBOL(filemap_fdatawrite);
 int filemap_fdatawrite_range(struct address_space *mapping, loff_t start,
 				loff_t end)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __filemap_fdatawrite_range(mapping, start, end, WB_SYNC_ALL);
 }
 EXPORT_SYMBOL(filemap_fdatawrite_range);
@@ -375,6 +401,7 @@ EXPORT_SYMBOL(filemap_fdatawrite_range);
  */
 int filemap_flush(struct address_space *mapping)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __filemap_fdatawrite(mapping, WB_SYNC_NONE);
 }
 EXPORT_SYMBOL(filemap_flush);
@@ -396,7 +423,9 @@ bool filemap_range_has_page(struct address_space *mapping,
 	struct page *page;
 
 	if (end_byte < start_byte)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (mapping->nrpages == 0)
 		return false;
@@ -417,8 +446,11 @@ static void __filemap_fdatawait_range(struct address_space *mapping,
 	int nr_pages;
 
 	if (end_byte < start_byte)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pagevec_init(&pvec, 0);
 	while ((index <= end) &&
 			(nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,
@@ -426,6 +458,7 @@ static void __filemap_fdatawait_range(struct address_space *mapping,
 			min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1)) != 0) {
 		unsigned i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for (i = 0; i < nr_pages; i++) {
 			struct page *page = pvec.pages[i];
 
@@ -433,9 +466,11 @@ static void __filemap_fdatawait_range(struct address_space *mapping,
 			if (page->index > end)
 				continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			wait_on_page_writeback(page);
 			ClearPageError(page);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pagevec_release(&pvec);
 		cond_resched();
 	}
@@ -500,6 +535,7 @@ EXPORT_SYMBOL(file_fdatawait_range);
  */
 int filemap_fdatawait_keep_errors(struct address_space *mapping)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__filemap_fdatawait_range(mapping, 0, LLONG_MAX);
 	return filemap_check_and_keep_errors(mapping);
 }
@@ -526,7 +562,9 @@ int filemap_write_and_wait(struct address_space *mapping)
 		if (err != -EIO) {
 			int err2 = filemap_fdatawait(mapping);
 			if (!err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				err = err2;
+}
 		} else {
 			/* Clear any previously stored errors */
 			filemap_check_errors(mapping);
@@ -555,6 +593,7 @@ int filemap_write_and_wait_range(struct address_space *mapping,
 	int err = 0;
 
 	if (mapping_needs_writeback(mapping)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = __filemap_fdatawrite_range(mapping, lstart, lend,
 						 WB_SYNC_ALL);
 		/* See comment of filemap_write_and_wait() */
@@ -576,6 +615,7 @@ EXPORT_SYMBOL(filemap_write_and_wait_range);
 
 void __filemap_set_wb_err(struct address_space *mapping, int err)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	errseq_t eseq = errseq_set(&mapping->wb_err, err);
 
 	trace_filemap_set_wb_err(mapping, eseq);
@@ -607,6 +647,7 @@ EXPORT_SYMBOL(__filemap_set_wb_err);
 int file_check_and_advance_wb_err(struct file *file)
 {
 	int err = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	errseq_t old = READ_ONCE(file->f_wb_err);
 	struct address_space *mapping = file->f_mapping;
 
@@ -652,6 +693,7 @@ int file_write_and_wait_range(struct file *file, loff_t lstart, loff_t lend)
 	struct address_space *mapping = file->f_mapping;
 
 	if (mapping_needs_writeback(mapping)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = __filemap_fdatawrite_range(mapping, lstart, lend,
 						 WB_SYNC_ALL);
 		/* See comment of filemap_write_and_wait() */
@@ -701,6 +743,7 @@ int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask)
 		new->mapping = mapping;
 		new->index = offset;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irqsave(&mapping->tree_lock, flags);
 		__delete_from_page_cache(old, NULL);
 		error = page_cache_tree_insert(mapping, new, NULL);
@@ -741,13 +784,19 @@ static int __add_to_page_cache_locked(struct page *page,
 		error = mem_cgroup_try_charge(page, current->mm,
 					      gfp_mask, &memcg, false);
 		if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return error;
+}
 	}
 
 	error = radix_tree_maybe_preload(gfp_mask & ~__GFP_HIGHMEM);
 	if (error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!huge)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mem_cgroup_cancel_charge(page, memcg, false);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
 	}
 
@@ -764,6 +813,7 @@ static int __add_to_page_cache_locked(struct page *page,
 	/* hugetlb pages do not participate in page cache accounting. */
 	if (!huge)
 		__inc_node_page_state(page, NR_FILE_PAGES);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock_irq(&mapping->tree_lock);
 	if (!huge)
 		mem_cgroup_commit_charge(page, memcg, false, false);
@@ -774,7 +824,10 @@ static int __add_to_page_cache_locked(struct page *page,
 	/* Leave page->index set: truncation relies upon it */
 	spin_unlock_irq(&mapping->tree_lock);
 	if (!huge)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mem_cgroup_cancel_charge(page, memcg, false);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	put_page(page);
 	return error;
 }
@@ -792,6 +845,7 @@ static int __add_to_page_cache_locked(struct page *page,
 int add_to_page_cache_locked(struct page *page, struct address_space *mapping,
 		pgoff_t offset, gfp_t gfp_mask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __add_to_page_cache_locked(page, mapping, offset,
 					  gfp_mask, NULL);
 }
@@ -807,7 +861,9 @@ int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
 	ret = __add_to_page_cache_locked(page, mapping, offset,
 					 gfp_mask, &shadow);
 	if (unlikely(ret))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__ClearPageLocked(page);
+}
 	else {
 		/*
 		 * The page might have been evicted from cache only
@@ -819,10 +875,13 @@ int add_to_page_cache_lru(struct page *page, struct address_space *mapping,
 		 */
 		if (!(gfp_mask & __GFP_WRITE) &&
 		    shadow && workingset_refault(shadow)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			SetPageActive(page);
 			workingset_activation(page);
 		} else
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ClearPageActive(page);
+}
 		lru_cache_add(page);
 	}
 	return ret;
@@ -838,6 +897,7 @@ struct page *__page_cache_alloc(gfp_t gfp)
 	if (cpuset_do_page_mem_spread()) {
 		unsigned int cpuset_mems_cookie;
 		do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			cpuset_mems_cookie = read_mems_allowed_begin();
 			n = cpuset_mem_spread_node();
 			page = __alloc_pages_node(n, gfp, 0);
@@ -899,21 +959,28 @@ static int wake_page_function(wait_queue_entry_t *wait, unsigned mode, int sync,
 		= container_of(wait, struct wait_page_queue, wait);
 
 	if (wait_page->page != key->page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	       return 0;
+}
 	key->page_match = 1;
 
 	if (wait_page->bit_nr != key->bit_nr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Stop walking if it's locked */
 	if (test_bit(key->bit_nr, &key->page->flags))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	return autoremove_wake_function(wait, mode, sync, key);
 }
 
 static void wake_up_page_bit(struct page *page, int bit_nr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	wait_queue_head_t *q = page_waitqueue(page);
 	struct wait_page_key key;
 	unsigned long flags;
@@ -940,7 +1007,9 @@ static void wake_up_page_bit(struct page *page, int bit_nr)
 		 */
 		spin_unlock_irqrestore(&q->lock, flags);
 		cpu_relax();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irqsave(&q->lock, flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__wake_up_locked_key_bookmark(q, TASK_NORMAL, &key, &bookmark);
 	}
 
@@ -954,6 +1023,7 @@ static void wake_up_page_bit(struct page *page, int bit_nr)
 	 * page waiters.
 	 */
 	if (!waitqueue_active(q) || !key.page_match) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ClearPageWaiters(page);
 		/*
 		 * It's possible to miss clearing Waiters here, when we woke
@@ -963,11 +1033,13 @@ static void wake_up_page_bit(struct page *page, int bit_nr)
 		 * That's okay, it's a rare case. The next waker will clear it.
 		 */
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock_irqrestore(&q->lock, flags);
 }
 
 static void wake_up_page(struct page *page, int bit)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!PageWaiters(page))
 		return;
 	wake_up_page_bit(page, bit);
@@ -987,15 +1059,18 @@ static inline int wait_on_page_bit_common(wait_queue_head_t *q,
 	wait_page.bit_nr = bit_nr;
 
 	for (;;) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irq(&q->lock);
 
 		if (likely(list_empty(&wait->entry))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__add_wait_queue_entry_tail(q, wait);
 			SetPageWaiters(page);
 		}
 
 		set_current_state(state);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irq(&q->lock);
 
 		if (likely(test_bit(bit_nr, &page->flags))) {
@@ -1006,11 +1081,14 @@ static inline int wait_on_page_bit_common(wait_queue_head_t *q,
 			if (!test_and_set_bit_lock(bit_nr, &page->flags))
 				break;
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!test_bit(bit_nr, &page->flags))
 				break;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(signal_pending_state(state, current))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = -EINTR;
 			break;
 		}
@@ -1031,6 +1109,7 @@ static inline int wait_on_page_bit_common(wait_queue_head_t *q,
 
 void wait_on_page_bit(struct page *page, int bit_nr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	wait_queue_head_t *q = page_waitqueue(page);
 	wait_on_page_bit_common(q, page, bit_nr, TASK_UNINTERRUPTIBLE, false);
 }
@@ -1038,6 +1117,7 @@ EXPORT_SYMBOL(wait_on_page_bit);
 
 int wait_on_page_bit_killable(struct page *page, int bit_nr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	wait_queue_head_t *q = page_waitqueue(page);
 	return wait_on_page_bit_common(q, page, bit_nr, TASK_KILLABLE, false);
 }
@@ -1051,6 +1131,7 @@ int wait_on_page_bit_killable(struct page *page, int bit_nr)
  */
 void add_page_wait_queue(struct page *page, wait_queue_entry_t *waiter)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	wait_queue_head_t *q = page_waitqueue(page);
 	unsigned long flags;
 
@@ -1101,6 +1182,7 @@ static inline bool clear_bit_unlock_is_negative_byte(long nr, volatile void *mem
  */
 void unlock_page(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG_ON(PG_waiters != 7);
 	page = compound_head(page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
@@ -1143,21 +1225,27 @@ void page_endio(struct page *page, bool is_write, int err)
 {
 	if (!is_write) {
 		if (!err) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			SetPageUptodate(page);
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ClearPageUptodate(page);
 			SetPageError(page);
 		}
 		unlock_page(page);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (err) {
 			struct address_space *mapping;
 
 			SetPageError(page);
 			mapping = page_mapping(page);
 			if (mapping)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				mapping_set_error(mapping, err);
+}
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		end_page_writeback(page);
 	}
 }
@@ -1177,6 +1265,7 @@ EXPORT_SYMBOL(__lock_page);
 
 int __lock_page_killable(struct page *__page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct page *page = compound_head(__page);
 	wait_queue_head_t *q = page_waitqueue(page);
 	return wait_on_page_bit_common(q, page, PG_locked, TASK_KILLABLE, true);
@@ -1197,6 +1286,7 @@ EXPORT_SYMBOL_GPL(__lock_page_killable);
 int __lock_page_or_retry(struct page *page, struct mm_struct *mm,
 			 unsigned int flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 		/*
 		 * CAUTION! In this case, mmap_sem is not released
@@ -1252,6 +1342,7 @@ pgoff_t page_cache_next_hole(struct address_space *mapping,
 {
 	unsigned long i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < max_scan; i++) {
 		struct page *page;
 
@@ -1331,10 +1422,12 @@ struct page *find_get_entry(struct address_space *mapping, pgoff_t offset)
 	page = NULL;
 	pagep = radix_tree_lookup_slot(&mapping->page_tree, offset);
 	if (pagep) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = radix_tree_deref_slot(pagep);
 		if (unlikely(!page))
 			goto out;
 		if (radix_tree_exception(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (radix_tree_deref_retry(page))
 				goto repeat;
 			/*
@@ -1351,6 +1444,7 @@ struct page *find_get_entry(struct address_space *mapping, pgoff_t offset)
 
 		/* The page was split under us? */
 		if (compound_head(page) != head) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
@@ -1361,6 +1455,7 @@ struct page *find_get_entry(struct address_space *mapping, pgoff_t offset)
 		 * include/linux/pagemap.h for details.
 		 */
 		if (unlikely(page != *pagep)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
@@ -1398,10 +1493,12 @@ struct page *find_lock_entry(struct address_space *mapping, pgoff_t offset)
 		lock_page(page);
 		/* Has the page been truncated? */
 		if (unlikely(page_mapping(page) != mapping)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 			put_page(page);
 			goto repeat;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(page_to_pgoff(page) != offset, page);
 	}
 	return page;
@@ -1441,54 +1538,75 @@ struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 repeat:
 	page = find_get_entry(mapping, offset);
 	if (radix_tree_exceptional_entry(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = NULL;
+}
 	if (!page)
 		goto no_page;
 
 	if (fgp_flags & FGP_LOCK) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (fgp_flags & FGP_NOWAIT) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!trylock_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				put_page(page);
 				return NULL;
 			}
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			lock_page(page);
 		}
 
 		/* Has the page been truncated? */
 		if (unlikely(page->mapping != mapping)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 			put_page(page);
 			goto repeat;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(page->index != offset, page);
 	}
 
 	if (page && (fgp_flags & FGP_ACCESSED))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mark_page_accessed(page);
+}
 
 no_page:
 	if (!page && (fgp_flags & FGP_CREAT)) {
 		int err;
 		if ((fgp_flags & FGP_WRITE) && mapping_cap_account_dirty(mapping))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			gfp_mask |= __GFP_WRITE;
+}
 		if (fgp_flags & FGP_NOFS)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			gfp_mask &= ~__GFP_FS;
+}
 
 		page = __page_cache_alloc(gfp_mask);
 		if (!page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 
 		if (WARN_ON_ONCE(!(fgp_flags & FGP_LOCK)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			fgp_flags |= FGP_LOCK;
+}
 
 		/* Init accessed so avoid atomic mark_page_accessed later */
 		if (fgp_flags & FGP_ACCESSED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__SetPageReferenced(page);
+}
 
 		err = add_to_page_cache_lru(page, mapping, offset,
 				gfp_mask & GFP_RECLAIM_MASK);
 		if (unlikely(err)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
 			page = NULL;
 			if (err == -EEXIST)
@@ -1496,6 +1614,7 @@ struct page *pagecache_get_page(struct address_space *mapping, pgoff_t offset,
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return page;
 }
 EXPORT_SYMBOL(pagecache_get_page);
@@ -1532,8 +1651,11 @@ unsigned find_get_entries(struct address_space *mapping,
 	struct radix_tree_iter iter;
 
 	if (!nr_entries)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_lock();
 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
 		struct page *head, *page;
@@ -1542,7 +1664,9 @@ unsigned find_get_entries(struct address_space *mapping,
 		if (unlikely(!page))
 			continue;
 		if (radix_tree_exception(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (radix_tree_deref_retry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				slot = radix_tree_iter_retry(&iter);
 				continue;
 			}
@@ -1560,12 +1684,14 @@ unsigned find_get_entries(struct address_space *mapping,
 
 		/* The page was split under us? */
 		if (compound_head(page) != head) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
 
 		/* Has the page moved? */
 		if (unlikely(page != *slot)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
@@ -1575,6 +1701,7 @@ unsigned find_get_entries(struct address_space *mapping,
 		if (++ret == nr_entries)
 			break;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_unlock();
 	return ret;
 }
@@ -1609,8 +1736,11 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
 	unsigned ret = 0;
 
 	if (unlikely(!nr_pages))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_lock();
 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, *start) {
 		struct page *head, *page;
@@ -1622,8 +1752,11 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
 		if (unlikely(!page))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (radix_tree_exception(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (radix_tree_deref_retry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				slot = radix_tree_iter_retry(&iter);
 				continue;
 			}
@@ -1635,24 +1768,29 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
 			continue;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		head = compound_head(page);
 		if (!page_cache_get_speculative(head))
 			goto repeat;
 
 		/* The page was split under us? */
 		if (compound_head(page) != head) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
 
 		/* Has the page moved? */
 		if (unlikely(page != *slot)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pages[ret] = page;
 		if (++ret == nr_pages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*start = pages[ret - 1]->index + 1;
 			goto out;
 		}
@@ -1665,7 +1803,9 @@ unsigned find_get_pages_range(struct address_space *mapping, pgoff_t *start,
 	 * already broken anyway.
 	 */
 	if (end == (pgoff_t)-1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*start = (pgoff_t)-1;
+}
 	else
 		*start = end + 1;
 out:
@@ -1694,7 +1834,9 @@ unsigned find_get_pages_contig(struct address_space *mapping, pgoff_t index,
 	unsigned int ret = 0;
 
 	if (unlikely(!nr_pages))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	rcu_read_lock();
 	radix_tree_for_each_contig(slot, &mapping->page_tree, &iter, index) {
@@ -1772,8 +1914,11 @@ unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index,
 	unsigned ret = 0;
 
 	if (unlikely(!nr_pages))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_lock();
 	radix_tree_for_each_tagged(slot, &mapping->page_tree,
 				   &iter, *index, tag) {
@@ -1783,8 +1928,11 @@ unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index,
 		if (unlikely(!page))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (radix_tree_exception(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (radix_tree_deref_retry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				slot = radix_tree_iter_retry(&iter);
 				continue;
 			}
@@ -1802,32 +1950,40 @@ unsigned find_get_pages_tag(struct address_space *mapping, pgoff_t *index,
 			continue;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		head = compound_head(page);
 		if (!page_cache_get_speculative(head))
 			goto repeat;
 
 		/* The page was split under us? */
 		if (compound_head(page) != head) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
 
 		/* Has the page moved? */
 		if (unlikely(page != *slot)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pages[ret] = page;
 		if (++ret == nr_pages)
 			break;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_unlock();
 
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*index = pages[ret - 1]->index + 1;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 EXPORT_SYMBOL(find_get_pages_tag);
@@ -1853,7 +2009,9 @@ unsigned find_get_entries_tag(struct address_space *mapping, pgoff_t start,
 	struct radix_tree_iter iter;
 
 	if (!nr_entries)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	rcu_read_lock();
 	radix_tree_for_each_tagged(slot, &mapping->page_tree,
@@ -1921,6 +2079,7 @@ EXPORT_SYMBOL(find_get_entries_tag);
 static void shrink_readahead_size_eio(struct file *filp,
 					struct file_ra_state *ra)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ra->ra_pages /= 4;
 }
 
@@ -1952,7 +2111,9 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 	int error = 0;
 
 	if (unlikely(*ppos >= inode->i_sb->s_maxbytes))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	iov_iter_truncate(iter, inode->i_sb->s_maxbytes);
 
 	index = *ppos >> PAGE_SHIFT;
@@ -1970,6 +2131,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		cond_resched();
 find_page:
 		if (fatal_signal_pending(current)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EINTR;
 			goto out;
 		}
@@ -1991,7 +2153,9 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 					index, last_index - index);
 		}
 		if (!PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (iocb->ki_flags & IOCB_NOWAIT) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				put_page(page);
 				goto would_block;
 			}
@@ -2004,23 +2168,28 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 			error = wait_on_page_locked_killable(page);
 			if (unlikely(error))
 				goto readpage_error;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (PageUptodate(page))
 				goto page_ok;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (inode->i_blkbits == PAGE_SHIFT ||
 					!mapping->a_ops->is_partially_uptodate)
 				goto page_not_up_to_date;
 			/* pipes can't handle partially uptodate pages */
 			if (unlikely(iter->type & ITER_PIPE))
 				goto page_not_up_to_date;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!trylock_page(page))
 				goto page_not_up_to_date;
 			/* Did it get truncated before we got the lock? */
 			if (!page->mapping)
 				goto page_not_up_to_date_locked;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!mapping->a_ops->is_partially_uptodate(page,
 							offset, iter->count))
 				goto page_not_up_to_date_locked;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 		}
 page_ok:
@@ -2033,9 +2202,11 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		 * another truncate extends the file - this is desired though).
 		 */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		isize = i_size_read(inode);
 		end_index = (isize - 1) >> PAGE_SHIFT;
 		if (unlikely(!isize || index > end_index)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
 			goto out;
 		}
@@ -2045,6 +2216,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		if (index == end_index) {
 			nr = ((isize - 1) & ~PAGE_MASK) + 1;
 			if (nr <= offset) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				put_page(page);
 				goto out;
 			}
@@ -2056,7 +2228,9 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		 * before reading the page on the kernel side.
 		 */
 		if (mapping_writably_mapped(mapping))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			flush_dcache_page(page);
+}
 
 		/*
 		 * When a sequential read accesses a page several times,
@@ -2064,6 +2238,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		 */
 		if (prev_index != index || offset != prev_offset)
 			mark_page_accessed(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		prev_index = index;
 
 		/*
@@ -2082,6 +2257,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		if (!iov_iter_count(iter))
 			goto out;
 		if (ret < nr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EFAULT;
 			goto out;
 		}
@@ -2096,6 +2272,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 page_not_up_to_date_locked:
 		/* Did it get truncated before we got the lock? */
 		if (!page->mapping) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 			put_page(page);
 			continue;
@@ -2103,6 +2280,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 
 		/* Did somebody else fill it already? */
 		if (PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 			goto page_ok;
 		}
@@ -2118,7 +2296,9 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		error = mapping->a_ops->readpage(filp, page);
 
 		if (unlikely(error)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (error == AOP_TRUNCATED_PAGE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				put_page(page);
 				error = 0;
 				goto find_page;
@@ -2126,11 +2306,15 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 			goto readpage_error;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = lock_page_killable(page);
 			if (unlikely(error))
 				goto readpage_error;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (page->mapping == NULL) {
 					/*
 					 * invalidate_mapping_pages got it
@@ -2139,11 +2323,13 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 					put_page(page);
 					goto find_page;
 				}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				unlock_page(page);
 				shrink_readahead_size_eio(filp, ra);
 				error = -EIO;
 				goto readpage_error;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 		}
 
@@ -2161,14 +2347,17 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 		 */
 		page = page_cache_alloc_cold(mapping);
 		if (!page) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -ENOMEM;
 			goto out;
 		}
 		error = add_to_page_cache_lru(page, mapping, index,
 				mapping_gfp_constraint(mapping, GFP_KERNEL));
 		if (error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
 			if (error == -EEXIST) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = 0;
 				goto find_page;
 			}
@@ -2200,6 +2389,7 @@ static ssize_t generic_file_buffered_read(struct kiocb *iocb,
 ssize_t
 generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	size_t count = iov_iter_count(iter);
 	ssize_t retval = 0;
 
@@ -2214,10 +2404,12 @@ generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
 
 		size = i_size_read(inode);
 		if (iocb->ki_flags & IOCB_NOWAIT) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (filemap_range_has_page(mapping, iocb->ki_pos,
 						   iocb->ki_pos + count - 1))
 				return -EAGAIN;
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			retval = filemap_write_and_wait_range(mapping,
 						iocb->ki_pos,
 					        iocb->ki_pos + count - 1);
@@ -2225,13 +2417,16 @@ generic_file_read_iter(struct kiocb *iocb, struct iov_iter *iter)
 				goto out;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		file_accessed(file);
 
 		retval = mapping->a_ops->direct_IO(iocb, iter);
 		if (retval >= 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			iocb->ki_pos += retval;
 			count -= retval;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		iov_iter_revert(iter, count - iov_iter_count(iter));
 
 		/*
@@ -2271,6 +2466,7 @@ static int page_cache_read(struct file *file, pgoff_t offset, gfp_t gfp_mask)
 	int ret;
 
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = __page_cache_alloc(gfp_mask|__GFP_COLD);
 		if (!page)
 			return -ENOMEM;
@@ -2303,7 +2499,9 @@ static void do_sync_mmap_readahead(struct vm_area_struct *vma,
 
 	/* If we don't want any read-ahead, don't bother */
 	if (vma->vm_flags & VM_RAND_READ)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (!ra->ra_pages)
 		return;
 
@@ -2347,7 +2545,9 @@ static void do_async_mmap_readahead(struct vm_area_struct *vma,
 
 	/* If we don't want any read-ahead, don't bother */
 	if (vma->vm_flags & VM_RAND_READ)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (ra->mmap_miss > 0)
 		ra->mmap_miss--;
 	if (PageReadahead(page))
@@ -2392,7 +2592,9 @@ int filemap_fault(struct vm_fault *vmf)
 
 	max_off = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 	if (unlikely(offset >= max_off))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_SIGBUS;
+}
 
 	/*
 	 * Do we have something in the page cache already?
@@ -2520,7 +2722,9 @@ void filemap_map_pages(struct vm_fault *vmf,
 		if (unlikely(!page))
 			goto next;
 		if (radix_tree_exception(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (radix_tree_deref_retry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				slot = radix_tree_iter_retry(&iter);
 				continue;
 			}
@@ -2533,12 +2737,14 @@ void filemap_map_pages(struct vm_fault *vmf,
 
 		/* The page was split under us? */
 		if (compound_head(page) != head) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
 
 		/* Has the page moved? */
 		if (unlikely(page != *slot)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto repeat;
 		}
@@ -2558,11 +2764,14 @@ void filemap_map_pages(struct vm_fault *vmf,
 			goto unlock;
 
 		if (file->f_ra.mmap_miss > 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			file->f_ra.mmap_miss--;
+}
 
 		vmf->address += (iter.index - last_pgoff) << PAGE_SHIFT;
 		if (vmf->pte)
 			vmf->pte += iter.index - last_pgoff;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		last_pgoff = iter.index;
 		if (alloc_set_pte(vmf, NULL, page))
 			goto unlock;
@@ -2579,6 +2788,7 @@ void filemap_map_pages(struct vm_fault *vmf,
 		if (iter.index == end_pgoff)
 			break;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_unlock();
 }
 EXPORT_SYMBOL(filemap_map_pages);
@@ -2593,6 +2803,7 @@ int filemap_page_mkwrite(struct vm_fault *vmf)
 	file_update_time(vmf->vma->vm_file);
 	lock_page(page);
 	if (page->mapping != inode->i_mapping) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(page);
 		ret = VM_FAULT_NOPAGE;
 		goto out;
@@ -2623,7 +2834,9 @@ int generic_file_mmap(struct file * file, struct vm_area_struct * vma)
 	struct address_space *mapping = file->f_mapping;
 
 	if (!mapping->a_ops->readpage)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOEXEC;
+}
 	file_accessed(file);
 	vma->vm_ops = &generic_file_vm_ops;
 	return 0;
@@ -2634,6 +2847,7 @@ int generic_file_mmap(struct file * file, struct vm_area_struct * vma)
  */
 int generic_file_readonly_mmap(struct file *file, struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((vma->vm_flags & VM_SHARED) && (vma->vm_flags & VM_MAYWRITE))
 		return -EINVAL;
 	return generic_file_mmap(file, vma);
@@ -2657,6 +2871,7 @@ static struct page *wait_on_page_read(struct page *page)
 	if (!IS_ERR(page)) {
 		wait_on_page_locked(page);
 		if (!PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
 			page = ERR_PTR(-EIO);
 		}
@@ -2677,9 +2892,12 @@ static struct page *do_read_cache_page(struct address_space *mapping,
 	if (!page) {
 		page = __page_cache_alloc(gfp | __GFP_COLD);
 		if (!page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ERR_PTR(-ENOMEM);
+}
 		err = add_to_page_cache_lru(page, mapping, index, gfp);
 		if (unlikely(err)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
 			if (err == -EEXIST)
 				goto repeat;
@@ -2690,13 +2908,16 @@ static struct page *do_read_cache_page(struct address_space *mapping,
 filler:
 		err = filler(data, page);
 		if (err < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
 			return ERR_PTR(err);
 		}
 
 		page = wait_on_page_read(page);
 		if (IS_ERR(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
 		goto out;
 	}
 	if (PageUptodate(page))
@@ -2742,6 +2963,7 @@ static struct page *do_read_cache_page(struct address_space *mapping,
 
 	/* Case c or d, restart the operation */
 	if (!page->mapping) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(page);
 		put_page(page);
 		goto repeat;
@@ -2749,6 +2971,7 @@ static struct page *do_read_cache_page(struct address_space *mapping,
 
 	/* Someone else locked and filled the page in a very small window */
 	if (PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(page);
 		goto out;
 	}
@@ -2816,7 +3039,9 @@ inline ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	loff_t pos;
 
 	if (!iov_iter_count(from))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* FIXME: this is for backwards compatibility with 2.4 */
 	if (iocb->ki_flags & IOCB_APPEND)
@@ -2825,13 +3050,17 @@ inline ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	pos = iocb->ki_pos;
 
 	if ((iocb->ki_flags & IOCB_NOWAIT) && !(iocb->ki_flags & IOCB_DIRECT))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (limit != RLIM_INFINITY) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (iocb->ki_pos >= limit) {
 			send_sig(SIGXFSZ, current, 0);
 			return -EFBIG;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		iov_iter_truncate(from, limit - (unsigned long)pos);
 	}
 
@@ -2840,8 +3069,12 @@ inline ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	 */
 	if (unlikely(pos + iov_iter_count(from) > MAX_NON_LFS &&
 				!(file->f_flags & O_LARGEFILE))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (pos >= MAX_NON_LFS)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EFBIG;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		iov_iter_truncate(from, MAX_NON_LFS - (unsigned long)pos);
 	}
 
@@ -2853,7 +3086,9 @@ inline ssize_t generic_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	 * Linus frestrict idea will clean these up nicely..
 	 */
 	if (unlikely(pos >= inode->i_sb->s_maxbytes))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EFBIG;
+}
 
 	iov_iter_truncate(from, inode->i_sb->s_maxbytes - pos);
 	return iov_iter_count(from);
@@ -2970,7 +3205,9 @@ struct page *grab_cache_page_write_begin(struct address_space *mapping,
 	int fgp_flags = FGP_LOCK|FGP_WRITE|FGP_CREAT;
 
 	if (flags & AOP_FLAG_NOFS)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		fgp_flags |= FGP_NOFS;
+}
 
 	page = pagecache_get_page(mapping, index, fgp_flags,
 			mapping_gfp_mask(mapping));
@@ -3013,11 +3250,13 @@ ssize_t generic_perform_write(struct file *file,
 		 * usercopies are used, below.
 		 */
 		if (unlikely(iov_iter_fault_in_readable(i, bytes))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			status = -EFAULT;
 			break;
 		}
 
 		if (fatal_signal_pending(current)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			status = -EINTR;
 			break;
 		}
@@ -3027,10 +3266,14 @@ ssize_t generic_perform_write(struct file *file,
 		if (unlikely(status < 0))
 			break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (mapping_writably_mapped(mapping))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			flush_dcache_page(page);
+}
 
 		copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_dcache_page(page);
 
 		status = a_ops->write_end(file, mapping, pos, bytes, copied,
@@ -3115,6 +3358,7 @@ ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 		if (written < 0 || !iov_iter_count(from) || IS_DAX(inode))
 			goto out;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		status = generic_perform_write(file, from, pos = iocb->ki_pos);
 		/*
 		 * If generic_perform_write() returned a synchronous error
@@ -3124,6 +3368,7 @@ ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 		 * will return -EFOO even if some bytes were written.
 		 */
 		if (unlikely(status < 0)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			err = status;
 			goto out;
 		}
@@ -3135,6 +3380,7 @@ ssize_t __generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 		endbyte = pos + status - 1;
 		err = filemap_write_and_wait_range(mapping, pos, endbyte);
 		if (err == 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			iocb->ki_pos = endbyte + 1;
 			written += status;
 			invalidate_mapping_pages(mapping,
@@ -3176,6 +3422,7 @@ ssize_t generic_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 	ret = generic_write_checks(iocb, from);
 	if (ret > 0)
 		ret = __generic_file_write_iter(iocb, from);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inode_unlock(inode);
 
 	if (ret > 0)
@@ -3207,10 +3454,13 @@ int try_to_release_page(struct page *page, gfp_t gfp_mask)
 
 	BUG_ON(!PageLocked(page));
 	if (PageWriteback(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (mapping && mapping->a_ops->releasepage)
 		return mapping->a_ops->releasepage(page, gfp_mask);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return try_to_free_buffers(page);
 }
 
diff --git a/mm/gup.c b/mm/gup.c
index e0d82b6..4b25904 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 #include <linux/kernel.h>
 #include <linux/errno.h>
 #include <linux/err.h>
@@ -32,7 +34,10 @@ static struct page *no_page_table(struct vm_area_struct *vma,
 	 * be zero-filled if handle_mm_fault() actually did handle it.
 	 */
 	if ((flags & FOLL_DUMP) && (!vma->vm_ops || !vma->vm_ops->fault))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-EFAULT);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -81,7 +86,9 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
 
 retry:
 	if (unlikely(pmd_bad(*pmd)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
+}
 
 	ptep = pte_offset_map_lock(mm, pmd, address, &ptl);
 	pte = *ptep;
@@ -94,18 +101,24 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
 		 */
 		if (likely(!(flags & FOLL_MIGRATION)))
 			goto no_page;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (pte_none(pte))
 			goto no_page;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_to_swp_entry(pte);
 		if (!is_migration_entry(entry))
 			goto no_page;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_unmap_unlock(ptep, ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		migration_entry_wait(mm, pmd, address);
 		goto retry;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((flags & FOLL_NUMA) && pte_protnone(pte))
 		goto no_page;
 	if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_unmap_unlock(ptep, ptl);
 		return NULL;
 	}
@@ -118,17 +131,22 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
 		 */
 		pgmap = get_dev_pagemap(pte_pfn(pte), NULL);
 		if (pgmap)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = pte_page(pte);
+}
 		else
 			goto no_page;
 	} else if (unlikely(!page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (flags & FOLL_DUMP) {
 			/* Avoid special (like zero) pages in core dumps */
 			page = ERR_PTR(-EFAULT);
 			goto out;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (is_zero_pfn(pte_pfn(pte))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = pte_page(pte);
 		} else {
 			int ret;
@@ -142,13 +160,17 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
 	if (flags & FOLL_SPLIT && PageTransCompound(page)) {
 		int ret;
 		get_page(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_unmap_unlock(ptep, ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		lock_page(page);
 		ret = split_huge_page(page);
 		unlock_page(page);
 		put_page(page);
 		if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ERR_PTR(ret);
+}
 		goto retry;
 	}
 
@@ -157,6 +179,7 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
 
 		/* drop the pgmap reference now that we hold the page */
 		if (pgmap) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_dev_pagemap(pgmap);
 			pgmap = NULL;
 		}
@@ -204,7 +227,9 @@ static struct page *follow_page_pte(struct vm_area_struct *vma,
 no_page:
 	pte_unmap_unlock(ptep, ptl);
 	if (!pte_none(pte))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	return no_page_table(vma, flags);
 }
 
@@ -219,67 +244,102 @@ static struct page *follow_pmd_mask(struct vm_area_struct *vma,
 
 	pmd = pmd_offset(pudp, address);
 	if (pmd_none(*pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
+}
 	if (pmd_huge(*pmd) && vma->vm_flags & VM_HUGETLB) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = follow_huge_pmd(mm, address, pmd, flags);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_hugepd(__hugepd(pmd_val(*pmd)))) {
 		page = follow_huge_pd(vma, address,
 				      __hugepd(pmd_val(*pmd)), flags,
 				      PMD_SHIFT);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
 	}
 retry:
 	if (!pmd_present(*pmd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (likely(!(flags & FOLL_MIGRATION)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return no_page_table(vma, flags);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON(thp_migration_supported() &&
 				  !is_pmd_migration_entry(*pmd));
 		if (is_pmd_migration_entry(*pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pmd_migration_entry_wait(mm, pmd);
+}
 		goto retry;
 	}
 	if (pmd_devmap(*pmd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ptl = pmd_lock(mm, pmd);
 		page = follow_devmap_pmd(vma, address, pmd, flags);
 		spin_unlock(ptl);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
 	}
 	if (likely(!pmd_trans_huge(*pmd)))
 		return follow_page_pte(vma, address, pmd, flags);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((flags & FOLL_NUMA) && pmd_protnone(*pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
+}
 
 retry_locked:
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_present(*pmd))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(ptl);
 		if (likely(!(flags & FOLL_MIGRATION)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return no_page_table(vma, flags);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pmd_migration_entry_wait(mm, pmd);
 		goto retry_locked;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(!pmd_trans_huge(*pmd))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(ptl);
 		return follow_page_pte(vma, address, pmd, flags);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (flags & FOLL_SPLIT) {
 		int ret;
 		page = pmd_page(*pmd);
 		if (is_huge_zero_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(ptl);
 			ret = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			split_huge_pmd(vma, pmd, address);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (pmd_trans_unstable(pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ret = -EBUSY;
+}
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			get_page(page);
 			spin_unlock(ptl);
 			lock_page(page);
@@ -287,14 +347,19 @@ static struct page *follow_pmd_mask(struct vm_area_struct *vma,
 			unlock_page(page);
 			put_page(page);
 			if (pmd_none(*pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return no_page_table(vma, flags);
+}
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret ? ERR_PTR(ret) :
 			follow_page_pte(vma, address, pmd, flags);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	page = follow_trans_huge_pmd(vma, address, pmd, flags);
 	spin_unlock(ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	*page_mask = HPAGE_PMD_NR - 1;
 	return page;
 }
@@ -311,30 +376,45 @@ static struct page *follow_pud_mask(struct vm_area_struct *vma,
 
 	pud = pud_offset(p4dp, address);
 	if (pud_none(*pud))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
+}
 	if (pud_huge(*pud) && vma->vm_flags & VM_HUGETLB) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = follow_huge_pud(mm, address, pud, flags);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_hugepd(__hugepd(pud_val(*pud)))) {
 		page = follow_huge_pd(vma, address,
 				      __hugepd(pud_val(*pud)), flags,
 				      PUD_SHIFT);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
 	}
 	if (pud_devmap(*pud)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ptl = pud_lock(mm, pud);
 		page = follow_devmap_pud(vma, address, pud, flags);
 		spin_unlock(ptl);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
 	}
 	if (unlikely(pud_bad(*pud)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
+}
 
 	return follow_pmd_mask(vma, address, pud, flags, page_mask);
 }
@@ -350,16 +430,23 @@ static struct page *follow_p4d_mask(struct vm_area_struct *vma,
 	p4d = p4d_offset(pgdp, address);
 	if (p4d_none(*p4d))
 		return no_page_table(vma, flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG_ON(p4d_huge(*p4d));
 	if (unlikely(p4d_bad(*p4d)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_hugepd(__hugepd(p4d_val(*p4d)))) {
 		page = follow_huge_pd(vma, address,
 				      __hugepd(p4d_val(*p4d)), flags,
 				      P4D_SHIFT);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
 	}
 	return follow_pud_mask(vma, address, p4d, flags, page_mask);
@@ -391,27 +478,41 @@ struct page *follow_page_mask(struct vm_area_struct *vma,
 	/* make this handle hugepd */
 	page = follow_huge_addr(mm, address, flags & FOLL_WRITE);
 	if (!IS_ERR(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(flags & FOLL_GET);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return page;
 	}
 
 	pgd = pgd_offset(mm, address);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pgd_huge(*pgd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = follow_huge_pgd(mm, address, pgd, flags);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_hugepd(__hugepd(pgd_val(*pgd)))) {
 		page = follow_huge_pd(vma, address,
 				      __hugepd(pgd_val(*pgd)), flags,
 				      PGDIR_SHIFT);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return no_page_table(vma, flags);
 	}
 
@@ -431,7 +532,9 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,
 
 	/* user gate pages are read-only */
 	if (gup_flags & FOLL_WRITE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EFAULT;
+}
 	if (address > TASK_SIZE)
 		pgd = pgd_offset_k(address);
 	else
@@ -485,39 +588,56 @@ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
 
 	/* mlock all present pages, but do not fault in new pages */
 	if ((*flags & (FOLL_POPULATE | FOLL_MLOCK)) == FOLL_MLOCK)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOENT;
+}
 	if (*flags & FOLL_WRITE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		fault_flags |= FAULT_FLAG_WRITE;
+}
 	if (*flags & FOLL_REMOTE)
 		fault_flags |= FAULT_FLAG_REMOTE;
 	if (nonblocking)
 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
 	if (*flags & FOLL_NOWAIT)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		fault_flags |= FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT;
+}
 	if (*flags & FOLL_TRIED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_WARN_ON_ONCE(fault_flags & FAULT_FLAG_ALLOW_RETRY);
 		fault_flags |= FAULT_FLAG_TRIED;
 	}
 
 	ret = handle_mm_fault(vma, address, fault_flags);
 	if (ret & VM_FAULT_ERROR) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		int err = vm_fault_to_errno(ret, *flags);
 
 		if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return err;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG();
 	}
 
 	if (tsk) {
 		if (ret & VM_FAULT_MAJOR)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			tsk->maj_flt++;
+}
 		else
 			tsk->min_flt++;
 	}
 
 	if (ret & VM_FAULT_RETRY) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (nonblocking)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*nonblocking = 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EBUSY;
 	}
 
@@ -532,6 +652,7 @@ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
 	 */
 	if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
 	        *flags |= FOLL_COW;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -542,12 +663,16 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 	int foreign = (gup_flags & FOLL_REMOTE);
 
 	if (vm_flags & (VM_IO | VM_PFNMAP))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EFAULT;
+}
 
 	if (write) {
 		if (!(vm_flags & VM_WRITE)) {
 			if (!(gup_flags & FOLL_FORCE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EFAULT;
+}
 			/*
 			 * We used to let the write,force case do COW in a
 			 * VM_MAYWRITE VM_SHARED !VM_WRITE vma, so ptrace could
@@ -558,24 +683,33 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)
 			 * just reject it.
 			 */
 			if (!is_cow_mapping(vm_flags))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EFAULT;
+}
 		}
 	} else if (!(vm_flags & VM_READ)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!(gup_flags & FOLL_FORCE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EFAULT;
+}
 		/*
 		 * Is there actually any vma we can reach here which does not
 		 * have VM_MAYREAD set?
 		 */
 		if (!(vm_flags & VM_MAYREAD))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EFAULT;
+}
 	}
 	/*
 	 * gups are always data accesses, not instruction
 	 * fetches, so execute=false here
 	 */
 	if (!arch_vma_access_permitted(vma, write, false, foreign))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EFAULT;
+}
 	return 0;
 }
 
@@ -645,8 +779,11 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 	struct vm_area_struct *vma = NULL;
 
 	if (!nr_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(!!pages != !!(gup_flags & FOLL_GET));
 
 	/*
@@ -671,14 +808,20 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 						gup_flags, &vma,
 						pages ? &pages[i] : NULL);
 				if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					return i ? : ret;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				page_mask = 0;
 				goto next_page;
 			}
 
 			if (!vma || check_vma_flags(vma, gup_flags))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return i ? : -EFAULT;
+}
 			if (is_vm_hugetlb_page(vma)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				i = follow_hugetlb_page(mm, vma, pages, vmas,
 						&start, &nr_pages, i,
 						gup_flags, nonblocking);
@@ -691,7 +834,9 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 		 * potentially allocating memory.
 		 */
 		if (unlikely(fatal_signal_pending(current)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return i ? i : -ERESTARTSYS;
+}
 		cond_resched();
 		page = follow_page_mask(vma, start, foll_flags, &page_mask);
 		if (!page) {
@@ -710,6 +855,7 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 			case -ENOENT:
 				goto next_page;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			BUG();
 		} else if (PTR_ERR(page) == -EEXIST) {
 			/*
@@ -718,11 +864,13 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 			 */
 			goto next_page;
 		} else if (IS_ERR(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return i ? i : PTR_ERR(page);
 		}
 		if (pages) {
 			pages[i] = page;
 			flush_anon_page(vma, page, start);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			flush_dcache_page(page);
 			page_mask = 0;
 		}
@@ -733,11 +881,14 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 		}
 		page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
 		if (page_increm > nr_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page_increm = nr_pages;
+}
 		i += page_increm;
 		start += page_increm * PAGE_SIZE;
 		nr_pages -= page_increm;
 	} while (nr_pages);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return i;
 }
 
@@ -746,6 +897,7 @@ static bool vma_permits_fault(struct vm_area_struct *vma,
 {
 	bool write   = !!(fault_flags & FAULT_FLAG_WRITE);
 	bool foreign = !!(fault_flags & FAULT_FLAG_REMOTE);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	vm_flags_t vm_flags = write ? VM_WRITE : VM_READ;
 
 	if (!(vm_flags & vma->vm_flags))
@@ -802,7 +954,9 @@ int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 	int ret, major = 0;
 
 	if (unlocked)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
+}
 
 retry:
 	vma = find_extend_vma(mm, address);
@@ -864,6 +1018,7 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,
 	if (pages)
 		flags |= FOLL_GET;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pages_done = 0;
 	lock_dropped = false;
 	for (;;) {
@@ -875,24 +1030,32 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,
 
 		/* VM_FAULT_RETRY cannot return errors */
 		if (!*locked) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			BUG_ON(ret < 0);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			BUG_ON(ret >= nr_pages);
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!pages)
 			/* If it's a prefault don't insist harder */
 			return ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (ret > 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nr_pages -= ret;
 			pages_done += ret;
 			if (!nr_pages)
 				break;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (*locked) {
 			/* VM_FAULT_RETRY didn't trigger */
 			if (!pages_done)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				pages_done = ret;
+}
 			break;
 		}
 		/* VM_FAULT_RETRY triggered, so seek to the faulting offset */
@@ -910,18 +1073,25 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,
 		ret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,
 				       pages, NULL, NULL);
 		if (ret != 1) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			BUG_ON(ret > 1);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!pages_done)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				pages_done = ret;
+}
 			break;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr_pages--;
 		pages_done++;
 		if (!nr_pages)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pages++;
 		start += PAGE_SIZE;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (notify_drop && lock_dropped && *locked) {
 		/*
 		 * We must let the caller know we temporarily dropped the lock
@@ -930,6 +1100,7 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,
 		up_read(&mm->mmap_sem);
 		*locked = 0;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return pages_done;
 }
 
@@ -958,6 +1129,7 @@ long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
 			   unsigned int gup_flags, struct page **pages,
 			   int *locked)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __get_user_pages_locked(current, current->mm, start, nr_pages,
 				       pages, NULL, locked, true,
 				       gup_flags | FOLL_TOUCH);
@@ -984,7 +1156,9 @@ static __always_inline long __get_user_pages_unlocked(struct task_struct *tsk,
 	ret = __get_user_pages_locked(tsk, mm, start, nr_pages, pages, NULL,
 				      &locked, false, gup_flags);
 	if (locked)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		up_read(&mm->mmap_sem);
+}
 	return ret;
 }
 
@@ -1006,6 +1180,7 @@ static __always_inline long __get_user_pages_unlocked(struct task_struct *tsk,
 long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
 			     struct page **pages, unsigned int gup_flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __get_user_pages_unlocked(current, current->mm, start, nr_pages,
 					 pages, gup_flags | FOLL_TOUCH);
 }
@@ -1089,6 +1264,7 @@ long get_user_pages(unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
 		struct vm_area_struct **vmas)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __get_user_pages_locked(current, current->mm, start, nr_pages,
 				       pages, vmas, NULL, false,
 				       gup_flags | FOLL_TOUCH);
@@ -1193,7 +1369,9 @@ long populate_vma_page_range(struct vm_area_struct *vma,
 
 	gup_flags = FOLL_TOUCH | FOLL_POPULATE | FOLL_MLOCK;
 	if (vma->vm_flags & VM_LOCKONFAULT)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		gup_flags &= ~FOLL_POPULATE;
+}
 	/*
 	 * We want to touch writable mappings with a write fault in order
 	 * to break COW, except for shared mappings because these don't COW
@@ -1246,6 +1424,7 @@ int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)
 			down_read(&mm->mmap_sem);
 			vma = find_vma(mm, nstart);
 		} else if (nstart >= vma->vm_end)
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vma = vma->vm_next;
 		if (!vma || vma->vm_start >= end)
 			break;
@@ -1257,7 +1436,9 @@ int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)
 		if (vma->vm_flags & (VM_IO | VM_PFNMAP))
 			continue;
 		if (nstart < vma->vm_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nstart = vma->vm_start;
+}
 		/*
 		 * Now fault in a range of pages. populate_vma_page_range()
 		 * double checks the vma flags, so that it won't mlock pages
@@ -1265,7 +1446,9 @@ int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)
 		 */
 		ret = populate_vma_page_range(vma, nstart, nend, &locked);
 		if (ret < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (ignore_errors) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ret = 0;
 				continue;	/* continue at next VMA */
 			}
@@ -1350,12 +1533,14 @@ struct page *get_dump_page(unsigned long addr)
  */
 static inline pte_t gup_get_pte(pte_t *ptep)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return READ_ONCE(*ptep);
 }
 #endif
 
 static void undo_dev_pagemap(int *nr, int nr_start, struct page **pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while ((*nr) - nr_start) {
 		struct page *page = pages[--(*nr)];
 
@@ -1374,6 +1559,7 @@ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
 
 	ptem = ptep = pte_offset_map(&pmd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_t pte = gup_get_pte(ptep);
 		struct page *head, *page;
 
@@ -1388,14 +1574,17 @@ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
 			goto pte_unmap;
 
 		if (pte_devmap(pte)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pgmap = get_dev_pagemap(pte_pfn(pte), pgmap);
 			if (unlikely(!pgmap)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				undo_dev_pagemap(nr, nr_start, pages);
 				goto pte_unmap;
 			}
 		} else if (pte_special(pte))
 			goto pte_unmap;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON(!pfn_valid(pte_pfn(pte)));
 		page = pte_page(pte);
 		head = compound_head(page);
@@ -1404,10 +1593,12 @@ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
 			goto pte_unmap;
 
 		if (unlikely(pte_val(pte) != pte_val(*ptep))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(head);
 			goto pte_unmap;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(compound_head(page) != head, page);
 
 		put_dev_pagemap(pgmap);
@@ -1417,6 +1608,7 @@ static int gup_pte_range(pmd_t pmd, unsigned long addr, unsigned long end,
 
 	} while (ptep++, addr += PAGE_SIZE, addr != end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = 1;
 
 pte_unmap:
@@ -1487,6 +1679,7 @@ static int __gup_device_huge_pud(pud_t pud, unsigned long addr,
 static int __gup_device_huge_pmd(pmd_t pmd, unsigned long addr,
 		unsigned long end, struct page **pages, int *nr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG();
 	return 0;
 }
@@ -1494,6 +1687,7 @@ static int __gup_device_huge_pmd(pmd_t pmd, unsigned long addr,
 static int __gup_device_huge_pud(pud_t pud, unsigned long addr,
 		unsigned long end, struct page **pages, int *nr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG();
 	return 0;
 }
@@ -1506,11 +1700,16 @@ static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
 	int refs;
 
 	if (!pmd_access_permitted(orig, write))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (pmd_devmap(orig))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return __gup_device_huge_pmd(orig, addr, end, pages, nr);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	refs = 0;
 	page = pmd_page(orig) + ((addr & ~PMD_MASK) >> PAGE_SHIFT);
 	do {
@@ -1522,17 +1721,22 @@ static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,
 
 	head = compound_head(pmd_page(orig));
 	if (!page_cache_add_speculative(head, refs)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*nr -= refs;
 		return 0;
 	}
 
 	if (unlikely(pmd_val(orig) != pmd_val(*pmdp))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*nr -= refs;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		while (refs--)
 			put_page(head);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	SetPageReferenced(head);
 	return 1;
 }
@@ -1544,7 +1748,9 @@ static int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,
 	int refs;
 
 	if (!pud_access_permitted(orig, write))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (pud_devmap(orig))
 		return __gup_device_huge_pud(orig, addr, end, pages, nr);
@@ -1582,6 +1788,7 @@ static int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,
 	int refs;
 	struct page *head, *page;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!pgd_access_permitted(orig, write))
 		return 0;
 
@@ -1624,7 +1831,9 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,
 
 		next = pmd_addr_end(addr, end);
 		if (!pmd_present(pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 
 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
 			/*
@@ -1633,7 +1842,9 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,
 			 * can be serialised against THP migration.
 			 */
 			if (pmd_protnone(pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return 0;
+}
 
 			if (!gup_huge_pmd(pmd, pmdp, addr, next, write,
 				pages, nr))
@@ -1648,9 +1859,12 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,
 					 PMD_SHIFT, next, write, pages, nr))
 				return 0;
 		} else if (!gup_pte_range(pmd, addr, next, write, pages, nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return 0;
+}
 	} while (pmdp++, addr = next, addr != end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 1;
 }
 
@@ -1662,23 +1876,31 @@ static int gup_pud_range(p4d_t p4d, unsigned long addr, unsigned long end,
 
 	pudp = pud_offset(&p4d, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pud_t pud = READ_ONCE(*pudp);
 
 		next = pud_addr_end(addr, end);
 		if (pud_none(pud))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 		if (unlikely(pud_huge(pud))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!gup_huge_pud(pud, pudp, addr, next, write,
 					  pages, nr))
 				return 0;
 		} else if (unlikely(is_hugepd(__hugepd(pud_val(pud))))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!gup_huge_pd(__hugepd(pud_val(pud)), addr,
 					 PUD_SHIFT, next, write, pages, nr))
 				return 0;
 		} else if (!gup_pmd_range(pud, addr, next, write, pages, nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 	} while (pudp++, addr = next, addr != end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 1;
 }
 
@@ -1690,20 +1912,30 @@ static int gup_p4d_range(pgd_t pgd, unsigned long addr, unsigned long end,
 
 	p4dp = p4d_offset(&pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		p4d_t p4d = READ_ONCE(*p4dp);
 
 		next = p4d_addr_end(addr, end);
 		if (p4d_none(p4d))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUILD_BUG_ON(p4d_huge(p4d));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(is_hugepd(__hugepd(p4d_val(p4d))))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!gup_huge_pd(__hugepd(p4d_val(p4d)), addr,
 					 P4D_SHIFT, next, write, pages, nr))
 				return 0;
 		} else if (!gup_pud_range(p4d, addr, next, write, pages, nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (p4dp++, addr = next, addr != end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 1;
 }
 
@@ -1719,17 +1951,24 @@ static void gup_pgd_range(unsigned long addr, unsigned long end,
 
 		next = pgd_addr_end(addr, end);
 		if (pgd_none(pgd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(pgd_huge(pgd))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!gup_huge_pgd(pgd, pgdp, addr, next, write,
 					  pages, nr))
 				return;
 		} else if (unlikely(is_hugepd(__hugepd(pgd_val(pgd))))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!gup_huge_pd(__hugepd(pgd_val(pgd)), addr,
 					 PGDIR_SHIFT, next, write, pages, nr))
 				return;
 		} else if (!gup_p4d_range(pgd, addr, next, write, pages, nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
 	} while (pgdp++, addr = next, addr != end);
 }
 
@@ -1837,13 +2076,17 @@ int get_user_pages_fast(unsigned long start, int nr_pages, int write,
 
 		/* Have to be a bit careful with return values */
 		if (nr > 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ret = nr;
+}
 			else
 				ret += nr;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index eba34cd..92b31c7 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  Copyright (C) 2009  Red Hat, Inc.
  *
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index c539941..e75fd59 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Generic hugetlb support.
  * (C) Nadia Yvette Chambers, April 2004
@@ -73,6 +75,7 @@ static int hugetlb_acct_memory(struct hstate *h, long delta);
 
 static inline void unlock_or_release_subpool(struct hugepage_subpool *spool)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bool free = (spool->count == 0) && (spool->used_hpages == 0);
 
 	spin_unlock(&spool->lock);
@@ -95,7 +98,9 @@ struct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,
 
 	spool = kzalloc(sizeof(*spool), GFP_KERNEL);
 	if (!spool)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	spin_lock_init(&spool->lock);
 	spool->count = 1;
@@ -114,6 +119,7 @@ struct hugepage_subpool *hugepage_new_subpool(struct hstate *h, long max_hpages,
 
 void hugepage_put_subpool(struct hugepage_subpool *spool)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&spool->lock);
 	BUG_ON(!spool->count);
 	spool->count--;
@@ -134,14 +140,21 @@ static long hugepage_subpool_get_pages(struct hugepage_subpool *spool,
 	long ret = delta;
 
 	if (!spool)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&spool->lock);
 
 	if (spool->max_hpages != -1) {		/* maximum size accounting */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if ((spool->used_hpages + delta) <= spool->max_hpages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spool->used_hpages += delta;
+}
 		else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = -ENOMEM;
 			goto unlock_ret;
 		}
@@ -149,6 +162,7 @@ static long hugepage_subpool_get_pages(struct hugepage_subpool *spool,
 
 	/* minimum size accounting */
 	if (spool->min_hpages != -1 && spool->rsv_hpages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (delta > spool->rsv_hpages) {
 			/*
 			 * Asking for more reserves than those already taken on
@@ -157,6 +171,7 @@ static long hugepage_subpool_get_pages(struct hugepage_subpool *spool,
 			ret = delta - spool->rsv_hpages;
 			spool->rsv_hpages = 0;
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = 0;	/* reserves already accounted for */
 			spool->rsv_hpages -= delta;
 		}
@@ -179,23 +194,34 @@ static long hugepage_subpool_put_pages(struct hugepage_subpool *spool,
 	long ret = delta;
 
 	if (!spool)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return delta;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&spool->lock);
 
 	if (spool->max_hpages != -1)		/* maximum size accounting */
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spool->used_hpages -= delta;
+}
 
 	 /* minimum size accounting */
 	if (spool->min_hpages != -1 && spool->used_hpages < spool->min_hpages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (spool->rsv_hpages + delta <= spool->min_hpages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = 0;
+}
 		else
 			ret = spool->rsv_hpages + delta - spool->min_hpages;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spool->rsv_hpages += delta;
 		if (spool->rsv_hpages > spool->min_hpages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spool->rsv_hpages = spool->min_hpages;
+}
 	}
 
 	/*
@@ -214,6 +240,7 @@ static inline struct hugepage_subpool *subpool_inode(struct inode *inode)
 
 static inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return subpool_inode(file_inode(vma->vm_file));
 }
 
@@ -377,10 +404,12 @@ static long region_chg(struct resv_map *resv, long f, long t)
 
 		trg = kmalloc(sizeof(*trg), GFP_KERNEL);
 		if (!trg) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			kfree(nrg);
 			return -ENOMEM;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock(&resv->lock);
 		list_add(&trg->link, &resv->region_cache);
 		resv->region_cache_count++;
@@ -401,7 +430,9 @@ static long region_chg(struct resv_map *resv, long f, long t)
 			spin_unlock(&resv->lock);
 			nrg = kmalloc(sizeof(*nrg), GFP_KERNEL);
 			if (!nrg)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENOMEM;
+}
 
 			nrg->from = f;
 			nrg->to   = f;
@@ -416,13 +447,18 @@ static long region_chg(struct resv_map *resv, long f, long t)
 
 	/* Round our left edge to the current segment if it encloses us. */
 	if (f > rg->from)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		f = rg->from;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	chg = t - f;
 
 	/* Check for and consume any regions we now overlap with. */
 	list_for_each_entry(rg, rg->link.prev, link) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (&rg->link == head)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (rg->from > t)
 			goto out;
 
@@ -430,9 +466,11 @@ static long region_chg(struct resv_map *resv, long f, long t)
 		 * us then we must extend ourselves.  Account for its
 		 * existing reservation. */
 		if (rg->to > t) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			chg += rg->to - t;
 			t = rg->to;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		chg -= rg->to - rg->from;
 	}
 
@@ -459,6 +497,7 @@ static long region_chg(struct resv_map *resv, long f, long t)
  */
 static void region_abort(struct resv_map *resv, long f, long t)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&resv->lock);
 	VM_BUG_ON(!resv->region_cache_count);
 	resv->adds_in_progress--;
@@ -509,6 +548,7 @@ static long region_del(struct resv_map *resv, long f, long t)
 			 */
 			if (!nrg &&
 			    resv->region_cache_count > resv->adds_in_progress) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nrg = list_first_entry(&resv->region_cache,
 							struct file_region,
 							link);
@@ -516,14 +556,19 @@ static long region_del(struct resv_map *resv, long f, long t)
 				resv->region_cache_count--;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!nrg) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_unlock(&resv->lock);
 				nrg = kmalloc(sizeof(*nrg), GFP_KERNEL);
 				if (!nrg)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					return -ENOMEM;
+}
 				goto retry;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			del += t - f;
 
 			/* New entry for end of split region */
@@ -546,15 +591,19 @@ static long region_del(struct resv_map *resv, long f, long t)
 			continue;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (f <= rg->from) {	/* Trim beginning of region */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			del += t - rg->from;
 			rg->from = t;
 		} else {		/* Trim end of region */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			del += rg->to - f;
 			rg->to = f;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&resv->lock);
 	kfree(nrg);
 	return del;
@@ -571,6 +620,7 @@ static long region_del(struct resv_map *resv, long f, long t)
  */
 void hugetlb_fix_reserve_counts(struct inode *inode)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hugepage_subpool *spool = subpool_inode(inode);
 	long rsv_adjust;
 
@@ -620,6 +670,7 @@ static long region_count(struct resv_map *resv, long f, long t)
 static pgoff_t vma_hugecache_offset(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long address)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ((address - vma->vm_start) >> huge_page_shift(h)) +
 			(vma->vm_pgoff >> huge_page_order(h));
 }
@@ -627,6 +678,7 @@ static pgoff_t vma_hugecache_offset(struct hstate *h,
 pgoff_t linear_hugepage_index(struct vm_area_struct *vma,
 				     unsigned long address)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma_hugecache_offset(hstate_vma(vma), vma, address);
 }
 EXPORT_SYMBOL_GPL(linear_hugepage_index);
@@ -640,7 +692,9 @@ unsigned long vma_kernel_pagesize(struct vm_area_struct *vma)
 	struct hstate *hstate;
 
 	if (!is_vm_hugetlb_page(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PAGE_SIZE;
+}
 
 	hstate = hstate_vma(vma);
 
@@ -657,6 +711,7 @@ EXPORT_SYMBOL_GPL(vma_kernel_pagesize);
 #ifndef vma_mmu_pagesize
 unsigned long vma_mmu_pagesize(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma_kernel_pagesize(vma);
 }
 #endif
@@ -697,20 +752,24 @@ static unsigned long get_vma_private_data(struct vm_area_struct *vma)
 static void set_vma_private_data(struct vm_area_struct *vma,
 							unsigned long value)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	vma->vm_private_data = (void *)value;
 }
 
 struct resv_map *resv_map_alloc(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);
 	struct file_region *rg = kmalloc(sizeof(*rg), GFP_KERNEL);
 
 	if (!resv_map || !rg) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(resv_map);
 		kfree(rg);
 		return NULL;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kref_init(&resv_map->refs);
 	spin_lock_init(&resv_map->lock);
 	INIT_LIST_HEAD(&resv_map->regions);
@@ -726,6 +785,7 @@ struct resv_map *resv_map_alloc(void)
 
 void resv_map_release(struct kref *ref)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct resv_map *resv_map = container_of(ref, struct resv_map, refs);
 	struct list_head *head = &resv_map->region_cache;
 	struct file_region *rg, *trg;
@@ -739,6 +799,7 @@ void resv_map_release(struct kref *ref)
 		kfree(rg);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(resv_map->adds_in_progress);
 
 	kfree(resv_map);
@@ -751,6 +812,7 @@ static inline struct resv_map *inode_resv_map(struct inode *inode)
 
 static struct resv_map *vma_resv_map(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);
 	if (vma->vm_flags & VM_MAYSHARE) {
 		struct address_space *mapping = vma->vm_file->f_mapping;
@@ -766,6 +828,7 @@ static struct resv_map *vma_resv_map(struct vm_area_struct *vma)
 
 static void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);
 	VM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);
 
@@ -775,6 +838,7 @@ static void set_vma_resv_map(struct vm_area_struct *vma, struct resv_map *map)
 
 static void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);
 	VM_BUG_ON_VMA(vma->vm_flags & VM_MAYSHARE, vma);
 
@@ -783,6 +847,7 @@ static void set_vma_resv_flags(struct vm_area_struct *vma, unsigned long flags)
 
 static int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);
 
 	return (get_vma_private_data(vma) & flag) != 0;
@@ -791,6 +856,7 @@ static int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)
 /* Reset counters to 0 and clear all HPAGE_RESV_* flags */
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_VMA(!is_vm_hugetlb_page(vma), vma);
 	if (!(vma->vm_flags & VM_MAYSHARE))
 		vma->vm_private_data = (void *)0;
@@ -799,6 +865,7 @@ void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 /* Returns true if the VMA has associated reserve pages */
 static bool vma_has_reserves(struct vm_area_struct *vma, long chg)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (vma->vm_flags & VM_NORESERVE) {
 		/*
 		 * This address is already reserved by other process(chg == 0),
@@ -861,6 +928,7 @@ static bool vma_has_reserves(struct vm_area_struct *vma, long chg)
 
 static void enqueue_huge_page(struct hstate *h, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int nid = page_to_nid(page);
 	list_move(&page->lru, &h->hugepage_freelists[nid]);
 	h->free_huge_pages++;
@@ -871,6 +939,7 @@ static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)
 {
 	struct page *page;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry(page, &h->hugepage_freelists[nid], lru)
 		if (!PageHWPoison(page))
 			break;
@@ -927,7 +996,9 @@ static struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask,
 static inline gfp_t htlb_alloc_mask(struct hstate *h)
 {
 	if (hugepages_treat_as_movable || hugepage_migration_supported(h))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return GFP_HIGHUSER_MOVABLE;
+}
 	else
 		return GFP_HIGHUSER;
 }
@@ -980,6 +1051,7 @@ static struct page *dequeue_huge_page_vma(struct hstate *h,
  */
 static int next_node_allowed(int nid, nodemask_t *nodes_allowed)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	nid = next_node_in(nid, *nodes_allowed);
 	VM_BUG_ON(nid >= MAX_NUMNODES);
 
@@ -988,6 +1060,7 @@ static int next_node_allowed(int nid, nodemask_t *nodes_allowed)
 
 static int get_valid_node_allowed(int nid, nodemask_t *nodes_allowed)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!node_isset(nid, *nodes_allowed))
 		nid = next_node_allowed(nid, nodes_allowed);
 	return nid;
@@ -1852,7 +1925,9 @@ static long __vma_reservation_common(struct hstate *h,
 
 	resv = vma_resv_map(vma);
 	if (!resv)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	idx = vma_hugecache_offset(h, vma, addr);
 	switch (mode) {
@@ -1906,24 +1981,28 @@ static long __vma_reservation_common(struct hstate *h,
 static long vma_needs_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);
 }
 
 static long vma_commit_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);
 }
 
 static void vma_end_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	(void)__vma_reservation_common(h, vma, addr, VMA_END_RESV);
 }
 
 static long vma_add_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __vma_reservation_common(h, vma, addr, VMA_ADD_RESV);
 }
 
@@ -1942,6 +2021,7 @@ static void restore_reserve_on_error(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long address,
 			struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(PagePrivate(page))) {
 		long rc = vma_needs_reservation(h, vma, address);
 
@@ -1974,6 +2054,7 @@ static void restore_reserve_on_error(struct hstate *h,
 struct page *alloc_huge_page(struct vm_area_struct *vma,
 				    unsigned long addr, int avoid_reserve)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hugepage_subpool *spool = subpool_vma(vma);
 	struct hstate *h = hstate_vma(vma);
 	struct page *page;
@@ -2082,6 +2163,7 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct page *page = alloc_huge_page(vma, addr, avoid_reserve);
 	if (IS_ERR(page))
 		page = NULL;
@@ -2095,6 +2177,7 @@ int __alloc_bootmem_huge_page(struct hstate *h)
 	struct huge_bootmem_page *m;
 	int nr_nodes, node;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {
 		void *addr;
 
@@ -2124,6 +2207,7 @@ int __alloc_bootmem_huge_page(struct hstate *h)
 static void __init prep_compound_huge_page(struct page *page,
 		unsigned int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(order > (MAX_ORDER - 1)))
 		prep_compound_gigantic_page(page, order);
 	else
@@ -2148,6 +2232,7 @@ static void __init gather_bootmem_prealloc(void)
 #endif
 		WARN_ON(page_count(page) != 1);
 		prep_compound_huge_page(page, h->order);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON(PageReserved(page));
 		prep_new_huge_page(h, page, page_to_nid(page));
 		/*
@@ -2157,7 +2242,9 @@ static void __init gather_bootmem_prealloc(void)
 		 * side-effects, like CommitLimit going negative.
 		 */
 		if (hstate_is_gigantic(h))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			adjust_managed_page_count(page, 1 << h->order);
+}
 	}
 }
 
@@ -2166,12 +2253,15 @@ static void __init hugetlb_hstate_alloc_pages(struct hstate *h)
 	unsigned long i;
 
 	for (i = 0; i < h->max_huge_pages; ++i) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (hstate_is_gigantic(h)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!alloc_bootmem_huge_page(h))
 				break;
 		} else if (!alloc_fresh_huge_page(h,
 					 &node_states[N_MEMORY]))
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		cond_resched();
 	}
 	if (i < h->max_huge_pages) {
@@ -2196,6 +2286,7 @@ static void __init hugetlb_init_hstates(void)
 		if (!hstate_is_gigantic(h))
 			hugetlb_hstate_alloc_pages(h);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(minimum_order == UINT_MAX);
 }
 
@@ -2256,6 +2347,7 @@ static int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,
 	VM_BUG_ON(delta != -1 && delta != 1);
 
 	if (delta < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
 			if (h->surplus_huge_pages_node[node])
 				goto found;
@@ -2281,6 +2373,7 @@ static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,
 {
 	unsigned long min_count, ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hstate_is_gigantic(h) && !gigantic_page_supported())
 		return h->max_huge_pages;
 
@@ -2374,6 +2467,7 @@ static struct hstate *kobj_to_hstate(struct kobject *kobj, int *nidp)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < HUGE_MAX_HSTATE; i++)
 		if (hstate_kobjs[i] == kobj) {
 			if (nidp)
@@ -2393,7 +2487,9 @@ static ssize_t nr_hugepages_show_common(struct kobject *kobj,
 
 	h = kobj_to_hstate(kobj, &nid);
 	if (nid == NUMA_NO_NODE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr_huge_pages = h->nr_huge_pages;
+}
 	else
 		nr_huge_pages = h->nr_huge_pages_node[nid];
 
@@ -2407,6 +2503,7 @@ static ssize_t __nr_hugepages_store_common(bool obey_mempolicy,
 	int err;
 	NODEMASK_ALLOC(nodemask_t, nodes_allowed, GFP_KERNEL | __GFP_NORETRY);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hstate_is_gigantic(h) && !gigantic_page_supported()) {
 		err = -EINVAL;
 		goto out;
@@ -2453,7 +2550,9 @@ static ssize_t nr_hugepages_store_common(bool obey_mempolicy,
 
 	err = kstrtoul(buf, 10, &count);
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return err;
+}
 
 	h = kobj_to_hstate(kobj, &nid);
 	return __nr_hugepages_store_common(obey_mempolicy, h, nid, count, len);
@@ -2462,12 +2561,14 @@ static ssize_t nr_hugepages_store_common(bool obey_mempolicy,
 static ssize_t nr_hugepages_show(struct kobject *kobj,
 				       struct kobj_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return nr_hugepages_show_common(kobj, attr, buf);
 }
 
 static ssize_t nr_hugepages_store(struct kobject *kobj,
 	       struct kobj_attribute *attr, const char *buf, size_t len)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return nr_hugepages_store_common(false, kobj, buf, len);
 }
 HSTATE_ATTR(nr_hugepages);
@@ -2481,12 +2582,14 @@ HSTATE_ATTR(nr_hugepages);
 static ssize_t nr_hugepages_mempolicy_show(struct kobject *kobj,
 				       struct kobj_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return nr_hugepages_show_common(kobj, attr, buf);
 }
 
 static ssize_t nr_hugepages_mempolicy_store(struct kobject *kobj,
 	       struct kobj_attribute *attr, const char *buf, size_t len)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return nr_hugepages_store_common(true, kobj, buf, len);
 }
 HSTATE_ATTR(nr_hugepages_mempolicy);
@@ -2496,6 +2599,7 @@ HSTATE_ATTR(nr_hugepages_mempolicy);
 static ssize_t nr_overcommit_hugepages_show(struct kobject *kobj,
 					struct kobj_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hstate *h = kobj_to_hstate(kobj, NULL);
 	return sprintf(buf, "%lu\n", h->nr_overcommit_huge_pages);
 }
@@ -2508,7 +2612,9 @@ static ssize_t nr_overcommit_hugepages_store(struct kobject *kobj,
 	struct hstate *h = kobj_to_hstate(kobj, NULL);
 
 	if (hstate_is_gigantic(h))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	err = kstrtoul(buf, 10, &input);
 	if (err)
@@ -2531,7 +2637,9 @@ static ssize_t free_hugepages_show(struct kobject *kobj,
 
 	h = kobj_to_hstate(kobj, &nid);
 	if (nid == NUMA_NO_NODE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		free_huge_pages = h->free_huge_pages;
+}
 	else
 		free_huge_pages = h->free_huge_pages_node[nid];
 
@@ -2542,6 +2650,7 @@ HSTATE_ATTR_RO(free_hugepages);
 static ssize_t resv_hugepages_show(struct kobject *kobj,
 					struct kobj_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hstate *h = kobj_to_hstate(kobj, NULL);
 	return sprintf(buf, "%lu\n", h->resv_huge_pages);
 }
@@ -2556,7 +2665,9 @@ static ssize_t surplus_hugepages_show(struct kobject *kobj,
 
 	h = kobj_to_hstate(kobj, &nid);
 	if (nid == NUMA_NO_NODE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		surplus_huge_pages = h->surplus_huge_pages;
+}
 	else
 		surplus_huge_pages = h->surplus_huge_pages_node[nid];
 
@@ -2589,12 +2700,17 @@ static int hugetlb_sysfs_add_hstate(struct hstate *h, struct kobject *parent,
 
 	hstate_kobjs[hi] = kobject_create_and_add(h->name, parent);
 	if (!hstate_kobjs[hi])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	retval = sysfs_create_group(hstate_kobjs[hi], hstate_attr_group);
 	if (retval)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kobject_put(hstate_kobjs[hi]);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return retval;
 }
 
@@ -2605,13 +2721,17 @@ static void __init hugetlb_sysfs_init(void)
 
 	hugepages_kobj = kobject_create_and_add("hugepages", mm_kobj);
 	if (!hugepages_kobj)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	for_each_hstate(h) {
 		err = hugetlb_sysfs_add_hstate(h, hugepages_kobj,
 					 hstate_kobjs, &hstate_attr_group);
 		if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_err("Hugetlb: Unable to add hstate %s", h->name);
+}
 	}
 }
 
@@ -2652,6 +2772,7 @@ static struct hstate *kobj_to_node_hstate(struct kobject *kobj, int *nidp)
 {
 	int nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (nid = 0; nid < nr_node_ids; nid++) {
 		struct node_hstate *nhs = &node_hstates[nid];
 		int i;
@@ -2677,7 +2798,9 @@ static void hugetlb_unregister_node(struct node *node)
 	struct node_hstate *nhs = &node_hstates[node->dev.id];
 
 	if (!nhs->hugepages_kobj)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;		/* no hstate attributes */
+}
 
 	for_each_hstate(h) {
 		int idx = hstate_index(h);
@@ -2703,7 +2826,9 @@ static void hugetlb_register_node(struct node *node)
 	int err;
 
 	if (nhs->hugepages_kobj)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;		/* already allocated */
+}
 
 	nhs->hugepages_kobj = kobject_create_and_add("hugepages",
 							&node->dev.kobj);
@@ -2732,6 +2857,7 @@ static void __init hugetlb_register_all_nodes(void)
 {
 	int nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node_state(nid, N_MEMORY) {
 		struct node *node = node_devices[nid];
 		if (node->dev.id == nid)
@@ -2764,10 +2890,13 @@ static int __init hugetlb_init(void)
 	int i;
 
 	if (!hugepages_supported())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (!size_to_hstate(default_hstate_size)) {
 		if (default_hstate_size != 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_err("HugeTLB: unsupported default_hugepagesz %lu. Reverting to %lu\n",
 			       default_hstate_size, HPAGE_SIZE);
 		}
@@ -2778,8 +2907,11 @@ static int __init hugetlb_init(void)
 	}
 	default_hstate_idx = hstate_index(size_to_hstate(default_hstate_size));
 	if (default_hstate_max_huge_pages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!default_hstate.max_huge_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			default_hstate.max_huge_pages = default_hstate_max_huge_pages;
+}
 	}
 
 	hugetlb_init_hstates();
@@ -2801,6 +2933,7 @@ static int __init hugetlb_init(void)
 
 	for (i = 0; i < num_fault_mutexes; i++)
 		mutex_init(&hugetlb_fault_mutex_table[i]);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 subsys_initcall(hugetlb_init);
@@ -2808,6 +2941,7 @@ subsys_initcall(hugetlb_init);
 /* Should be called on processing a hugepagesz=... option */
 void __init hugetlb_bad_size(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	parsed_valid_hugepagesz = false;
 }
 
@@ -2817,6 +2951,7 @@ void __init hugetlb_add_hstate(unsigned int order)
 	unsigned long i;
 
 	if (size_to_hstate(PAGE_SIZE << order)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("hugepagesz= specified twice, ignoring\n");
 		return;
 	}
@@ -2844,6 +2979,7 @@ static int __init hugetlb_nrpages_setup(char *s)
 	static unsigned long *last_mhp;
 
 	if (!parsed_valid_hugepagesz) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("hugepages = %s preceded by "
 			"an unsupported hugepagesz, ignoring\n", s);
 		parsed_valid_hugepagesz = true;
@@ -2882,6 +3018,7 @@ __setup("hugepages=", hugetlb_nrpages_setup);
 
 static int __init hugetlb_default_setup(char *s)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	default_hstate_size = memparse(s, &s);
 	return 1;
 }
@@ -2892,6 +3029,7 @@ static unsigned int cpuset_mems_nr(unsigned int *array)
 	int node;
 	unsigned int nr = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node_mask(node, cpuset_current_mems_allowed)
 		nr += array[node];
 
@@ -2907,6 +3045,7 @@ static int hugetlb_sysctl_handler_common(bool obey_mempolicy,
 	unsigned long tmp = h->max_huge_pages;
 	int ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!hugepages_supported())
 		return -EOPNOTSUPP;
 
@@ -2927,6 +3066,7 @@ int hugetlb_sysctl_handler(struct ctl_table *table, int write,
 			  void __user *buffer, size_t *length, loff_t *ppos)
 {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return hugetlb_sysctl_handler_common(false, table, write,
 							buffer, length, ppos);
 }
@@ -2935,6 +3075,7 @@ int hugetlb_sysctl_handler(struct ctl_table *table, int write,
 int hugetlb_mempolicy_sysctl_handler(struct ctl_table *table, int write,
 			  void __user *buffer, size_t *length, loff_t *ppos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return hugetlb_sysctl_handler_common(true, table, write,
 							buffer, length, ppos);
 }
@@ -2948,6 +3089,7 @@ int hugetlb_overcommit_handler(struct ctl_table *table, int write,
 	unsigned long tmp;
 	int ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!hugepages_supported())
 		return -EOPNOTSUPP;
 
@@ -2977,7 +3119,9 @@ void hugetlb_report_meminfo(struct seq_file *m)
 {
 	struct hstate *h = &default_hstate;
 	if (!hugepages_supported())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	seq_printf(m,
 			"HugePages_Total:   %5lu\n"
 			"HugePages_Free:    %5lu\n"
@@ -2994,6 +3138,7 @@ void hugetlb_report_meminfo(struct seq_file *m)
 int hugetlb_report_node_meminfo(int nid, char *buf)
 {
 	struct hstate *h = &default_hstate;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!hugepages_supported())
 		return 0;
 	return sprintf(buf,
@@ -3010,6 +3155,7 @@ void hugetlb_show_meminfo(void)
 	struct hstate *h;
 	int nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!hugepages_supported())
 		return;
 
@@ -3066,15 +3212,20 @@ static int hugetlb_acct_memory(struct hstate *h, long delta)
 		if (gather_surplus_pages(h, delta) < 0)
 			goto out;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (delta > cpuset_mems_nr(h->free_huge_pages_node)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return_unused_surplus_pages(h, delta);
 			goto out;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = 0;
 	if (delta < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return_unused_surplus_pages(h, (unsigned long) -delta);
+}
 
 out:
 	spin_unlock(&hugetlb_lock);
@@ -3083,6 +3234,7 @@ static int hugetlb_acct_memory(struct hstate *h, long delta)
 
 static void hugetlb_vm_op_open(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct resv_map *resv = vma_resv_map(vma);
 
 	/*
@@ -3099,6 +3251,7 @@ static void hugetlb_vm_op_open(struct vm_area_struct *vma)
 
 static void hugetlb_vm_op_close(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hstate *h = hstate_vma(vma);
 	struct resv_map *resv = vma_resv_map(vma);
 	struct hugepage_subpool *spool = subpool_vma(vma);
@@ -3127,6 +3280,7 @@ static void hugetlb_vm_op_close(struct vm_area_struct *vma)
 
 static int hugetlb_vm_op_split(struct vm_area_struct *vma, unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (addr & ~(huge_page_mask(hstate_vma(vma))))
 		return -EINVAL;
 	return 0;
@@ -3140,6 +3294,7 @@ static int hugetlb_vm_op_split(struct vm_area_struct *vma, unsigned long addr)
  */
 static int hugetlb_vm_op_fault(struct vm_fault *vmf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG();
 	return 0;
 }
@@ -3157,6 +3312,7 @@ static pte_t make_huge_pte(struct vm_area_struct *vma, struct page *page,
 	pte_t entry;
 
 	if (writable) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = huge_pte_mkwrite(huge_pte_mkdirty(mk_huge_pte(page,
 					 vma->vm_page_prot)));
 	} else {
@@ -3177,13 +3333,16 @@ static void set_huge_ptep_writable(struct vm_area_struct *vma,
 
 	entry = huge_pte_mkwrite(huge_pte_mkdirty(huge_ptep_get(ptep)));
 	if (huge_ptep_set_access_flags(vma, address, ptep, entry, 1))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		update_mmu_cache(vma, address, ptep);
 }
+}
 
 bool is_hugetlb_entry_migration(pte_t pte)
 {
 	swp_entry_t swp;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (huge_pte_none(pte) || pte_present(pte))
 		return false;
 	swp = pte_to_swp_entry(pte);
@@ -3197,6 +3356,7 @@ static int is_hugetlb_entry_hwpoisoned(pte_t pte)
 {
 	swp_entry_t swp;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (huge_pte_none(pte) || pte_present(pte))
 		return 0;
 	swp = pte_to_swp_entry(pte);
@@ -3224,7 +3384,9 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 	mmun_start = vma->vm_start;
 	mmun_end = vma->vm_end;
 	if (cow)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);
+}
 
 	for (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {
 		spinlock_t *src_ptl, *dst_ptl;
@@ -3300,6 +3462,7 @@ void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
 	const unsigned long mmun_start = start;	/* For mmu_notifiers */
 	const unsigned long mmun_end   = end;	/* For mmu_notifiers */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON(!is_vm_hugetlb_page(vma));
 	BUG_ON(start & ~huge_page_mask(h));
 	BUG_ON(end & ~huge_page_mask(h));
@@ -3382,6 +3545,7 @@ void __unmap_hugepage_range_final(struct mmu_gather *tlb,
 			  struct vm_area_struct *vma, unsigned long start,
 			  unsigned long end, struct page *ref_page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__unmap_hugepage_range(tlb, vma, start, end, ref_page);
 
 	/*
@@ -3419,6 +3583,7 @@ void unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start,
 static void unmap_ref_private(struct mm_struct *mm, struct vm_area_struct *vma,
 			      struct page *page, unsigned long address)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hstate *h = hstate_vma(vma);
 	struct vm_area_struct *iter_vma;
 	struct address_space *mapping;
@@ -3627,7 +3792,9 @@ static bool hugetlbfs_pagecache_present(struct hstate *h,
 
 	page = find_get_page(mapping, idx);
 	if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_page(page);
+}
 	return page != NULL;
 }
 
@@ -3639,7 +3806,9 @@ int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 	int err = add_to_page_cache(page, mapping, idx, GFP_KERNEL);
 
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return err;
+}
 	ClearPagePrivate(page);
 
 	spin_lock(&inode->i_lock);
@@ -3652,6 +3821,7 @@ static int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,
 			   struct address_space *mapping, pgoff_t idx,
 			   unsigned long address, pte_t *ptep, unsigned int flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hstate *h = hstate_vma(vma);
 	int ret = VM_FAULT_SIGBUS;
 	int anon_rmap = 0;
@@ -3818,6 +3988,7 @@ u32 hugetlb_fault_mutex_hash(struct hstate *h, struct mm_struct *mm,
 	u32 hash;
 
 	if (vma->vm_flags & VM_SHARED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		key[0] = (unsigned long) mapping;
 		key[1] = idx;
 	} else {
@@ -3861,6 +4032,7 @@ int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 
 	ptep = huge_pte_offset(mm, address, huge_page_size(h));
 	if (ptep) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = huge_ptep_get(ptep);
 		if (unlikely(is_hugetlb_entry_migration(entry))) {
 			migration_entry_wait_huge(vma, mm, ptep);
@@ -4003,6 +4175,7 @@ int hugetlb_mcopy_atomic_pte(struct mm_struct *dst_mm,
 	struct page *page;
 
 	if (!*pagep) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = -ENOMEM;
 		page = alloc_huge_page(dst_vma, dst_addr, 0);
 		if (IS_ERR(page))
@@ -4123,6 +4296,7 @@ long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	struct hstate *h = hstate_vma(vma);
 	int err = -EFAULT;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (vaddr < vma->vm_end && remainder) {
 		pte_t *pte;
 		spinlock_t *ptl = NULL;
@@ -4274,6 +4448,7 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,
 	struct hstate *h = hstate_vma(vma);
 	unsigned long pages = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(address >= end);
 	flush_cache_range(vma, address, end);
 
@@ -4350,7 +4525,9 @@ int hugetlb_reserve_pages(struct inode *inode,
 	 * without using reserves
 	 */
 	if (vm_flags & VM_NORESERVE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * Shared mappings base their reservation on the number of pages that
@@ -4359,15 +4536,20 @@ int hugetlb_reserve_pages(struct inode *inode,
 	 * called to make the mapping read-write. Assume !vma is a shm mapping
 	 */
 	if (!vma || vma->vm_flags & VM_MAYSHARE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		resv_map = inode_resv_map(inode);
 
 		chg = region_chg(resv_map, from, to);
 
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		resv_map = resv_map_alloc();
 		if (!resv_map)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		chg = to - from;
 
 		set_vma_resv_map(vma, resv_map);
@@ -4375,6 +4557,7 @@ int hugetlb_reserve_pages(struct inode *inode,
 	}
 
 	if (chg < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = chg;
 		goto out_err;
 	}
@@ -4386,6 +4569,7 @@ int hugetlb_reserve_pages(struct inode *inode,
 	 */
 	gbl_reserve = hugepage_subpool_get_pages(spool, chg);
 	if (gbl_reserve < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = -ENOSPC;
 		goto out_err;
 	}
@@ -4413,6 +4597,7 @@ int hugetlb_reserve_pages(struct inode *inode,
 	 * else has to be done for private mappings here
 	 */
 	if (!vma || vma->vm_flags & VM_MAYSHARE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		long add = region_add(resv_map, from, to);
 
 		if (unlikely(chg > add)) {
@@ -4430,6 +4615,7 @@ int hugetlb_reserve_pages(struct inode *inode,
 			hugetlb_acct_memory(h, -rsv_adjust);
 		}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 out_err:
 	if (!vma || vma->vm_flags & VM_MAYSHARE)
@@ -4437,13 +4623,16 @@ int hugetlb_reserve_pages(struct inode *inode,
 		if (chg >= 0)
 			region_abort(resv_map, from, to);
 	if (vma && is_vma_resv_set(vma, HPAGE_RESV_OWNER))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kref_put(&resv_map->refs, resv_map_release);
+}
 	return ret;
 }
 
 long hugetlb_unreserve_pages(struct inode *inode, long start, long end,
 								long freed)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hstate *h = hstate_inode(inode);
 	struct resv_map *resv_map = inode_resv_map(inode);
 	long chg = 0;
@@ -4458,9 +4647,12 @@ long hugetlb_unreserve_pages(struct inode *inode, long start, long end,
 		 * allocated.  If end == LONG_MAX, it will not fail.
 		 */
 		if (chg < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return chg;
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&inode->i_lock);
 	inode->i_blocks -= (blocks_per_huge_page(h) * freed);
 	spin_unlock(&inode->i_lock);
@@ -4526,6 +4718,7 @@ static bool vma_shareable(struct vm_area_struct *vma, unsigned long addr)
  */
 pte_t *huge_pmd_share(struct mm_struct *mm, unsigned long addr, pud_t *pud)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct vm_area_struct *vma = find_vma(mm, addr);
 	struct address_space *mapping = vma->vm_file->f_mapping;
 	pgoff_t idx = ((addr - vma->vm_start) >> PAGE_SHIFT) +
@@ -4591,6 +4784,7 @@ int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep)
 	p4d_t *p4d = p4d_offset(pgd, *addr);
 	pud_t *pud = pud_offset(p4d, *addr);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(page_count(virt_to_page(ptep)) == 0);
 	if (page_count(virt_to_page(ptep)) == 1)
 		return 0;
@@ -4627,7 +4821,9 @@ pte_t *huge_pte_alloc(struct mm_struct *mm,
 	pgd = pgd_offset(mm, addr);
 	p4d = p4d_alloc(mm, pgd, addr);
 	if (!p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	pud = pud_alloc(mm, p4d, addr);
 	if (pud) {
 		if (sz == PUD_SIZE) {
@@ -4664,7 +4860,9 @@ pte_t *huge_pte_offset(struct mm_struct *mm,
 
 	pgd = pgd_offset(mm, addr);
 	if (!pgd_present(*pgd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	p4d = p4d_offset(pgd, addr);
 	if (!p4d_present(*p4d))
 		return NULL;
@@ -4703,6 +4901,7 @@ struct page * __weak
 follow_huge_pd(struct vm_area_struct *vma,
 	       unsigned long address, hugepd_t hpd, int flags, int pdshift)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN(1, "hugepd follow called with no support for hugepage directory format\n");
 	return NULL;
 }
@@ -4748,6 +4947,7 @@ struct page * __weak
 follow_huge_pud(struct mm_struct *mm, unsigned long address,
 		pud_t *pud, int flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (flags & FOLL_GET)
 		return NULL;
 
@@ -4757,6 +4957,7 @@ follow_huge_pud(struct mm_struct *mm, unsigned long address,
 struct page * __weak
 follow_huge_pgd(struct mm_struct *mm, unsigned long address, pgd_t *pgd, int flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (flags & FOLL_GET)
 		return NULL;
 
@@ -4769,6 +4970,7 @@ bool isolate_huge_page(struct page *page, struct list_head *list)
 
 	VM_BUG_ON_PAGE(!PageHead(page), page);
 	spin_lock(&hugetlb_lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!page_huge_active(page) || !get_page_unless_zero(page)) {
 		ret = false;
 		goto unlock;
@@ -4782,6 +4984,7 @@ bool isolate_huge_page(struct page *page, struct list_head *list)
 
 void putback_active_hugepage(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(!PageHead(page), page);
 	spin_lock(&hugetlb_lock);
 	set_page_huge_active(page);
diff --git a/mm/hugetlb_cgroup.c b/mm/hugetlb_cgroup.c
index eec1150..7937c8c 100644
--- a/mm/hugetlb_cgroup.c
+++ b/mm/hugetlb_cgroup.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *
  * Copyright IBM Corporation, 2012
diff --git a/mm/internal.h b/mm/internal.h
index 1df011f..974f701 100644
--- a/mm/internal.h
+++ b/mm/internal.h
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* internal.h: mm/ internal definitions
  *
  * Copyright (C) 2004 Red Hat, Inc. All Rights Reserved.
diff --git a/mm/interval_tree.c b/mm/interval_tree.c
index b476643..63a1f26 100644
--- a/mm/interval_tree.c
+++ b/mm/interval_tree.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/interval_tree.c - interval tree for mapping->i_mmap
  *
@@ -13,6 +15,7 @@
 
 static inline unsigned long vma_start_pgoff(struct vm_area_struct *v)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return v->vm_pgoff;
 }
 
@@ -37,6 +40,7 @@ void vma_interval_tree_insert_after(struct vm_area_struct *node,
 	VM_BUG_ON_VMA(vma_start_pgoff(node) != vma_start_pgoff(prev), node);
 
 	if (!prev->shared.rb.rb_right) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		parent = prev;
 		link = &prev->shared.rb.rb_right;
 	} else {
@@ -61,6 +65,7 @@ void vma_interval_tree_insert_after(struct vm_area_struct *node,
 
 static inline unsigned long avc_start_pgoff(struct anon_vma_chain *avc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma_start_pgoff(avc->vma);
 }
 
@@ -93,6 +98,7 @@ struct anon_vma_chain *
 anon_vma_interval_tree_iter_first(struct rb_root_cached *root,
 				  unsigned long first, unsigned long last)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __anon_vma_interval_tree_iter_first(root, first, last);
 }
 
@@ -100,6 +106,7 @@ struct anon_vma_chain *
 anon_vma_interval_tree_iter_next(struct anon_vma_chain *node,
 				 unsigned long first, unsigned long last)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __anon_vma_interval_tree_iter_next(node, first, last);
 }
 
diff --git a/mm/khugepaged.c b/mm/khugepaged.c
index 43cb304..51975ec 100644
--- a/mm/khugepaged.c
+++ b/mm/khugepaged.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
diff --git a/mm/ksm.c b/mm/ksm.c
index 6cb60f4..0f675b3 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Memory merging support.
  *
diff --git a/mm/list_lru.c b/mm/list_lru.c
index f141f0c..fc98764 100644
--- a/mm/list_lru.c
+++ b/mm/list_lru.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Copyright (c) 2013 Red Hat, Inc. and Parallels Inc. All rights reserved.
  * Authors: David Chinner and Glauber Costa
@@ -90,24 +92,28 @@ list_lru_from_kmem(struct list_lru_node *nlru, void *ptr)
 #else
 static inline bool list_lru_memcg_aware(struct list_lru *lru)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
 static inline struct list_lru_one *
 list_lru_from_memcg_idx(struct list_lru_node *nlru, int idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return &nlru->lru;
 }
 
 static inline struct list_lru_one *
 list_lru_from_kmem(struct list_lru_node *nlru, void *ptr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return &nlru->lru;
 }
 #endif /* CONFIG_MEMCG && !CONFIG_SLOB */
 
 bool list_lru_add(struct list_lru *lru, struct list_head *item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int nid = page_to_nid(virt_to_page(item));
 	struct list_lru_node *nlru = &lru->node[nid];
 	struct list_lru_one *l;
@@ -121,6 +127,7 @@ bool list_lru_add(struct list_lru *lru, struct list_head *item)
 		spin_unlock(&nlru->lock);
 		return true;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&nlru->lock);
 	return false;
 }
@@ -128,6 +135,7 @@ EXPORT_SYMBOL_GPL(list_lru_add);
 
 bool list_lru_del(struct list_lru *lru, struct list_head *item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int nid = page_to_nid(virt_to_page(item));
 	struct list_lru_node *nlru = &lru->node[nid];
 	struct list_lru_one *l;
@@ -141,6 +149,7 @@ bool list_lru_del(struct list_lru *lru, struct list_head *item)
 		spin_unlock(&nlru->lock);
 		return true;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&nlru->lock);
 	return false;
 }
@@ -148,6 +157,7 @@ EXPORT_SYMBOL_GPL(list_lru_del);
 
 void list_lru_isolate(struct list_lru_one *list, struct list_head *item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_del_init(item);
 	list->nr_items--;
 }
@@ -156,6 +166,7 @@ EXPORT_SYMBOL_GPL(list_lru_isolate);
 void list_lru_isolate_move(struct list_lru_one *list, struct list_head *item,
 			   struct list_head *head)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_move(item, head);
 	list->nr_items--;
 }
@@ -179,6 +190,7 @@ static unsigned long __list_lru_count_one(struct list_lru *lru,
 unsigned long list_lru_count_one(struct list_lru *lru,
 				 int nid, struct mem_cgroup *memcg)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __list_lru_count_one(lru, nid, memcg_cache_id(memcg));
 }
 EXPORT_SYMBOL_GPL(list_lru_count_one);
@@ -215,6 +227,7 @@ __list_lru_walk_one(struct list_lru *lru, int nid, int memcg_idx,
 		 */
 		if (!*nr_to_walk)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		--*nr_to_walk;
 
 		ret = isolate(item, l, &nlru->lock, cb_arg);
@@ -249,6 +262,7 @@ __list_lru_walk_one(struct list_lru *lru, int nid, int memcg_idx,
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&nlru->lock);
 	return isolated;
 }
@@ -258,6 +272,7 @@ list_lru_walk_one(struct list_lru *lru, int nid, struct mem_cgroup *memcg,
 		  list_lru_walk_cb isolate, void *cb_arg,
 		  unsigned long *nr_to_walk)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __list_lru_walk_one(lru, nid, memcg_cache_id(memcg),
 				   isolate, cb_arg, nr_to_walk);
 }
@@ -521,6 +536,7 @@ void memcg_drain_all_list_lrus(int src_idx, int dst_idx)
 #else
 static int memcg_init_list_lru(struct list_lru *lru, bool memcg_aware)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -544,13 +560,17 @@ int __list_lru_init(struct list_lru *lru, bool memcg_aware,
 
 	for_each_node(i) {
 		spin_lock_init(&lru->node[i].lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (key)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			lockdep_set_class(&lru->node[i].lock, key);
+}
 		init_one_lru(&lru->node[i].lru);
 	}
 
 	err = memcg_init_list_lru(lru, memcg_aware);
 	if (err) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(lru->node);
 		/* Do this so a list_lru_destroy() doesn't crash: */
 		lru->node = NULL;
@@ -568,7 +588,9 @@ void list_lru_destroy(struct list_lru *lru)
 {
 	/* Already destroyed or not yet initialized? */
 	if (!lru->node)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	memcg_get_cache_ids();
 
diff --git a/mm/maccess.c b/mm/maccess.c
index 78f9274..590de45 100644
--- a/mm/maccess.c
+++ b/mm/maccess.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Access kernel memory without faulting.
  */
@@ -85,6 +87,7 @@ EXPORT_SYMBOL_GPL(probe_kernel_write);
  */
 long strncpy_from_unsafe(char *dst, const void *unsafe_addr, long count)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mm_segment_t old_fs = get_fs();
 	const void *src = unsafe_addr;
 	long ret;
diff --git a/mm/madvise.c b/mm/madvise.c
index 751e97a..a5729a8 100644
--- a/mm/madvise.c
+++ b/mm/madvise.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *	linux/mm/madvise.c
@@ -36,6 +38,7 @@
  */
 static int madvise_need_mmap_write(int behavior)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	switch (behavior) {
 	case MADV_REMOVE:
 	case MADV_WILLNEED:
@@ -76,17 +79,21 @@ static long madvise_behavior(struct vm_area_struct *vma,
 		break;
 	case MADV_DOFORK:
 		if (vma->vm_flags & VM_IO) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EINVAL;
 			goto out;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		new_flags &= ~VM_DONTCOPY;
 		break;
 	case MADV_WIPEONFORK:
 		/* MADV_WIPEONFORK is only supported on anonymous memory. */
 		if (vma->vm_file || vma->vm_flags & VM_SHARED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EINVAL;
 			goto out;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		new_flags |= VM_WIPEONFORK;
 		break;
 	case MADV_KEEPONFORK:
@@ -97,6 +104,7 @@ static long madvise_behavior(struct vm_area_struct *vma,
 		break;
 	case MADV_DODUMP:
 		if (new_flags & VM_SPECIAL) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EINVAL;
 			goto out;
 		}
@@ -111,7 +119,9 @@ static long madvise_behavior(struct vm_area_struct *vma,
 			 * slab, are temporarily unavailable.
 			 */
 			if (error == -ENOMEM)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = -EAGAIN;
+}
 			goto out;
 		}
 		break;
@@ -124,7 +134,9 @@ static long madvise_behavior(struct vm_area_struct *vma,
 			 * slab, are temporarily unavailable.
 			 */
 			if (error == -ENOMEM)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = -EAGAIN;
+}
 			goto out;
 		}
 		break;
@@ -140,6 +152,7 @@ static long madvise_behavior(struct vm_area_struct *vma,
 			  vma->vm_file, pgoff, vma_policy(vma),
 			  vma->vm_userfaultfd_ctx);
 	if (*prev) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vma = *prev;
 		goto success;
 	}
@@ -148,6 +161,7 @@ static long madvise_behavior(struct vm_area_struct *vma,
 
 	if (start != vma->vm_start) {
 		if (unlikely(mm->map_count >= sysctl_max_map_count)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -ENOMEM;
 			goto out;
 		}
@@ -158,16 +172,21 @@ static long madvise_behavior(struct vm_area_struct *vma,
 			 * slab, are temporarily unavailable.
 			 */
 			if (error == -ENOMEM)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = -EAGAIN;
+}
 			goto out;
 		}
 	}
 
 	if (end != vma->vm_end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(mm->map_count >= sysctl_max_map_count)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -ENOMEM;
 			goto out;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = __split_vma(mm, vma, end, 0);
 		if (error) {
 			/*
@@ -175,7 +194,9 @@ static long madvise_behavior(struct vm_area_struct *vma,
 			 * slab, are temporarily unavailable.
 			 */
 			if (error == -ENOMEM)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = -EAGAIN;
+}
 			goto out;
 		}
 	}
@@ -198,7 +219,9 @@ static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,
 	unsigned long index;
 
 	if (pmd_none_or_trans_huge_or_clear_bad(pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	for (index = start; index != end; index += PAGE_SIZE) {
 		pte_t pte;
@@ -208,20 +231,26 @@ static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,
 
 		orig_pte = pte_offset_map_lock(vma->vm_mm, pmd, start, &ptl);
 		pte = *(orig_pte + ((index - start) / PAGE_SIZE));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_unmap_unlock(orig_pte, ptl);
 
 		if (pte_present(pte) || pte_none(pte))
 			continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_to_swp_entry(pte);
 		if (unlikely(non_swap_entry(entry)))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = read_swap_cache_async(entry, GFP_HIGHUSER_MOVABLE,
 							vma, index, false);
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -247,6 +276,7 @@ static void force_shm_swapin_readahead(struct vm_area_struct *vma,
 	struct page *page;
 	swp_entry_t swap;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (; start < end; start += PAGE_SIZE) {
 		index = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 
@@ -283,7 +313,9 @@ static long madvise_willneed(struct vm_area_struct *vma,
 		return 0;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (shmem_mapping(file->f_mapping)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		force_shm_swapin_readahead(vma, start, end,
 					file->f_mapping);
 		return 0;
@@ -293,14 +325,19 @@ static long madvise_willneed(struct vm_area_struct *vma,
 		return -EBADF;
 #endif
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (IS_DAX(file_inode(file))) {
 		/* no bad return value, but ignore advice */
 		return 0;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	start = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 	if (end > vma->vm_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		end = vma->vm_end;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	end = ((end - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 
 	force_page_cache_readahead(file->f_mapping, file, start, end - start);
@@ -322,12 +359,17 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
 
 	next = pmd_addr_end(addr, end);
 	if (pmd_trans_huge(*pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))
 			goto next;
+}
 
 	if (pmd_trans_unstable(pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	tlb_remove_check_page_size_change(tlb, PAGE_SIZE);
 	orig_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
 	flush_tlb_batched_pending(mm);
@@ -348,6 +390,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
 			entry = pte_to_swp_entry(ptent);
 			if (non_swap_entry(entry))
 				continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nr_swap--;
 			free_swap_and_cache(entry);
 			pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
@@ -364,20 +407,27 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
 		 * deactivate all pages.
 		 */
 		if (PageTransCompound(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (page_mapcount(page) != 1)
 				goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			get_page(page);
 			if (!trylock_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				put_page(page);
 				goto out;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pte_unmap_unlock(orig_pte, ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (split_huge_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				unlock_page(page);
 				put_page(page);
 				pte_offset_map_lock(mm, pmd, addr, &ptl);
 				goto out;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 			put_page(page);
 			pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
@@ -386,9 +436,11 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
 			continue;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(PageTransCompound(page), page);
 
 		if (PageSwapCache(page) || PageDirty(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!trylock_page(page))
 				continue;
 			/*
@@ -396,15 +448,19 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
 			 * PG_dirty of the page.
 			 */
 			if (page_mapcount(page) != 1) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				unlock_page(page);
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (PageSwapCache(page) && !try_to_free_swap(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				unlock_page(page);
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ClearPageDirty(page);
 			unlock_page(page);
 		}
@@ -428,12 +484,17 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
 	}
 out:
 	if (nr_swap) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (current->mm == mm)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			sync_mm_rss(mm);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		add_mm_counter(mm, MM_SWAPENTS, nr_swap);
 	}
 	arch_leave_lazy_mmu_mode();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_unmap_unlock(orig_pte, ptl);
 	cond_resched();
 next:
@@ -450,8 +511,10 @@ static void madvise_free_page_range(struct mmu_gather *tlb,
 		.private = tlb,
 	};
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	tlb_start_vma(tlb, vma);
 	walk_page_range(addr, end, &free_walk);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	tlb_end_vma(tlb, vma);
 }
 
@@ -464,14 +527,20 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,
 
 	/* MADV_FREE works for only anon vma at the moment */
 	if (!vma_is_anonymous(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	start = max(vma->vm_start, start_addr);
 	if (start >= vma->vm_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	end = min(vma->vm_end, end_addr);
 	if (end <= vma->vm_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	lru_add_drain();
 	tlb_gather_mmu(&tlb, mm, start, end);
@@ -518,15 +587,22 @@ static long madvise_dontneed_free(struct vm_area_struct *vma,
 {
 	*prev = vma;
 	if (!can_madv_dontneed_vma(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!userfaultfd_remove(vma, start, end)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*prev = NULL; /* mmap_sem has been dropped, prev is stale */
 
 		down_read(&current->mm->mmap_sem);
 		vma = find_vma(current->mm, start);
 		if (!vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (start < vma->vm_start) {
 			/*
 			 * This "vma" under revalidation is the one
@@ -539,8 +615,12 @@ static long madvise_dontneed_free(struct vm_area_struct *vma,
 			 */
 			return -ENOMEM;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!can_madv_dontneed_vma(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EINVAL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (end > vma->vm_end) {
 			/*
 			 * Don't fail if end > vma->vm_end. If the old
@@ -556,6 +636,7 @@ static long madvise_dontneed_free(struct vm_area_struct *vma,
 			 */
 			end = vma->vm_end;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_WARN_ON(start >= end);
 	}
 
@@ -582,7 +663,9 @@ static long madvise_remove(struct vm_area_struct *vma,
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
 	if (vma->vm_flags & VM_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	f = vma->vm_file;
 
@@ -694,6 +777,7 @@ madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,
 static bool
 madvise_behavior_valid(int behavior)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	switch (behavior) {
 	case MADV_DOFORK:
 	case MADV_DONTFORK:
@@ -799,23 +883,34 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 	struct blk_plug plug;
 
 	if (!madvise_behavior_valid(behavior))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	if (start & ~PAGE_MASK)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 	len = (len_in + ~PAGE_MASK) & PAGE_MASK;
 
 	/* Check to see whether len was rounded up from small -ve to zero */
 	if (len_in && !len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	end = start + len;
 	if (end < start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	error = 0;
 	if (end == start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 #ifdef CONFIG_MEMORY_FAILURE
 	if (behavior == MADV_HWPOISON || behavior == MADV_SOFT_OFFLINE)
@@ -825,7 +920,9 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 	write = madvise_need_mmap_write(behavior);
 	if (write) {
 		if (down_write_killable(&current->mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EINTR;
+}
 	} else {
 		down_read(&current->mm->mmap_sem);
 	}
@@ -848,6 +945,7 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 
 		/* Here start < (end|vma->vm_end). */
 		if (start < vma->vm_start) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unmapped_error = -ENOMEM;
 			start = vma->vm_start;
 			if (start >= end)
@@ -857,15 +955,21 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)
 		/* Here vma->vm_start <= start < (end|vma->vm_end) */
 		tmp = vma->vm_end;
 		if (end < tmp)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			tmp = end;
+}
 
 		/* Here vma->vm_start <= start < tmp <= (end|vma->vm_end). */
 		error = madvise_vma(vma, &prev, start, tmp, behavior);
 		if (error)
 			goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start = tmp;
 		if (prev && start < prev->vm_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			start = prev->vm_end;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = unmapped_error;
 		if (start >= end)
 			goto out;
diff --git a/mm/memblock.c b/mm/memblock.c
index 9120578..5ad93ca 100644
--- a/mm/memblock.c
+++ b/mm/memblock.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Procedures for maintaining information about logical memory blocks.
  *
@@ -76,6 +78,7 @@ static inline phys_addr_t memblock_cap_size(phys_addr_t base, phys_addr_t *size)
 static unsigned long __init_memblock memblock_addrs_overlap(phys_addr_t base1, phys_addr_t size1,
 				       phys_addr_t base2, phys_addr_t size2)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ((base1 < (base2 + size2)) && (base2 < (base1 + size1)));
 }
 
@@ -84,6 +87,7 @@ bool __init_memblock memblock_overlaps_region(struct memblock_type *type,
 {
 	unsigned long i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < type->cnt; i++)
 		if (memblock_addrs_overlap(base, size, type->regions[i].base,
 					   type->regions[i].size))
@@ -113,6 +117,7 @@ __memblock_find_range_bottom_up(phys_addr_t start, phys_addr_t end,
 	phys_addr_t this_start, this_end, cand;
 	u64 i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_free_mem_range(i, nid, flags, &this_start, &this_end, NULL) {
 		this_start = clamp(this_start, start, end);
 		this_end = clamp(this_end, start, end);
@@ -157,9 +162,12 @@ __memblock_find_range_top_down(phys_addr_t start, phys_addr_t end,
 
 		cand = round_down(this_end - size, align);
 		if (cand >= this_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return cand;
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -214,7 +222,9 @@ phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,
 		ret = __memblock_find_range_bottom_up(bottom_up_start, end,
 						      size, align, nid, flags);
 		if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 
 		/*
 		 * we always limit bottom-up allocation above the kernel,
@@ -257,6 +267,7 @@ phys_addr_t __init_memblock memblock_find_in_range(phys_addr_t start,
 					    NUMA_NO_NODE, flags);
 
 	if (!ret && (flags & MEMBLOCK_MIRROR)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("Could not allocate %pap bytes of mirrored memory\n",
 			&size);
 		flags &= ~MEMBLOCK_MIRROR;
@@ -275,6 +286,7 @@ static void __init_memblock memblock_remove_region(struct memblock_type *type, u
 
 	/* Special case for empty arrays */
 	if (type->cnt == 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON(type->total_size != 0);
 		type->cnt = 1;
 		type->regions[0].base = 0;
@@ -293,6 +305,7 @@ void __init memblock_discard(void)
 	phys_addr_t addr, size;
 
 	if (memblock.reserved.regions != memblock_reserved_init_regions) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		addr = __pa(memblock.reserved.regions);
 		size = PAGE_ALIGN(sizeof(struct memblock_region) *
 				  memblock.reserved.max);
@@ -300,6 +313,7 @@ void __init memblock_discard(void)
 	}
 
 	if (memblock.memory.regions != memblock_memory_init_regions) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		addr = __pa(memblock.memory.regions);
 		size = PAGE_ALIGN(sizeof(struct memblock_region) *
 				  memblock.memory.max);
@@ -337,7 +351,9 @@ static int __init_memblock memblock_double_array(struct memblock_type *type,
 	 * of memory that aren't suitable for allocation
 	 */
 	if (!memblock_can_resize)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	/* Calculate new doubled size */
 	old_size = type->max * sizeof(struct memblock_region);
@@ -512,7 +528,9 @@ int __init_memblock memblock_add_range(struct memblock_type *type,
 	struct memblock_region *rgn;
 
 	if (!size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* special case for empty array */
 	if (type->regions[0].size == 0) {
@@ -569,7 +587,9 @@ int __init_memblock memblock_add_range(struct memblock_type *type,
 	}
 
 	if (!nr_new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * If this was the first round, resize array and repeat for actual
@@ -578,7 +598,10 @@ int __init_memblock memblock_add_range(struct memblock_type *type,
 	if (!insert) {
 		while (type->cnt + nr_new > type->max)
 			if (memblock_double_array(type, obase, size) < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENOMEM;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		insert = true;
 		goto repeat;
 	} else {
@@ -590,6 +613,7 @@ int __init_memblock memblock_add_range(struct memblock_type *type,
 int __init_memblock memblock_add_node(phys_addr_t base, phys_addr_t size,
 				       int nid)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_add_range(&memblock.memory, base, size, nid, 0);
 }
 
@@ -630,12 +654,16 @@ static int __init_memblock memblock_isolate_range(struct memblock_type *type,
 	*start_rgn = *end_rgn = 0;
 
 	if (!size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* we'll create at most two more regions */
 	while (type->cnt + 2 > type->max)
 		if (memblock_double_array(type, base, size) < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 
 	for_each_memblock_type(type, rgn) {
 		phys_addr_t rbase = rgn->base;
@@ -676,6 +704,7 @@ static int __init_memblock memblock_isolate_range(struct memblock_type *type,
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -687,15 +716,19 @@ static int __init_memblock memblock_remove_range(struct memblock_type *type,
 
 	ret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	for (i = end_rgn - 1; i >= start_rgn; i--)
 		memblock_remove_region(type, i);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
 int __init_memblock memblock_remove(phys_addr_t base, phys_addr_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_remove_range(&memblock.memory, base, size);
 }
 
@@ -735,11 +768,15 @@ static int __init_memblock memblock_setclr_flag(phys_addr_t base,
 
 	ret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	for (i = start_rgn; i < end_rgn; i++)
 		if (set)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			memblock_set_region_flags(&type->regions[i], flag);
+}
 		else
 			memblock_clear_region_flags(&type->regions[i], flag);
 
@@ -756,6 +793,7 @@ static int __init_memblock memblock_setclr_flag(phys_addr_t base,
  */
 int __init_memblock memblock_mark_hotplug(phys_addr_t base, phys_addr_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_setclr_flag(base, size, 1, MEMBLOCK_HOTPLUG);
 }
 
@@ -780,6 +818,7 @@ int __init_memblock memblock_clear_hotplug(phys_addr_t base, phys_addr_t size)
  */
 int __init_memblock memblock_mark_mirror(phys_addr_t base, phys_addr_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	system_has_some_mirror = true;
 
 	return memblock_setclr_flag(base, size, 1, MEMBLOCK_MIRROR);
@@ -794,6 +833,7 @@ int __init_memblock memblock_mark_mirror(phys_addr_t base, phys_addr_t size)
  */
 int __init_memblock memblock_mark_nomap(phys_addr_t base, phys_addr_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_setclr_flag(base, size, 1, MEMBLOCK_NOMAP);
 }
 
@@ -806,6 +846,7 @@ int __init_memblock memblock_mark_nomap(phys_addr_t base, phys_addr_t size)
  */
 int __init_memblock memblock_clear_nomap(phys_addr_t base, phys_addr_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_setclr_flag(base, size, 0, MEMBLOCK_NOMAP);
 }
 
@@ -904,12 +945,22 @@ void __init_memblock __next_mem_range(u64 *idx, int nid, ulong flags,
 			continue;
 
 		if (!type_b) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (out_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				*out_start = m_start;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (out_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				*out_end = m_end;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (out_nid)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				*out_nid = m_nid;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			idx_a++;
 			*idx = (u32)idx_a | (u64)idx_b << 32;
 			return;
@@ -923,6 +974,7 @@ void __init_memblock __next_mem_range(u64 *idx, int nid, ulong flags,
 
 			r = &type_b->regions[idx_b];
 			r_start = idx_b ? r[-1].base + r[-1].size : 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			r_end = idx_b < type_b->cnt ?
 				r->base : ULLONG_MAX;
 
@@ -940,7 +992,9 @@ void __init_memblock __next_mem_range(u64 *idx, int nid, ulong flags,
 				if (out_end)
 					*out_end = min(m_end, r_end);
 				if (out_nid)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					*out_nid = m_nid;
+}
 				/*
 				 * The region which ends first is
 				 * advanced for the next iteration.
@@ -986,7 +1040,9 @@ void __init_memblock __next_mem_range_rev(u64 *idx, int nid, ulong flags,
 	int idx_b = *idx >> 32;
 
 	if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nid = NUMA_NO_NODE;
+}
 
 	if (*idx == (u64)ULLONG_MAX) {
 		idx_a = type_a->cnt - 1;
@@ -1020,12 +1076,22 @@ void __init_memblock __next_mem_range_rev(u64 *idx, int nid, ulong flags,
 			continue;
 
 		if (!type_b) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (out_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				*out_start = m_start;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (out_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				*out_end = m_end;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (out_nid)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				*out_nid = m_nid;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			idx_a--;
 			*idx = (u32)idx_a | (u64)idx_b << 32;
 			return;
@@ -1055,7 +1121,9 @@ void __init_memblock __next_mem_range_rev(u64 *idx, int nid, ulong flags,
 				if (out_end)
 					*out_end = min(m_end, r_end);
 				if (out_nid)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					*out_nid = m_nid;
+}
 				if (m_start >= r_start)
 					idx_a--;
 				else
@@ -1113,7 +1181,9 @@ unsigned long __init_memblock memblock_next_valid_pfn(unsigned long pfn,
 		mid = (right + left) / 2;
 
 		if (addr < type->regions[mid].base)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			right = mid;
+}
 		else if (addr >= (type->regions[mid].base +
 				  type->regions[mid].size))
 			left = mid + 1;
@@ -1124,7 +1194,9 @@ unsigned long __init_memblock memblock_next_valid_pfn(unsigned long pfn,
 	} while (left < right);
 
 	if (right == type->cnt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return max_pfn;
+}
 	else
 		return min(PHYS_PFN(type->regions[right].base), max_pfn);
 }
@@ -1150,7 +1222,9 @@ int __init_memblock memblock_set_node(phys_addr_t base, phys_addr_t size,
 
 	ret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	for (i = start_rgn; i < end_rgn; i++)
 		memblock_set_region_node(&type->regions[i], nid);
@@ -1167,7 +1241,9 @@ static phys_addr_t __init memblock_alloc_range_nid(phys_addr_t size,
 	phys_addr_t found;
 
 	if (!align)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		align = SMP_CACHE_BYTES;
+}
 
 	found = memblock_find_in_range_node(size, align, start, end, nid,
 					    flags);
@@ -1186,6 +1262,7 @@ phys_addr_t __init memblock_alloc_range(phys_addr_t size, phys_addr_t align,
 					phys_addr_t start, phys_addr_t end,
 					ulong flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_alloc_range_nid(size, align, start, end, NUMA_NO_NODE,
 					flags);
 }
@@ -1194,11 +1271,13 @@ static phys_addr_t __init memblock_alloc_base_nid(phys_addr_t size,
 					phys_addr_t align, phys_addr_t max_addr,
 					int nid, ulong flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_alloc_range_nid(size, align, 0, max_addr, nid, flags);
 }
 
 phys_addr_t __init memblock_alloc_nid(phys_addr_t size, phys_addr_t align, int nid)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ulong flags = choose_memblock_flags();
 	phys_addr_t ret;
 
@@ -1215,6 +1294,7 @@ phys_addr_t __init memblock_alloc_nid(phys_addr_t size, phys_addr_t align, int n
 
 phys_addr_t __init __memblock_alloc_base(phys_addr_t size, phys_addr_t align, phys_addr_t max_addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_alloc_base_nid(size, align, max_addr, NUMA_NO_NODE,
 				       MEMBLOCK_NONE);
 }
@@ -1226,19 +1306,23 @@ phys_addr_t __init memblock_alloc_base(phys_addr_t size, phys_addr_t align, phys
 	alloc = __memblock_alloc_base(size, align, max_addr);
 
 	if (alloc == 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("ERROR: Failed to allocate %pa bytes below %pa.\n",
 		      &size, &max_addr);
+}
 
 	return alloc;
 }
 
 phys_addr_t __init memblock_alloc(phys_addr_t size, phys_addr_t align)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_alloc_base(size, align, MEMBLOCK_ALLOC_ACCESSIBLE);
 }
 
 phys_addr_t __init memblock_alloc_try_nid(phys_addr_t size, phys_addr_t align, int nid)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	phys_addr_t res = memblock_alloc_nid(size, align, nid);
 
 	if (res)
@@ -1283,7 +1367,9 @@ static void * __init memblock_virt_alloc_internal(
 	ulong flags = choose_memblock_flags();
 
 	if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nid = NUMA_NO_NODE;
+}
 
 	/*
 	 * Detect any accidental use of these APIs after slab is ready, as at
@@ -1291,39 +1377,53 @@ static void * __init memblock_virt_alloc_internal(
 	 * internal data may be destroyed (after execution of free_all_bootmem)
 	 */
 	if (WARN_ON_ONCE(slab_is_available()))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return kzalloc_node(size, GFP_NOWAIT, nid);
+}
 
 	if (!align)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		align = SMP_CACHE_BYTES;
+}
 
 	if (max_addr > memblock.current_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		max_addr = memblock.current_limit;
+}
 again:
 	alloc = memblock_find_in_range_node(size, align, min_addr, max_addr,
 					    nid, flags);
 	if (alloc && !memblock_reserve(alloc, size))
 		goto done;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (nid != NUMA_NO_NODE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		alloc = memblock_find_in_range_node(size, align, min_addr,
 						    max_addr, NUMA_NO_NODE,
 						    flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (alloc && !memblock_reserve(alloc, size))
 			goto done;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (min_addr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		min_addr = 0;
 		goto again;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (flags & MEMBLOCK_MIRROR) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flags &= ~MEMBLOCK_MIRROR;
 		pr_warn("Could not allocate %pap bytes of mirrored memory\n",
 			&size);
 		goto again;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 done:
 	ptr = phys_to_virt(alloc);
@@ -1400,11 +1500,15 @@ void * __init memblock_virt_alloc_try_nid(
 	ptr = memblock_virt_alloc_internal(size, align,
 					   min_addr, max_addr, nid);
 	if (ptr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ptr;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	panic("%s: Failed to allocate %llu bytes align=0x%llx nid=%d from=0x%llx max_addr=0x%llx\n",
 	      __func__, (u64)size, (u64)align, nid, (u64)min_addr,
 	      (u64)max_addr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1421,6 +1525,7 @@ void __init __memblock_free_early(phys_addr_t base, phys_addr_t size)
 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
 		     __func__, (u64)base, (u64)base + size - 1,
 		     (void *)_RET_IP_);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kmemleak_free_part_phys(base, size);
 	memblock_remove_range(&memblock.reserved, base, size);
 }
@@ -1438,6 +1543,7 @@ void __init __memblock_free_late(phys_addr_t base, phys_addr_t size)
 {
 	u64 cursor, end;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	memblock_dbg("%s: [%#016llx-%#016llx] %pF\n",
 		     __func__, (u64)base, (u64)base + size - 1,
 		     (void *)_RET_IP_);
@@ -1457,11 +1563,13 @@ void __init __memblock_free_late(phys_addr_t base, phys_addr_t size)
 
 phys_addr_t __init_memblock memblock_phys_mem_size(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock.memory.total_size;
 }
 
 phys_addr_t __init_memblock memblock_reserved_size(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock.reserved.total_size;
 }
 
@@ -1472,6 +1580,7 @@ phys_addr_t __init memblock_mem_size(unsigned long limit_pfn)
 	unsigned long start_pfn, end_pfn;
 
 	for_each_memblock(memory, r) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start_pfn = memblock_region_memory_base_pfn(r);
 		end_pfn = memblock_region_memory_end_pfn(r);
 		start_pfn = min_t(unsigned long, start_pfn, limit_pfn);
@@ -1485,6 +1594,7 @@ phys_addr_t __init memblock_mem_size(unsigned long limit_pfn)
 /* lowest address */
 phys_addr_t __init_memblock memblock_start_of_DRAM(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock.memory.regions[0].base;
 }
 
@@ -1521,7 +1631,9 @@ void __init memblock_enforce_memory_limit(phys_addr_t limit)
 	phys_addr_t max_addr = (phys_addr_t)ULLONG_MAX;
 
 	if (!limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	max_addr = __find_max_addr(limit);
 
@@ -1542,7 +1654,9 @@ void __init memblock_cap_memory_range(phys_addr_t base, phys_addr_t size)
 	int i, ret;
 
 	if (!size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	ret = memblock_isolate_range(&memblock.memory, base, size,
 						&start_rgn, &end_rgn);
@@ -1569,7 +1683,9 @@ void __init memblock_mem_limit_remove_map(phys_addr_t limit)
 	phys_addr_t max_addr;
 
 	if (!limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	max_addr = __find_max_addr(limit);
 
@@ -1588,28 +1704,34 @@ static int __init_memblock memblock_search(struct memblock_type *type, phys_addr
 		unsigned int mid = (right + left) / 2;
 
 		if (addr < type->regions[mid].base)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			right = mid;
+}
 		else if (addr >= (type->regions[mid].base +
 				  type->regions[mid].size))
 			left = mid + 1;
 		else
 			return mid;
 	} while (left < right);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return -1;
 }
 
 bool __init memblock_is_reserved(phys_addr_t addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_search(&memblock.reserved, addr) != -1;
 }
 
 bool __init_memblock memblock_is_memory(phys_addr_t addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_search(&memblock.memory, addr) != -1;
 }
 
 int __init_memblock memblock_is_map_memory(phys_addr_t addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int i = memblock_search(&memblock.memory, addr);
 
 	if (i == -1)
@@ -1625,7 +1747,9 @@ int __init_memblock memblock_search_pfn_nid(unsigned long pfn,
 	int mid = memblock_search(type, PFN_PHYS(pfn));
 
 	if (mid == -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	*start_pfn = PFN_DOWN(type->regions[mid].base);
 	*end_pfn = PFN_DOWN(type->regions[mid].base + type->regions[mid].size);
@@ -1650,7 +1774,9 @@ int __init_memblock memblock_is_region_memory(phys_addr_t base, phys_addr_t size
 	phys_addr_t end = base + memblock_cap_size(base, &size);
 
 	if (idx == -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	return (memblock.memory.regions[idx].base +
 		 memblock.memory.regions[idx].size) >= end;
 }
@@ -1667,6 +1793,7 @@ int __init_memblock memblock_is_region_memory(phys_addr_t base, phys_addr_t size
  */
 bool __init_memblock memblock_is_region_reserved(phys_addr_t base, phys_addr_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	memblock_cap_size(base, &size);
 	return memblock_overlaps_region(&memblock.reserved, base, size);
 }
@@ -1689,6 +1816,7 @@ void __init_memblock memblock_trim_memory(phys_addr_t align)
 			r->base = start;
 			r->size = end - start;
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			memblock_remove_region(&memblock.memory,
 					       r - memblock.memory.regions);
 			r--;
@@ -1703,6 +1831,7 @@ void __init_memblock memblock_set_current_limit(phys_addr_t limit)
 
 phys_addr_t __init_memblock memblock_get_current_limit(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock.current_limit;
 }
 
@@ -1715,6 +1844,7 @@ static void __init_memblock memblock_dump(struct memblock_type *type)
 
 	pr_info(" %s.cnt  = 0x%lx\n", type->name, type->cnt);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_memblock_type(type, rgn) {
 		char nid_buf[32] = "";
 
@@ -1739,6 +1869,7 @@ memblock_reserved_memory_within(phys_addr_t start_addr, phys_addr_t end_addr)
 	unsigned long size = 0;
 	int idx;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_memblock_type((&memblock.reserved), rgn) {
 		phys_addr_t start, end;
 
@@ -1757,6 +1888,7 @@ memblock_reserved_memory_within(phys_addr_t start_addr, phys_addr_t end_addr)
 
 void __init_memblock __memblock_dump_all(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pr_info("MEMBLOCK configuration:\n");
 	pr_info(" memory size = %pa reserved size = %pa\n",
 		&memblock.memory.total_size,
@@ -1776,6 +1908,7 @@ void __init memblock_allow_resize(void)
 
 static int __init early_memblock(char *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (p && strstr(p, "debug"))
 		memblock_debug = 1;
 	return 0;
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index 66e7efa..d3c6ed2 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* memcontrol.c - Memory Controller
  *
  * Copyright IBM Corporation, 2007
diff --git a/mm/memory.c b/mm/memory.c
index fc77791..ec10463 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/memory.c
  *
@@ -119,6 +121,7 @@ int randomize_va_space __read_mostly =
 
 static int __init disable_randmaps(char *s)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	randomize_va_space = 0;
 	return 1;
 }
@@ -157,6 +160,7 @@ void sync_mm_rss(struct mm_struct *mm)
 
 static void add_mm_counter_fast(struct mm_struct *mm, int member, int val)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct task_struct *task = current;
 
 	if (likely(task->mm == mm))
@@ -172,7 +176,9 @@ static void add_mm_counter_fast(struct mm_struct *mm, int member, int val)
 static void check_sync_rss_stat(struct task_struct *task)
 {
 	if (unlikely(task != current))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (unlikely(task->rss_stat.events++ > TASK_RSS_EVENTS_THRESH))
 		sync_mm_rss(task->mm);
 }
@@ -200,11 +206,15 @@ static bool tlb_next_batch(struct mmu_gather *tlb)
 	}
 
 	if (tlb->batch_count == MAX_GATHER_BATCH_COUNT)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	batch = (void *)__get_free_pages(GFP_NOWAIT | __GFP_NOWARN, 0);
 	if (!batch)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	tlb->batch_count++;
 	batch->next = NULL;
@@ -242,7 +252,9 @@ void arch_tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm,
 static void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
 {
 	if (!tlb->end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	tlb_flush(tlb);
 	mmu_notifier_invalidate_range(tlb->mm, tlb->start, tlb->end);
@@ -315,9 +327,13 @@ bool __tlb_remove_page_size(struct mmu_gather *tlb, struct page *page, int page_
 	batch->pages[batch->nr++] = page;
 	if (batch->nr == batch->max) {
 		if (!tlb_next_batch(tlb))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return true;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		batch = tlb->active;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(batch->nr > batch->max, page);
 
 	return false;
@@ -354,6 +370,7 @@ static void tlb_remove_table_rcu(struct rcu_head *head)
 	struct mmu_table_batch *batch;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	batch = container_of(head, struct mmu_table_batch, rcu);
 
 	for (i = 0; i < batch->nr; i++)
@@ -381,6 +398,7 @@ void tlb_remove_table(struct mmu_gather *tlb, void *table)
 	 * concurrent page-table walk.
 	 */
 	if (atomic_read(&tlb->mm->mm_users) < 2) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__tlb_remove_table(table);
 		return;
 	}
@@ -388,6 +406,7 @@ void tlb_remove_table(struct mmu_gather *tlb, void *table)
 	if (*batch == NULL) {
 		*batch = (struct mmu_table_batch *)__get_free_page(GFP_NOWAIT | __GFP_NOWARN);
 		if (*batch == NULL) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			tlb_remove_table_one(table);
 			return;
 		}
@@ -395,8 +414,10 @@ void tlb_remove_table(struct mmu_gather *tlb, void *table)
 	}
 	(*batch)->tables[(*batch)->nr++] = table;
 	if ((*batch)->nr == MAX_TABLE_BATCH)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		tlb_table_flush(tlb);
 }
+}
 
 #endif /* CONFIG_HAVE_RCU_TABLE_FREE */
 
@@ -460,14 +481,20 @@ static inline void free_pmd_range(struct mmu_gather *tlb, pud_t *pud,
 
 	start &= PUD_MASK;
 	if (start < floor)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (ceiling) {
 		ceiling &= PUD_MASK;
 		if (!ceiling)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
 	}
 	if (end - 1 > ceiling - 1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	pmd = pmd_offset(pud, start);
 	pud_clear(pud);
@@ -494,14 +521,20 @@ static inline void free_pud_range(struct mmu_gather *tlb, p4d_t *p4d,
 
 	start &= P4D_MASK;
 	if (start < floor)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (ceiling) {
 		ceiling &= P4D_MASK;
 		if (!ceiling)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
 	}
 	if (end - 1 > ceiling - 1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	pud = pud_offset(p4d, start);
 	p4d_clear(p4d);
@@ -519,23 +552,32 @@ static inline void free_p4d_range(struct mmu_gather *tlb, pgd_t *pgd,
 	start = addr;
 	p4d = p4d_offset(pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(p4d))
 			continue;
 		free_pud_range(tlb, p4d, addr, next, floor, ceiling);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (p4d++, addr = next, addr != end);
 
 	start &= PGDIR_MASK;
 	if (start < floor)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (ceiling) {
 		ceiling &= PGDIR_MASK;
 		if (!ceiling)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
 	}
 	if (end - 1 > ceiling - 1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	p4d = p4d_offset(pgd, start);
 	pgd_clear(pgd);
 	p4d_free_tlb(tlb, p4d, start);
@@ -581,17 +623,23 @@ void free_pgd_range(struct mmu_gather *tlb,
 	if (addr < floor) {
 		addr += PMD_SIZE;
 		if (!addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
 	}
 	if (ceiling) {
 		ceiling &= PMD_MASK;
 		if (!ceiling)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
 	}
 	if (end - 1 > ceiling - 1)
 		end -= PMD_SIZE;
 	if (addr > end - 1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	/*
 	 * We add page table cache pages with PAGE_SIZE,
 	 * (see pte_free_tlb()), flush the tlb if we need
@@ -629,6 +677,7 @@ void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
 			 */
 			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
 			       && !is_vm_hugetlb_page(next)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				vma = next;
 				next = vma->vm_next;
 				unlink_anon_vmas(vma);
@@ -637,6 +686,7 @@ void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
 			free_pgd_range(tlb, addr, vma->vm_end,
 				floor, next ? next->vm_start : ceiling);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vma = next;
 	}
 }
@@ -646,7 +696,9 @@ int __pte_alloc(struct mm_struct *mm, pmd_t *pmd, unsigned long address)
 	spinlock_t *ptl;
 	pgtable_t new = pte_alloc_one(mm, address);
 	if (!new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/*
 	 * Ensure all pte setup (eg. pte page lock and page clearing) are
@@ -669,9 +721,13 @@ int __pte_alloc(struct mm_struct *mm, pmd_t *pmd, unsigned long address)
 		pmd_populate(mm, pmd, new);
 		new = NULL;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(ptl);
 	if (new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_free(mm, new);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -679,7 +735,9 @@ int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
 {
 	pte_t *new = pte_alloc_one_kernel(&init_mm, address);
 	if (!new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	smp_wmb(); /* See comment in __pte_alloc */
 
@@ -688,14 +746,19 @@ int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
 		pmd_populate_kernel(&init_mm, pmd, new);
 		new = NULL;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&init_mm.page_table_lock);
 	if (new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_free_kernel(&init_mm, new);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
 static inline void init_rss_vec(int *rss)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	memset(rss, 0, sizeof(int) * NR_MM_COUNTERS);
 }
 
@@ -735,6 +798,7 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 	 * or allow a steady drip of one report per second.
 	 */
 	if (nr_shown == 60) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (time_before(jiffies, resume)) {
 			nr_unshown++;
 			return;
@@ -827,11 +891,17 @@ struct page *_vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 		if (likely(!pte_special(pte)))
 			goto check_pfn;
 		if (vma->vm_ops && vma->vm_ops->find_special_page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return vma->vm_ops->find_special_page(vma, addr);
+}
 		if (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 		if (is_zero_pfn(pfn))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 
 		/*
 		 * Device public pages are special pages (they are ZONE_DEVICE
@@ -849,36 +919,55 @@ struct page *_vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
 			struct page *page = pfn_to_page(pfn);
 
 			if (is_device_public_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (with_public_device)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					return page;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return NULL;
 			}
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		print_bad_pte(vma, addr, pte, NULL);
 		return NULL;
 	}
 
 	/* !HAVE_PTE_SPECIAL case follows: */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (vma->vm_flags & VM_MIXEDMAP) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!pfn_valid(pfn))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return NULL;
+}
 			goto out;
 		} else {
 			unsigned long off;
 			off = (addr - vma->vm_start) >> PAGE_SHIFT;
 			if (pfn == vma->vm_pgoff + off)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return NULL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!is_cow_mapping(vma->vm_flags))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return NULL;
+}
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_zero_pfn(pfn))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 check_pfn:
 	if (unlikely(pfn > highest_memmap_pfn)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		print_bad_pte(vma, addr, pte, NULL);
 		return NULL;
 	}
@@ -948,22 +1037,31 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 
 	/* pte contains position in swap or file, so copy. */
 	if (unlikely(!pte_present(pte))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		swp_entry_t entry = pte_to_swp_entry(pte);
 
 		if (likely(!non_swap_entry(entry))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (swap_duplicate(entry) < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return entry.val;
+}
 
 			/* make sure dst_mm is on swapoff's mmlist. */
 			if (unlikely(list_empty(&dst_mm->mmlist))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_lock(&mmlist_lock);
 				if (list_empty(&dst_mm->mmlist))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					list_add(&dst_mm->mmlist,
 							&src_mm->mmlist);
+}
 				spin_unlock(&mmlist_lock);
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			rss[MM_SWAPENTS]++;
 		} else if (is_migration_entry(entry)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = migration_entry_to_page(entry);
 
 			rss[mm_counter(page)]++;
@@ -977,10 +1075,14 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 				make_migration_entry_read(&entry);
 				pte = swp_entry_to_pte(entry);
 				if (pte_swp_soft_dirty(*src_pte))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					pte = pte_swp_mksoft_dirty(pte);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				set_pte_at(src_mm, addr, src_pte, pte);
 			}
 		} else if (is_device_private_entry(entry)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = device_private_entry_to_page(entry);
 
 			/*
@@ -1005,6 +1107,7 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			 */
 			if (is_write_device_private_entry(entry) &&
 			    is_cow_mapping(vm_flags)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				make_device_private_entry_read(&entry);
 				pte = swp_entry_to_pte(entry);
 				set_pte_at(src_mm, addr, src_pte, pte);
@@ -1027,7 +1130,10 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	 * the child
 	 */
 	if (vm_flags & VM_SHARED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte = pte_mkclean(pte);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte = pte_mkold(pte);
 
 	page = vm_normal_page(vma, addr, pte);
@@ -1036,6 +1142,7 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		page_dup_rmap(page, false);
 		rss[mm_counter(page)]++;
 	} else if (pte_devmap(pte)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = pte_page(pte);
 
 		/*
@@ -1044,6 +1151,7 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		 * MEMORY_DEVICE_CACHE_COHERENT in memory_hotplug.h
 		 */
 		if (is_device_public_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			get_page(page);
 			page_dup_rmap(page, false);
 			rss[mm_counter(page)]++;
@@ -1071,10 +1179,13 @@ static int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 
 	dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
 	if (!dst_pte)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	src_pte = pte_offset_map(src_pmd, addr);
 	src_ptl = pte_lockptr(src_mm, src_pmd);
 	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	orig_src_pte = src_pte;
 	orig_dst_pte = dst_pte;
 	arch_enter_lazy_mmu_mode();
@@ -1085,6 +1196,7 @@ static int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		 * could generate latencies in another task on another CPU.
 		 */
 		if (progress >= 32) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			progress = 0;
 			if (need_resched() ||
 			    spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))
@@ -1102,19 +1214,26 @@ static int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
 
 	arch_leave_lazy_mmu_mode();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(src_ptl);
 	pte_unmap(orig_src_pte);
 	add_mm_rss_vec(dst_mm, rss);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_unmap_unlock(orig_dst_pte, dst_ptl);
 	cond_resched();
 
 	if (entry.val) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (add_swap_count_continuation(entry, GFP_KERNEL) < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		progress = 0;
 	}
 	if (addr != end)
 		goto again;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -1127,7 +1246,9 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src
 
 	dst_pmd = pmd_alloc(dst_mm, dst_pud, addr);
 	if (!dst_pmd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	src_pmd = pmd_offset(src_pud, addr);
 	do {
 		next = pmd_addr_end(addr, end);
@@ -1138,7 +1259,9 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src
 			err = copy_huge_pmd(dst_mm, src_mm,
 					    dst_pmd, src_pmd, addr, vma);
 			if (err == -ENOMEM)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENOMEM;
+}
 			if (!err)
 				continue;
 			/* fall through */
@@ -1149,6 +1272,7 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src
 						vma, addr, next))
 			return -ENOMEM;
 	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -1161,7 +1285,9 @@ static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src
 
 	dst_pud = pud_alloc(dst_mm, dst_p4d, addr);
 	if (!dst_pud)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	src_pud = pud_offset(src_p4d, addr);
 	do {
 		next = pud_addr_end(addr, end);
@@ -1172,7 +1298,10 @@ static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src
 			err = copy_huge_pud(dst_mm, src_mm,
 					    dst_pud, src_pud, addr, vma);
 			if (err == -ENOMEM)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENOMEM;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!err)
 				continue;
 			/* fall through */
@@ -1183,6 +1312,7 @@ static inline int copy_pud_range(struct mm_struct *dst_mm, struct mm_struct *src
 						vma, addr, next))
 			return -ENOMEM;
 	} while (dst_pud++, src_pud++, addr = next, addr != end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -1195,15 +1325,20 @@ static inline int copy_p4d_range(struct mm_struct *dst_mm, struct mm_struct *src
 
 	dst_p4d = p4d_alloc(dst_mm, dst_pgd, addr);
 	if (!dst_p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	src_p4d = p4d_offset(src_pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(src_p4d))
 			continue;
 		if (copy_pud_range(dst_mm, src_mm, dst_p4d, src_p4d,
 						vma, addr, next))
 			return -ENOMEM;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (dst_p4d++, src_p4d++, addr = next, addr != end);
 	return 0;
 }
@@ -1231,7 +1366,9 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		return 0;
 
 	if (is_vm_hugetlb_page(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return copy_hugetlb_page_range(dst_mm, src_mm, vma);
+}
 
 	if (unlikely(vma->vm_flags & VM_PFNMAP)) {
 		/*
@@ -1240,7 +1377,9 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		 */
 		ret = track_pfn_copy(vma);
 		if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 	}
 
 	/*
@@ -1256,6 +1395,7 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 		mmu_notifier_invalidate_range_start(src_mm, mmun_start,
 						    mmun_end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = 0;
 	dst_pgd = pgd_offset(dst_mm, addr);
 	src_pgd = pgd_offset(src_mm, addr);
@@ -1265,6 +1405,7 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 			continue;
 		if (unlikely(copy_p4d_range(dst_mm, src_mm, dst_pgd, src_pgd,
 					    vma, addr, next))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = -ENOMEM;
 			break;
 		}
@@ -1272,6 +1413,7 @@ int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
 
 	if (is_cow)
 		mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -1322,6 +1464,7 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 
 			if (!PageAnon(page)) {
 				if (pte_dirty(ptent)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					force_flush = 1;
 					set_page_dirty(page);
 				}
@@ -1332,8 +1475,11 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			rss[mm_counter(page)]--;
 			page_remove_rmap(page, false);
 			if (unlikely(page_mapcount(page) < 0))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				print_bad_pte(vma, addr, ptent, page);
+}
 			if (unlikely(__tlb_remove_page(tlb, page))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				force_flush = 1;
 				addr += PAGE_SIZE;
 				break;
@@ -1341,10 +1487,14 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 			continue;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_to_swp_entry(ptent);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (non_swap_entry(entry) && is_device_private_entry(entry)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			struct page *page = device_private_entry_to_page(entry);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (unlikely(details && details->check_mapping)) {
 				/*
 				 * unmap_shared_mapping_pages() wants to
@@ -1356,6 +1506,7 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 					continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
 			rss[mm_counter(page)]--;
 			page_remove_rmap(page, false);
@@ -1367,17 +1518,25 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 		if (unlikely(details))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_to_swp_entry(ptent);
 		if (!non_swap_entry(entry))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			rss[MM_SWAPENTS]--;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		else if (is_migration_entry(entry)) {
 			struct page *page;
 
 			page = migration_entry_to_page(entry);
 			rss[mm_counter(page)]--;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(!free_swap_and_cache(entry)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			print_bad_pte(vma, addr, ptent, NULL);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 
@@ -1387,6 +1546,7 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 	/* Do the actual TLB flush before dropping ptl */
 	if (force_flush)
 		tlb_flush_mmu_tlbonly(tlb);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_unmap_unlock(start_pte, ptl);
 
 	/*
@@ -1396,6 +1556,7 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,
 	 * memory too. Restart if we didn't do everything.
 	 */
 	if (force_flush) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		force_flush = 0;
 		tlb_flush_mmu_free(tlb);
 		if (addr != end)
@@ -1418,6 +1579,7 @@ static inline unsigned long zap_pmd_range(struct mmu_gather *tlb,
 		next = pmd_addr_end(addr, end);
 		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
 			if (next - addr != HPAGE_PMD_SIZE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				VM_BUG_ON_VMA(vma_is_anonymous(vma) &&
 				    !rwsem_is_locked(&tlb->mm->mmap_sem), vma);
 				__split_huge_pmd(vma, pmd, addr, false, NULL);
@@ -1454,8 +1616,11 @@ static inline unsigned long zap_pud_range(struct mmu_gather *tlb,
 	do {
 		next = pud_addr_end(addr, end);
 		if (pud_trans_huge(*pud) || pud_devmap(*pud)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (next - addr != HPAGE_PUD_SIZE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				VM_BUG_ON_VMA(!rwsem_is_locked(&tlb->mm->mmap_sem), vma);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				split_huge_pud(vma, pud, addr);
 			} else if (zap_huge_pud(tlb, vma, pud, addr))
 				goto next;
@@ -1481,6 +1646,7 @@ static inline unsigned long zap_p4d_range(struct mmu_gather *tlb,
 
 	p4d = p4d_offset(pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(p4d))
 			continue;
@@ -1499,6 +1665,7 @@ void unmap_page_range(struct mmu_gather *tlb,
 	unsigned long next;
 
 	BUG_ON(addr >= end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	tlb_start_vma(tlb, vma);
 	pgd = pgd_offset(vma->vm_mm, addr);
 	do {
@@ -1507,6 +1674,7 @@ void unmap_page_range(struct mmu_gather *tlb,
 			continue;
 		next = zap_p4d_range(tlb, vma, pgd, addr, next, details);
 	} while (pgd++, addr = next, addr != end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	tlb_end_vma(tlb, vma);
 }
 
@@ -1520,10 +1688,14 @@ static void unmap_single_vma(struct mmu_gather *tlb,
 	unsigned long end;
 
 	if (start >= vma->vm_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	end = min(vma->vm_end, end_addr);
 	if (end <= vma->vm_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (vma->vm_file)
 		uprobe_munmap(vma, start, end);
@@ -1545,6 +1717,7 @@ static void unmap_single_vma(struct mmu_gather *tlb,
 			 * safe to do nothing in this case.
 			 */
 			if (vma->vm_file) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				i_mmap_lock_write(vma->vm_file->f_mapping);
 				__unmap_hugepage_range_final(tlb, vma, start, end, NULL);
 				i_mmap_unlock_write(vma->vm_file->f_mapping);
@@ -1661,6 +1834,7 @@ static void zap_page_range_single(struct vm_area_struct *vma, unsigned long addr
 int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
 		unsigned long size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (address < vma->vm_start || address + size > vma->vm_end ||
 	    		!(vma->vm_flags & VM_PFNMAP))
 		return -1;
@@ -1680,14 +1854,21 @@ pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,
 	pgd = pgd_offset(mm, addr);
 	p4d = p4d_alloc(mm, pgd, addr);
 	if (!p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	pud = pud_alloc(mm, p4d, addr);
 	if (!pud)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	pmd = pmd_alloc(mm, pud, addr);
 	if (!pmd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(pmd_trans_huge(*pmd));
 	return pte_alloc_map_lock(mm, pmd, addr, ptl);
 }
@@ -1710,6 +1891,7 @@ static int insert_page(struct vm_area_struct *vma, unsigned long addr,
 	retval = -EINVAL;
 	if (PageAnon(page))
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	retval = -ENOMEM;
 	flush_dcache_page(page);
 	pte = get_locked_pte(mm, addr, &ptl);
@@ -1764,6 +1946,7 @@ static int insert_page(struct vm_area_struct *vma, unsigned long addr,
 int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,
 			struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (addr < vma->vm_start || addr >= vma->vm_end)
 		return -EFAULT;
 	if (!page_count(page))
@@ -1789,8 +1972,10 @@ static int insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 	pte = get_locked_pte(mm, addr, &ptl);
 	if (!pte)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	retval = -EBUSY;
 	if (!pte_none(*pte)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (mkwrite) {
 			/*
 			 * For read faults on private mappings the PFN passed
@@ -1801,6 +1986,7 @@ static int insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 			 */
 			if (WARN_ON_ONCE(pte_pfn(*pte) != pfn_t_to_pfn(pfn)))
 				goto out_unlock;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			entry = *pte;
 			goto out_mkwrite;
 		} else
@@ -1809,12 +1995,15 @@ static int insert_pfn(struct vm_area_struct *vma, unsigned long addr,
 
 	/* Ok, finally just insert the thing.. */
 	if (pfn_t_devmap(pfn))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_mkdevmap(pfn_t_pte(pfn, prot));
+}
 	else
 		entry = pte_mkspecial(pfn_t_pte(pfn, prot));
 
 out_mkwrite:
 	if (mkwrite) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_mkyoung(entry);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	}
@@ -1885,7 +2074,9 @@ int vm_insert_pfn_prot(struct vm_area_struct *vma, unsigned long addr,
 	BUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));
 
 	if (addr < vma->vm_start || addr >= vma->vm_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EFAULT;
+}
 
 	track_pfn_insert(vma, &pgprot, __pfn_to_pfn_t(pfn, PFN_DEV));
 
@@ -1901,6 +2092,7 @@ static int __vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
 {
 	pgprot_t pgprot = vma->vm_page_prot;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!(vma->vm_flags & VM_MIXEDMAP));
 
 	if (addr < vma->vm_start || addr >= vma->vm_end)
@@ -1932,6 +2124,7 @@ static int __vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
 int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
 			pfn_t pfn)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __vm_insert_mixed(vma, addr, pfn, false);
 
 }
@@ -1940,6 +2133,7 @@ EXPORT_SYMBOL(vm_insert_mixed);
 int vm_insert_mixed_mkwrite(struct vm_area_struct *vma, unsigned long addr,
 			pfn_t pfn)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __vm_insert_mixed(vma, addr, pfn, true);
 }
 EXPORT_SYMBOL(vm_insert_mixed_mkwrite);
@@ -1956,6 +2150,7 @@ static int remap_pte_range(struct mm_struct *mm, pmd_t *pmd,
 	pte_t *pte;
 	spinlock_t *ptl;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
 	if (!pte)
 		return -ENOMEM;
@@ -1980,7 +2175,9 @@ static inline int remap_pmd_range(struct mm_struct *mm, pud_t *pud,
 	pfn -= addr >> PAGE_SHIFT;
 	pmd = pmd_alloc(mm, pud, addr);
 	if (!pmd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	VM_BUG_ON(pmd_trans_huge(*pmd));
 	do {
 		next = pmd_addr_end(addr, end);
@@ -2001,7 +2198,9 @@ static inline int remap_pud_range(struct mm_struct *mm, p4d_t *p4d,
 	pfn -= addr >> PAGE_SHIFT;
 	pud = pud_alloc(mm, p4d, addr);
 	if (!pud)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
 		next = pud_addr_end(addr, end);
 		if (remap_pmd_range(mm, pud, addr, next,
@@ -2021,7 +2220,9 @@ static inline int remap_p4d_range(struct mm_struct *mm, pgd_t *pgd,
 	pfn -= addr >> PAGE_SHIFT;
 	p4d = p4d_alloc(mm, pgd, addr);
 	if (!p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
 		next = p4d_addr_end(addr, end);
 		if (remap_pud_range(mm, p4d, addr, next,
@@ -2070,6 +2271,7 @@ int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
 	 * See vm_normal_page() for details.
 	 */
 	if (is_cow_mapping(vma->vm_flags)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (addr != vma->vm_start || end != vma->vm_end)
 			return -EINVAL;
 		vma->vm_pgoff = pfn;
@@ -2119,7 +2321,9 @@ int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long
 
 	/* Check that the physical memory area passed in looks valid */
 	if (start + len < start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	/*
 	 * You *really* shouldn't map things that aren't page-aligned,
 	 * but we've historically allowed it because IO memory might
@@ -2156,6 +2360,7 @@ static int apply_to_pte_range(struct mm_struct *mm, pmd_t *pmd,
 	pgtable_t token;
 	spinlock_t *uninitialized_var(ptl);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte = (mm == &init_mm) ?
 		pte_alloc_kernel(pmd, addr) :
 		pte_alloc_map_lock(mm, pmd, addr, &ptl);
@@ -2189,6 +2394,7 @@ static int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,
 	unsigned long next;
 	int err;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(pud_huge(*pud));
 
 	pmd = pmd_alloc(mm, pud, addr);
@@ -2213,7 +2419,9 @@ static int apply_to_pud_range(struct mm_struct *mm, p4d_t *p4d,
 
 	pud = pud_alloc(mm, p4d, addr);
 	if (!pud)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
 		next = pud_addr_end(addr, end);
 		err = apply_to_pmd_range(mm, pud, addr, next, fn, data);
@@ -2233,7 +2441,9 @@ static int apply_to_p4d_range(struct mm_struct *mm, pgd_t *pgd,
 
 	p4d = p4d_alloc(mm, pgd, addr);
 	if (!p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
 		next = p4d_addr_end(addr, end);
 		err = apply_to_pud_range(mm, p4d, addr, next, fn, data);
@@ -2255,6 +2465,7 @@ int apply_to_page_range(struct mm_struct *mm, unsigned long addr,
 	unsigned long end = addr + size;
 	int err;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON(addr >= end))
 		return -EINVAL;
 
@@ -2284,6 +2495,7 @@ static inline int pte_unmap_same(struct mm_struct *mm, pmd_t *pmd,
 	int same = 1;
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
 	if (sizeof(pte_t) > sizeof(unsigned long)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spinlock_t *ptl = pte_lockptr(mm, pmd);
 		spin_lock(ptl);
 		same = pte_same(*page_table, orig_pte);
@@ -2296,6 +2508,7 @@ static inline int pte_unmap_same(struct mm_struct *mm, pmd_t *pmd,
 
 static inline void cow_user_page(struct page *dst, struct page *src, unsigned long va, struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	debug_dma_assert_idle(src);
 
 	/*
@@ -2305,6 +2518,7 @@ static inline void cow_user_page(struct page *dst, struct page *src, unsigned lo
 	 * fails, we just zero-fill it. Live with it.
 	 */
 	if (unlikely(!src)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		void *kaddr = kmap_atomic(dst);
 		void __user *uaddr = (void __user *)(va & PAGE_MASK);
 
@@ -2315,8 +2529,12 @@ static inline void cow_user_page(struct page *dst, struct page *src, unsigned lo
 		 * zeroes.
 		 */
 		if (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			clear_page(kaddr);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kunmap_atomic(kaddr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_dcache_page(dst);
 	} else
 		copy_user_highpage(dst, src, va, vma);
@@ -2354,7 +2572,9 @@ static int do_page_mkwrite(struct vm_fault *vmf)
 	/* Restore original flags so that caller is not surprised */
 	vmf->flags = old_flags;
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 	if (unlikely(!(ret & VM_FAULT_LOCKED))) {
 		lock_page(page);
 		if (!page->mapping) {
@@ -2422,13 +2642,19 @@ static inline void wp_page_reuse(struct vm_fault *vmf)
 	 * unrelated process.
 	 */
 	if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page_cpupid_xchg_last(page, (1 << LAST_CPUPID_SHIFT) - 1);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	entry = pte_mkyoung(vmf->orig_pte);
 	entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	if (ptep_set_access_flags(vma, vmf->address, vmf->pte, entry, 1))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		update_mmu_cache(vma, vmf->address, vmf->pte);
+}
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 }
 
@@ -2479,6 +2705,7 @@ static int wp_page_copy(struct vm_fault *vmf)
 	if (mem_cgroup_try_charge(new_page, mm, GFP_KERNEL, &memcg, false))
 		goto oom_free_new;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__SetPageUptodate(new_page);
 
 	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
@@ -2497,6 +2724,7 @@ static int wp_page_copy(struct vm_fault *vmf)
 		} else {
 			inc_mm_counter_fast(mm, MM_ANONPAGES);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_cache_page(vma, vmf->address, pte_pfn(vmf->orig_pte));
 		entry = mk_pte(new_page, vma->vm_page_prot);
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
@@ -2561,9 +2789,13 @@ static int wp_page_copy(struct vm_fault *vmf)
 		 * keep the mlocked page.
 		 */
 		if (page_copied && (vma->vm_flags & VM_LOCKED)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			lock_page(old_page);	/* LRU manipulation */
 			if (PageMlocked(old_page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				munlock_vma_page(old_page);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(old_page);
 		}
 		put_page(old_page);
@@ -2573,7 +2805,10 @@ static int wp_page_copy(struct vm_fault *vmf)
 	put_page(new_page);
 oom:
 	if (old_page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_page(old_page);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return VM_FAULT_OOM;
 }
 
@@ -2594,6 +2829,7 @@ static int wp_page_copy(struct vm_fault *vmf)
  */
 int finish_mkwrite_fault(struct vm_fault *vmf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON_ONCE(!(vmf->vma->vm_flags & VM_SHARED));
 	vmf->pte = pte_offset_map_lock(vmf->vma->vm_mm, vmf->pmd, vmf->address,
 				       &vmf->ptl);
@@ -2617,6 +2853,7 @@ static int wp_pfn_shared(struct vm_fault *vmf)
 {
 	struct vm_area_struct *vma = vmf->vma;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (vma->vm_ops && vma->vm_ops->pfn_mkwrite) {
 		int ret;
 
@@ -2638,6 +2875,7 @@ static int wp_page_shared(struct vm_fault *vmf)
 
 	get_page(vmf->page);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (vma->vm_ops && vma->vm_ops->page_mkwrite) {
 		int tmp;
 
@@ -2711,17 +2949,24 @@ static int do_wp_page(struct vm_fault *vmf)
 	if (PageAnon(vmf->page) && !PageKsm(vmf->page)) {
 		int total_map_swapcount;
 		if (!trylock_page(vmf->page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			get_page(vmf->page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			lock_page(vmf->page);
 			vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd,
 					vmf->address, &vmf->ptl);
 			if (!pte_same(*vmf->pte, vmf->orig_pte)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				unlock_page(vmf->page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				pte_unmap_unlock(vmf->pte, vmf->ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				put_page(vmf->page);
 				return 0;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(vmf->page);
 		}
 		if (reuse_swap_page(vmf->page, &total_map_swapcount)) {
@@ -2742,6 +2987,7 @@ static int do_wp_page(struct vm_fault *vmf)
 		unlock_page(vmf->page);
 	} else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
 					(VM_WRITE|VM_SHARED))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return wp_page_shared(vmf);
 	}
 
@@ -2774,10 +3020,14 @@ static inline void unmap_mapping_range_tree(struct rb_root_cached *root,
 		vea = vba + vma_pages(vma) - 1;
 		zba = details->first_index;
 		if (zba < vba)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zba = vba;
+}
 		zea = details->last_index;
 		if (zea > vea)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zea = vea;
+}
 
 		unmap_mapping_range_vma(vma,
 			((zba - vba) << PAGE_SHIFT) + vma->vm_start,
@@ -2815,18 +3065,24 @@ void unmap_mapping_range(struct address_space *mapping,
 		long long holeend =
 			(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
 		if (holeend & ~(long long)ULONG_MAX)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			hlen = ULONG_MAX - hba + 1;
+}
 	}
 
 	details.check_mapping = even_cows ? NULL : mapping;
 	details.first_index = hba;
 	details.last_index = hba + hlen - 1;
 	if (details.last_index < details.first_index)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		details.last_index = ULONG_MAX;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	i_mmap_lock_write(mapping);
 	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap.rb_root)))
 		unmap_mapping_range_tree(&mapping->i_mmap, &details);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	i_mmap_unlock_write(mapping);
 }
 EXPORT_SYMBOL(unmap_mapping_range);
@@ -2853,7 +3109,9 @@ int do_swap_page(struct vm_fault *vmf)
 	bool vma_readahead = swap_use_vma_readahead();
 
 	if (vma_readahead)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = swap_readahead_detect(vmf, &swap_ra);
+}
 	if (!pte_unmap_same(vma->vm_mm, vmf->pmd, vmf->pte, vmf->orig_pte)) {
 		if (page)
 			put_page(page);
@@ -3058,7 +3316,9 @@ static int do_anonymous_page(struct vm_fault *vmf)
 
 	/* File mapping without ->vm_ops ? */
 	if (vma->vm_flags & VM_SHARED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_SIGBUS;
+}
 
 	/*
 	 * Use pte_alloc() instead of pte_alloc_map().  We can't run
@@ -3071,11 +3331,15 @@ static int do_anonymous_page(struct vm_fault *vmf)
 	 * Here we only have down_read(mmap_sem).
 	 */
 	if (pte_alloc(vma->vm_mm, vmf->pmd, vmf->address))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_OOM;
+}
 
 	/* See the comment in pte_alloc_one_map() */
 	if (unlikely(pmd_trans_unstable(vmf->pmd)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Use the zero-page for reads */
 	if (!(vmf->flags & FAULT_FLAG_WRITE) &&
@@ -3091,7 +3355,9 @@ static int do_anonymous_page(struct vm_fault *vmf)
 			goto unlock;
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return handle_userfault(vmf, VM_UFFD_MISSING);
 		}
 		goto setpte;
@@ -3116,7 +3382,9 @@ static int do_anonymous_page(struct vm_fault *vmf)
 
 	entry = mk_pte(page, vma->vm_page_prot);
 	if (vma->vm_flags & VM_WRITE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_mkwrite(pte_mkdirty(entry));
+}
 
 	vmf->pte = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address,
 			&vmf->ptl);
@@ -3129,7 +3397,9 @@ static int do_anonymous_page(struct vm_fault *vmf)
 
 	/* Deliver the page fault to userland, check inside PT lock */
 	if (userfaultfd_missing(vma)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mem_cgroup_cancel_charge(page, memcg, false);
 		put_page(page);
 		return handle_userfault(vmf, VM_UFFD_MISSING);
@@ -3173,8 +3443,12 @@ static int __do_fault(struct vm_fault *vmf)
 		return ret;
 
 	if (unlikely(PageHWPoison(vmf->page))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (ret & VM_FAULT_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(vmf->page);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_page(vmf->page);
 		vmf->page = NULL;
 		return VM_FAULT_HWPOISON;
@@ -3185,6 +3459,7 @@ static int __do_fault(struct vm_fault *vmf)
 	else
 		VM_BUG_ON_PAGE(!PageLocked(vmf->page), vmf->page);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -3208,6 +3483,7 @@ static int pte_alloc_one_map(struct vm_fault *vmf)
 	if (vmf->prealloc_pte) {
 		vmf->ptl = pmd_lock(vma->vm_mm, vmf->pmd);
 		if (unlikely(!pmd_none(*vmf->pmd))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(vmf->ptl);
 			goto map_pte;
 		}
@@ -3217,6 +3493,7 @@ static int pte_alloc_one_map(struct vm_fault *vmf)
 		spin_unlock(vmf->ptl);
 		vmf->prealloc_pte = NULL;
 	} else if (unlikely(pte_alloc(vma->vm_mm, vmf->pmd, vmf->address))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_OOM;
 	}
 map_pte:
@@ -3232,7 +3509,9 @@ static int pte_alloc_one_map(struct vm_fault *vmf)
 	 * C, which is what pmd_trans_unstable() provides.
 	 */
 	if (pmd_devmap_trans_unstable(vmf->pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_NOPAGE;
+}
 
 	/*
 	 * At this point we know that our vmf->pmd points to a page of ptes
@@ -3333,6 +3612,7 @@ static int do_set_pmd(struct vm_fault *vmf, struct page *page)
 #else
 static int do_set_pmd(struct vm_fault *vmf, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG();
 	return 0;
 }
@@ -3367,19 +3647,26 @@ int alloc_set_pte(struct vm_fault *vmf, struct mem_cgroup *memcg,
 
 		ret = do_set_pmd(vmf, page);
 		if (ret != VM_FAULT_FALLBACK)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 	}
 
 	if (!vmf->pte) {
 		ret = pte_alloc_one_map(vmf);
 		if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 	}
 
 	/* Re-check under ptl */
 	if (unlikely(!pte_none(*vmf->pte)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_NOPAGE;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_icache_page(vma, page);
 	entry = mk_pte(page, vma->vm_page_prot);
 	if (write)
@@ -3448,6 +3735,7 @@ static unsigned long fault_around_bytes __read_mostly =
 #ifdef CONFIG_DEBUG_FS
 static int fault_around_bytes_get(void *data, u64 *val)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	*val = fault_around_bytes;
 	return 0;
 }
@@ -3459,6 +3747,7 @@ static int fault_around_bytes_get(void *data, u64 *val)
  */
 static int fault_around_bytes_set(void *data, u64 val)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (val / PAGE_SIZE > PTRS_PER_PTE)
 		return -EINVAL;
 	if (val > PAGE_SIZE)
@@ -3477,7 +3766,9 @@ static int __init fault_around_debugfs(void)
 	ret = debugfs_create_file_unsafe("fault_around_bytes", 0644, NULL, NULL,
 			&fault_around_bytes_fops);
 	if (!ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("Failed to create fault_around_bytes in debugfs");
+}
 	return 0;
 }
 late_initcall(fault_around_debugfs);
@@ -3542,6 +3833,7 @@ static int do_fault_around(struct vm_fault *vmf)
 
 	/* Huge page is mapped? Page fault is solved */
 	if (pmd_trans_huge(*vmf->pmd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = VM_FAULT_NOPAGE;
 		goto out;
 	}
@@ -3553,7 +3845,9 @@ static int do_fault_around(struct vm_fault *vmf)
 	/* check if the page fault is solved */
 	vmf->pte -= (vmf->address >> PAGE_SHIFT) - (address >> PAGE_SHIFT);
 	if (!pte_none(*vmf->pte))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = VM_FAULT_NOPAGE;
+}
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
 out:
 	vmf->address = address;
@@ -3574,17 +3868,24 @@ static int do_read_fault(struct vm_fault *vmf)
 	if (vma->vm_ops->map_pages && fault_around_bytes >> PAGE_SHIFT > 1) {
 		ret = do_fault_around(vmf);
 		if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 	}
 
 	ret = __do_fault(vmf);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	ret |= finish_fault(vmf);
 	unlock_page(vmf->page);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_page(vmf->page);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -3594,14 +3895,19 @@ static int do_cow_fault(struct vm_fault *vmf)
 	int ret;
 
 	if (unlikely(anon_vma_prepare(vma)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_OOM;
+}
 
 	vmf->cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, vmf->address);
 	if (!vmf->cow_page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_OOM;
+}
 
 	if (mem_cgroup_try_charge(vmf->cow_page, vma->vm_mm, GFP_KERNEL,
 				&vmf->memcg, false)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_page(vmf->cow_page);
 		return VM_FAULT_OOM;
 	}
@@ -3610,7 +3916,9 @@ static int do_cow_fault(struct vm_fault *vmf)
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		goto uncharge_out;
 	if (ret & VM_FAULT_DONE_COW)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	copy_user_highpage(vmf->cow_page, vmf->page, vmf->address, vma);
 	__SetPageUptodate(vmf->cow_page);
@@ -3620,6 +3928,7 @@ static int do_cow_fault(struct vm_fault *vmf)
 	put_page(vmf->page);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
 		goto uncharge_out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 uncharge_out:
 	mem_cgroup_cancel_charge(vmf->cow_page, vmf->memcg, false);
@@ -3634,17 +3943,22 @@ static int do_shared_fault(struct vm_fault *vmf)
 
 	ret = __do_fault(vmf);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	/*
 	 * Check if the backing address space wants to know that the page is
 	 * about to become writable
 	 */
 	if (vma->vm_ops->page_mkwrite) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(vmf->page);
 		tmp = do_page_mkwrite(vmf);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(!tmp ||
 				(tmp & (VM_FAULT_ERROR | VM_FAULT_NOPAGE)))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(vmf->page);
 			return tmp;
 		}
@@ -3653,6 +3967,7 @@ static int do_shared_fault(struct vm_fault *vmf)
 	ret |= finish_fault(vmf);
 	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
 					VM_FAULT_RETRY))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(vmf->page);
 		put_page(vmf->page);
 		return ret;
@@ -3675,7 +3990,9 @@ static int do_fault(struct vm_fault *vmf)
 
 	/* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
 	if (!vma->vm_ops->fault)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = VM_FAULT_SIGBUS;
+}
 	else if (!(vmf->flags & FAULT_FLAG_WRITE))
 		ret = do_read_fault(vmf);
 	else if (!(vma->vm_flags & VM_SHARED))
@@ -3685,6 +4002,7 @@ static int do_fault(struct vm_fault *vmf)
 
 	/* preallocated pagetable is unused: free it */
 	if (vmf->prealloc_pte) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_free(vma->vm_mm, vmf->prealloc_pte);
 		vmf->prealloc_pte = NULL;
 	}
@@ -3695,6 +4013,7 @@ static int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
 				unsigned long addr, int page_nid,
 				int *flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	get_page(page);
 
 	count_vm_numa_event(NUMA_HINT_FAULTS);
@@ -3726,6 +4045,7 @@ static int do_numa_page(struct vm_fault *vmf)
 	vmf->ptl = pte_lockptr(vma->vm_mm, vmf->pmd);
 	spin_lock(vmf->ptl);
 	if (unlikely(!pte_same(*vmf->pte, vmf->orig_pte))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		goto out;
 	}
@@ -3801,7 +4121,10 @@ static inline int create_huge_pmd(struct vm_fault *vmf)
 	if (vma_is_anonymous(vmf->vma))
 		return do_huge_pmd_anonymous_page(vmf);
 	if (vmf->vma->vm_ops->huge_fault)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return VM_FAULT_FALLBACK;
 }
 
@@ -3809,8 +4132,11 @@ static int wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)
 {
 	if (vma_is_anonymous(vmf->vma))
 		return do_huge_pmd_wp_page(vmf, orig_pmd);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (vmf->vma->vm_ops->huge_fault)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return vmf->vma->vm_ops->huge_fault(vmf, PE_SIZE_PMD);
+}
 
 	/* COW handled on pte level: split pmd */
 	VM_BUG_ON_VMA(vmf->vma->vm_flags & VM_SHARED, vmf->vma);
@@ -3821,6 +4147,7 @@ static int wp_huge_pmd(struct vm_fault *vmf, pmd_t orig_pmd)
 
 static inline bool vma_is_accessible(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE);
 }
 
@@ -3878,7 +4205,9 @@ static int handle_pte_fault(struct vm_fault *vmf)
 	} else {
 		/* See comment in pte_alloc_one_map() */
 		if (pmd_devmap_trans_unstable(vmf->pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 		/*
 		 * A regular pmd is established and it can't morph into a huge
 		 * pmd from under us anymore at this point because we hold the
@@ -3898,6 +4227,7 @@ static int handle_pte_fault(struct vm_fault *vmf)
 		 */
 		barrier();
 		if (pte_none(vmf->orig_pte)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pte_unmap(vmf->pte);
 			vmf->pte = NULL;
 		}
@@ -3911,10 +4241,15 @@ static int handle_pte_fault(struct vm_fault *vmf)
 	}
 
 	if (!pte_present(vmf->orig_pte))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return do_swap_page(vmf);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pte_protnone(vmf->orig_pte) && vma_is_accessible(vmf->vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return do_numa_page(vmf);
+}
 
 	vmf->ptl = pte_lockptr(vmf->vma->vm_mm, vmf->pmd);
 	spin_lock(vmf->ptl);
@@ -3924,11 +4259,14 @@ static int handle_pte_fault(struct vm_fault *vmf)
 	if (vmf->flags & FAULT_FLAG_WRITE) {
 		if (!pte_write(entry))
 			return do_wp_page(vmf);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		entry = pte_mkdirty(entry);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	entry = pte_mkyoung(entry);
 	if (ptep_set_access_flags(vmf->vma, vmf->address, vmf->pte, entry,
 				vmf->flags & FAULT_FLAG_WRITE)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		update_mmu_cache(vmf->vma, vmf->address, vmf->pte);
 	} else {
 		/*
@@ -3938,7 +4276,9 @@ static int handle_pte_fault(struct vm_fault *vmf)
 		 * with threads.
 		 */
 		if (vmf->flags & FAULT_FLAG_WRITE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			flush_tlb_fix_spurious_fault(vmf->vma, vmf->address);
+}
 	}
 unlock:
 	pte_unmap_unlock(vmf->pte, vmf->ptl);
@@ -3970,15 +4310,21 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	pgd = pgd_offset(mm, address);
 	p4d = p4d_alloc(mm, pgd, address);
 	if (!p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_OOM;
+}
 
 	vmf.pud = pud_alloc(mm, p4d, address);
 	if (!vmf.pud)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_OOM;
+}
 	if (pud_none(*vmf.pud) && transparent_hugepage_enabled(vma)) {
 		ret = create_huge_pud(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 	} else {
 		pud_t orig_pud = *vmf.pud;
 
@@ -3987,11 +4333,16 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 
 			/* NUMA case for anonymous PUDs would go here */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (dirty && !pud_write(orig_pud)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ret = wp_huge_pud(&vmf, orig_pud);
 				if (!(ret & VM_FAULT_FALLBACK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					return ret;
+}
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				huge_pud_set_accessed(&vmf, orig_pud);
 				return 0;
 			}
@@ -4000,30 +4351,42 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 
 	vmf.pmd = pmd_alloc(mm, vmf.pud, address);
 	if (!vmf.pmd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return VM_FAULT_OOM;
+}
 	if (pmd_none(*vmf.pmd) && transparent_hugepage_enabled(vma)) {
 		ret = create_huge_pmd(&vmf);
 		if (!(ret & VM_FAULT_FALLBACK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 	} else {
 		pmd_t orig_pmd = *vmf.pmd;
 
 		barrier();
 		if (unlikely(is_swap_pmd(orig_pmd))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			VM_BUG_ON(thp_migration_supported() &&
 					  !is_pmd_migration_entry(orig_pmd));
 			if (is_pmd_migration_entry(orig_pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				pmd_migration_entry_wait(mm, vmf.pmd);
+}
 			return 0;
 		}
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (pmd_protnone(orig_pmd) && vma_is_accessible(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return do_huge_pmd_numa_page(&vmf, orig_pmd);
+}
 
 			if (dirty && !pmd_write(orig_pmd)) {
 				ret = wp_huge_pmd(&vmf, orig_pmd);
 				if (!(ret & VM_FAULT_FALLBACK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					return ret;
+}
 			} else {
 				huge_pmd_set_accessed(&vmf, orig_pmd);
 				return 0;
@@ -4047,6 +4410,7 @@ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 
 	__set_current_state(TASK_RUNNING);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	count_vm_event(PGFAULT);
 	count_memcg_event_mm(vma->vm_mm, PGFAULT);
 
@@ -4066,7 +4430,9 @@ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		mem_cgroup_oom_enable();
 
 	if (unlikely(is_vm_hugetlb_page(vma)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
+}
 	else
 		ret = __handle_mm_fault(vma, address, flags);
 
@@ -4079,9 +4445,12 @@ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 		 * Just clean up the OOM state peacefully.
 		 */
 		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mem_cgroup_oom_synchronize(false);
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(handle_mm_fault);
@@ -4118,14 +4487,18 @@ int __pud_alloc(struct mm_struct *mm, p4d_t *p4d, unsigned long address)
 {
 	pud_t *new = pud_alloc_one(mm, address);
 	if (!new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	smp_wmb(); /* See comment in __pte_alloc */
 
 	spin_lock(&mm->page_table_lock);
 #ifndef __ARCH_HAS_5LEVEL_HACK
 	if (p4d_present(*p4d))		/* Another has populated it */
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pud_free(mm, new);
+}
 	else
 		p4d_populate(mm, p4d, new);
 #else
@@ -4149,7 +4522,9 @@ int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
 	spinlock_t *ptl;
 	pmd_t *new = pmd_alloc_one(mm, address);
 	if (!new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	smp_wmb(); /* See comment in __pte_alloc */
 
@@ -4159,6 +4534,7 @@ int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
 		mm_inc_nr_pmds(mm);
 		pud_populate(mm, pud, new);
 	} else	/* Another has populated it */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pmd_free(mm, new);
 #else
 	if (!pgd_present(*pud)) {
@@ -4183,6 +4559,7 @@ static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,
 	pte_t *ptep;
 
 	pgd = pgd_offset(mm, address);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
 		goto out;
 
@@ -4281,7 +4658,9 @@ int follow_pfn(struct vm_area_struct *vma, unsigned long address,
 	pte_t *ptep;
 
 	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	ret = follow_pte(vma->vm_mm, address, &ptep, &ptl);
 	if (ret)
@@ -4304,6 +4683,7 @@ int follow_phys(struct vm_area_struct *vma,
 	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
 		goto out;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (follow_pte(vma->vm_mm, address, &ptep, &ptl))
 		goto out;
 	pte = *ptep;
@@ -4330,7 +4710,9 @@ int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
 	int offset = addr & (PAGE_SIZE-1);
 
 	if (follow_phys(vma, addr, write, &prot, &phys_addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	maddr = ioremap_prot(phys_addr, PAGE_ALIGN(len + offset), prot);
 	if (write)
@@ -4373,16 +4755,22 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
 			 * we can access using slightly different code.
 			 */
 			vma = find_vma(mm, addr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!vma || vma->vm_start > addr)
 				break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (vma->vm_ops && vma->vm_ops->access)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ret = vma->vm_ops->access(vma, addr, buf,
 							  len, write);
+}
 			if (ret <= 0)
 				break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bytes = ret;
 #endif
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bytes = len;
 			offset = addr & (PAGE_SIZE-1);
 			if (bytes > PAGE_SIZE-offset)
@@ -4438,7 +4826,9 @@ int access_process_vm(struct task_struct *tsk, unsigned long addr,
 
 	mm = get_task_mm(tsk);
 	if (!mm)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	ret = __access_remote_vm(tsk, mm, addr, buf, len, gup_flags);
 
@@ -4461,7 +4851,9 @@ void print_vma_addr(char *prefix, unsigned long ip)
 	 * contexts (in exception stacks, etc.):
 	 */
 	if (preempt_count())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	down_read(&mm->mmap_sem);
 	vma = find_vma(mm, ip);
@@ -4473,7 +4865,9 @@ void print_vma_addr(char *prefix, unsigned long ip)
 
 			p = file_path(f, buf, PAGE_SIZE);
 			if (IS_ERR(p))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				p = "?";
+}
 			printk("%s%s[%lx+%lx]", prefix, kbasename(p),
 					vma->vm_start,
 					vma->vm_end - vma->vm_start);
@@ -4513,6 +4907,7 @@ static void clear_gigantic_page(struct page *page,
 	int i;
 	struct page *p = page;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	might_sleep();
 	for (i = 0; i < pages_per_huge_page;
 	     i++, p = mem_map_next(p, page, i)) {
@@ -4528,6 +4923,7 @@ void clear_huge_page(struct page *page,
 		~(((unsigned long)pages_per_huge_page << PAGE_SHIFT) - 1);
 
 	if (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		clear_gigantic_page(page, addr, pages_per_huge_page);
 		return;
 	}
@@ -4580,6 +4976,7 @@ static void copy_user_gigantic_page(struct page *dst, struct page *src,
 	struct page *dst_base = dst;
 	struct page *src_base = src;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < pages_per_huge_page; ) {
 		cond_resched();
 		copy_user_highpage(dst, src, addr + i*PAGE_SIZE, vma);
@@ -4597,6 +4994,7 @@ void copy_user_huge_page(struct page *dst, struct page *src,
 	int i;
 
 	if (unlikely(pages_per_huge_page > MAX_ORDER_NR_PAGES)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		copy_user_gigantic_page(dst, src, addr, vma,
 					pages_per_huge_page);
 		return;
@@ -4619,6 +5017,7 @@ long copy_huge_page_from_user(struct page *dst_page,
 	unsigned long i, rc = 0;
 	unsigned long ret_val = pages_per_huge_page * PAGE_SIZE;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < pages_per_huge_page; i++) {
 		if (allow_pagefault)
 			page_kaddr = kmap(dst_page + i);
diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c
index d4b5f29..88b4566 100644
--- a/mm/memory_hotplug.c
+++ b/mm/memory_hotplug.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/memory_hotplug.c
  *
diff --git a/mm/mempool.c b/mm/mempool.c
index c4a23cd..89dfeac 100644
--- a/mm/mempool.c
+++ b/mm/mempool.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  linux/mm/mempool.c
@@ -105,6 +107,7 @@ static inline void poison_element(mempool_t *pool, void *element)
 
 static void kasan_poison_element(mempool_t *pool, void *element)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)
 		kasan_poison_kfree(element);
 	if (pool->alloc == mempool_alloc_pages)
@@ -113,6 +116,7 @@ static void kasan_poison_element(mempool_t *pool, void *element)
 
 static void kasan_unpoison_element(mempool_t *pool, void *element, gfp_t flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pool->alloc == mempool_alloc_slab || pool->alloc == mempool_kmalloc)
 		kasan_unpoison_slab(element);
 	if (pool->alloc == mempool_alloc_pages)
@@ -122,6 +126,7 @@ static void kasan_unpoison_element(mempool_t *pool, void *element, gfp_t flags)
 static void add_element(mempool_t *pool, void *element)
 {
 	BUG_ON(pool->curr_nr >= pool->min_nr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	poison_element(pool, element);
 	kasan_poison_element(pool, element);
 	pool->elements[pool->curr_nr++] = element;
@@ -131,6 +136,7 @@ static void *remove_element(mempool_t *pool, gfp_t flags)
 {
 	void *element = pool->elements[--pool->curr_nr];
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(pool->curr_nr < 0);
 	kasan_unpoison_element(pool, element, flags);
 	check_element(pool, element);
@@ -147,6 +153,7 @@ static void *remove_element(mempool_t *pool, gfp_t flags)
  */
 void mempool_destroy(mempool_t *pool)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(!pool))
 		return;
 
@@ -188,10 +195,13 @@ mempool_t *mempool_create_node(int min_nr, mempool_alloc_t *alloc_fn,
 	mempool_t *pool;
 	pool = kzalloc_node(sizeof(*pool), gfp_mask, node_id);
 	if (!pool)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	pool->elements = kmalloc_node(min_nr * sizeof(void *),
 				      gfp_mask, node_id);
 	if (!pool->elements) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(pool);
 		return NULL;
 	}
@@ -210,11 +220,13 @@ mempool_t *mempool_create_node(int min_nr, mempool_alloc_t *alloc_fn,
 
 		element = pool->alloc(gfp_mask, pool->pool_data);
 		if (unlikely(!element)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mempool_destroy(pool);
 			return NULL;
 		}
 		add_element(pool, element);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return pool;
 }
 EXPORT_SYMBOL(mempool_create_node);
@@ -241,6 +253,7 @@ int mempool_resize(mempool_t *pool, int new_min_nr)
 	void **new_elements;
 	unsigned long flags;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(new_min_nr <= 0);
 	might_sleep();
 
@@ -319,6 +332,7 @@ void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 	VM_WARN_ON_ONCE(gfp_mask & __GFP_ZERO);
 	might_sleep_if(gfp_mask & __GFP_DIRECT_RECLAIM);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	gfp_mask |= __GFP_NOMEMALLOC;	/* don't allocate emergency reserves */
 	gfp_mask |= __GFP_NORETRY;	/* don't loop in __alloc_pages */
 	gfp_mask |= __GFP_NOWARN;	/* failures are OK */
@@ -329,10 +343,15 @@ void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 
 	element = pool->alloc(gfp_temp, pool->pool_data);
 	if (likely(element != NULL))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return element;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irqsave(&pool->lock, flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (likely(pool->curr_nr)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		element = remove_element(pool, gfp_temp);
 		spin_unlock_irqrestore(&pool->lock, flags);
 		/* paired with rmb in mempool_free(), read comment there */
@@ -350,6 +369,7 @@ void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 	 * alloc failed with that and @pool was empty, retry immediately.
 	 */
 	if (gfp_temp != gfp_mask) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irqrestore(&pool->lock, flags);
 		gfp_temp = gfp_mask;
 		goto repeat_alloc;
@@ -357,12 +377,14 @@ void *mempool_alloc(mempool_t *pool, gfp_t gfp_mask)
 
 	/* We must not sleep if !__GFP_DIRECT_RECLAIM */
 	if (!(gfp_mask & __GFP_DIRECT_RECLAIM)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irqrestore(&pool->lock, flags);
 		return NULL;
 	}
 
 	/* Let's wait for someone else to return an element to @pool */
 	init_wait(&wait);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
 
 	spin_unlock_irqrestore(&pool->lock, flags);
@@ -391,7 +413,9 @@ void mempool_free(void *element, mempool_t *pool)
 	unsigned long flags;
 
 	if (unlikely(element == NULL))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * Paired with the wmb in mempool_alloc().  The preceding read is
@@ -427,13 +451,17 @@ void mempool_free(void *element, mempool_t *pool)
 	 * pool waking up the waiters.
 	 */
 	if (unlikely(pool->curr_nr < pool->min_nr)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irqsave(&pool->lock, flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (likely(pool->curr_nr < pool->min_nr)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			add_element(pool, element);
 			spin_unlock_irqrestore(&pool->lock, flags);
 			wake_up(&pool->wait);
 			return;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irqrestore(&pool->lock, flags);
 	}
 	pool->free(element, pool->pool_data);
@@ -471,6 +499,7 @@ EXPORT_SYMBOL(mempool_kmalloc);
 
 void mempool_kfree(void *element, void *pool_data)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kfree(element);
 }
 EXPORT_SYMBOL(mempool_kfree);
diff --git a/mm/mlock.c b/mm/mlock.c
index 46af369..eba0c63 100644
--- a/mm/mlock.c
+++ b/mm/mlock.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *	linux/mm/mlock.c
@@ -29,9 +31,15 @@
 bool can_do_mlock(void)
 {
 	if (rlimit(RLIMIT_MEMLOCK) != 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (capable(CAP_IPC_LOCK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 EXPORT_SYMBOL(can_do_mlock);
@@ -58,6 +66,7 @@ EXPORT_SYMBOL(can_do_mlock);
  */
 void clear_page_mlock(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!TestClearPageMlocked(page))
 		return;
 
@@ -84,15 +93,19 @@ void mlock_vma_page(struct page *page)
 	/* Serialize with page migration */
 	BUG_ON(!PageLocked(page));
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(PageTail(page), page);
 	VM_BUG_ON_PAGE(PageCompound(page) && PageDoubleMap(page), page);
 
 	if (!TestSetPageMlocked(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mod_zone_page_state(page_zone(page), NR_MLOCK,
 				    hpage_nr_pages(page));
 		count_vm_event(UNEVICTABLE_PGMLOCKED);
 		if (!isolate_lru_page(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			putback_lru_page(page);
+}
 	}
 }
 
@@ -102,6 +115,7 @@ void mlock_vma_page(struct page *page)
  */
 static bool __munlock_isolate_lru_page(struct page *page, bool getpage)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (PageLRU(page)) {
 		struct lruvec *lruvec;
 
@@ -149,6 +163,7 @@ static void __munlock_isolated_page(struct page *page)
  */
 static void __munlock_isolation_failed(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (PageUnevictable(page))
 		__count_vm_event(UNEVICTABLE_PGSTRANDED);
 	else
@@ -218,6 +233,7 @@ unsigned int munlock_vma_page(struct page *page)
  */
 static int __mlock_posix_error_return(long retval)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (retval == -EFAULT)
 		retval = -ENOMEM;
 	else if (retval == -ENOMEM)
@@ -240,6 +256,7 @@ static int __mlock_posix_error_return(long retval)
 static bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,
 		int *pgrescued)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 
@@ -262,6 +279,7 @@ static bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,
  */
 static void __putback_lru_fast(struct pagevec *pvec, int pgrescued)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	count_vm_events(UNEVICTABLE_PGMUNLOCKED, pagevec_count(pvec));
 	/*
 	 *__pagevec_lru_add() calls release_pages() so we don't call
@@ -293,6 +311,7 @@ static void __munlock_pagevec(struct pagevec *pvec, struct zone *zone)
 
 	/* Phase 1: page isolation */
 	spin_lock_irq(zone_lru_lock(zone));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < nr; i++) {
 		struct page *page = pvec->pages[i];
 
@@ -439,6 +458,7 @@ static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,
 void munlock_vma_pages_range(struct vm_area_struct *vma,
 			     unsigned long start, unsigned long end)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	vma->vm_flags &= VM_LOCKED_CLEAR_MASK;
 
 	while (start < end) {
@@ -530,17 +550,20 @@ static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,
 			  vma->vm_file, pgoff, vma_policy(vma),
 			  vma->vm_userfaultfd_ctx);
 	if (*prev) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vma = *prev;
 		goto success;
 	}
 
 	if (start != vma->vm_start) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = split_vma(mm, vma, start, 1);
 		if (ret)
 			goto out;
 	}
 
 	if (end != vma->vm_end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = split_vma(mm, vma, end, 0);
 		if (ret)
 			goto out;
@@ -552,9 +575,13 @@ static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,
 	 */
 	nr_pages = (end - start) >> PAGE_SHIFT;
 	if (!lock)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr_pages = -nr_pages;
+}
 	else if (old_flags & VM_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr_pages = 0;
+}
 	mm->locked_vm += nr_pages;
 
 	/*
@@ -584,17 +611,26 @@ static int apply_vma_lock_flags(unsigned long start, size_t len,
 	VM_BUG_ON(len != PAGE_ALIGN(len));
 	end = start + len;
 	if (end < start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	if (end == start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	vma = find_vma(current->mm, start);
 	if (!vma || vma->vm_start > start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	prev = vma->vm_prev;
 	if (start > vma->vm_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		prev = vma;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (nstart = start ; ; ) {
 		vm_flags_t newflags = vma->vm_flags & VM_LOCKED_CLEAR_MASK;
 
@@ -603,22 +639,31 @@ static int apply_vma_lock_flags(unsigned long start, size_t len,
 		/* Here we know that  vma->vm_start <= nstart < vma->vm_end. */
 		tmp = vma->vm_end;
 		if (tmp > end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			tmp = end;
+}
 		error = mlock_fixup(vma, &prev, nstart, tmp, newflags);
 		if (error)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nstart = tmp;
 		if (nstart < prev->vm_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nstart = prev->vm_end;
+}
 		if (nstart >= end)
 			break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vma = prev->vm_next;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!vma || vma->vm_start != nstart) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -ENOMEM;
 			break;
 		}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return error;
 }
 
@@ -636,7 +681,9 @@ static int count_mm_mlocked_page_nr(struct mm_struct *mm,
 	int count = 0;
 
 	if (mm == NULL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mm = current->mm;
+}
 
 	vma = find_vma(mm, start);
 	if (vma == NULL)
@@ -668,7 +715,9 @@ static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla
 	int error = -ENOMEM;
 
 	if (!can_do_mlock())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 
 	lru_add_drain_all();	/* flush pagevec */
 
@@ -680,7 +729,9 @@ static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla
 	locked = len >> PAGE_SHIFT;
 
 	if (down_write_killable(&current->mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 
 	locked += current->mm->locked_vm;
 	if ((locked > lock_limit) && (!capable(CAP_IPC_LOCK))) {
@@ -700,11 +751,16 @@ static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla
 
 	up_write(&current->mm->mmap_sem);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	error = __mm_populate(start, len, 0);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return __mlock_posix_error_return(error);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -718,7 +774,9 @@ SYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)
 	vm_flags_t vm_flags = VM_LOCKED;
 
 	if (flags & ~MLOCK_ONFAULT)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (flags & MLOCK_ONFAULT)
 		vm_flags |= VM_LOCKONFAULT;
@@ -734,7 +792,9 @@ SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)
 	start &= PAGE_MASK;
 
 	if (down_write_killable(&current->mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 	ret = apply_vma_lock_flags(start, len, 0);
 	up_write(&current->mm->mmap_sem);
 
@@ -758,6 +818,7 @@ static int apply_mlockall_flags(int flags)
 
 	current->mm->def_flags &= VM_LOCKED_CLEAR_MASK;
 	if (flags & MCL_FUTURE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		current->mm->def_flags |= VM_LOCKED;
 
 		if (flags & MCL_ONFAULT)
@@ -792,6 +853,7 @@ SYSCALL_DEFINE1(mlockall, int, flags)
 	unsigned long lock_limit;
 	int ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!flags || (flags & ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)))
 		return -EINVAL;
 
@@ -823,7 +885,9 @@ SYSCALL_DEFINE0(munlockall)
 	int ret;
 
 	if (down_write_killable(&current->mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 	ret = apply_mlockall_flags(0);
 	up_write(&current->mm->mmap_sem);
 	return ret;
@@ -843,7 +907,9 @@ int user_shm_lock(size_t size, struct user_struct *user)
 	locked = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	lock_limit = rlimit(RLIMIT_MEMLOCK);
 	if (lock_limit == RLIM_INFINITY)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		allowed = 1;
+}
 	lock_limit >>= PAGE_SHIFT;
 	spin_lock(&shmlock_user_lock);
 	if (!allowed &&
@@ -859,6 +925,7 @@ int user_shm_lock(size_t size, struct user_struct *user)
 
 void user_shm_unlock(size_t size, struct user_struct *user)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&shmlock_user_lock);
 	user->locked_shm -= (size + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	spin_unlock(&shmlock_user_lock);
diff --git a/mm/mm_init.c b/mm/mm_init.c
index 5b72266..efcd173 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm_init.c - Memory initialisation verification and debugging
  *
@@ -27,8 +29,11 @@ void __init mminit_verify_zonelist(void)
 	int nid;
 
 	if (mminit_loglevel < MMINIT_VERIFY)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_online_node(nid) {
 		pg_data_t *pgdat = NODE_DATA(nid);
 		struct zone *zone;
@@ -36,7 +41,9 @@ void __init mminit_verify_zonelist(void)
 		struct zonelist *zonelist;
 		int i, listid, zoneid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(MAX_ZONELISTS > 2);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for (i = 0; i < MAX_ZONELISTS * MAX_NR_ZONES; i++) {
 
 			/* Identify the zone and nodelist */
@@ -60,6 +67,7 @@ void __init mminit_verify_zonelist(void)
 				pr_cont("0:%s ", zone->name);
 #endif /* CONFIG_NUMA */
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_cont("\n");
 		}
 	}
@@ -107,16 +115,25 @@ void __init mminit_verify_pageflags_layout(void)
 		"Last cpupid not in page flags");
 #endif
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (SECTIONS_WIDTH) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		shift -= SECTIONS_WIDTH;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(shift != SECTIONS_PGSHIFT);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (NODES_WIDTH) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		shift -= NODES_WIDTH;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(shift != NODES_PGSHIFT);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ZONES_WIDTH) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		shift -= ZONES_WIDTH;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(shift != ZONES_PGSHIFT);
 	}
 
@@ -127,11 +144,13 @@ void __init mminit_verify_pageflags_layout(void)
 	add_mask = (ZONES_MASK << ZONES_PGSHIFT) +
 			(NODES_MASK << NODES_PGSHIFT) +
 			(SECTIONS_MASK << SECTIONS_PGSHIFT);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(or_mask != add_mask);
 }
 
 static __init int set_mminit_loglevel(char *str)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	get_option(&str, &mminit_loglevel);
 	return 0;
 }
@@ -159,6 +178,7 @@ static void __meminit mm_compute_batch(void)
 static int __meminit mm_compute_batch_notifier(struct notifier_block *self,
 					unsigned long action, void *arg)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	switch (action) {
 	case MEM_ONLINE:
 	case MEM_OFFLINE:
@@ -190,7 +210,9 @@ static int __init mm_sysfs_init(void)
 {
 	mm_kobj = kobject_create_and_add("mm", kernel_kobj);
 	if (!mm_kobj)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	return 0;
 }
diff --git a/mm/mmap.c b/mm/mmap.c
index 0de87a3..2fa1ffa 100644
--- a/mm/mmap.c
+++ b/mm/mmap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/mmap.c
  *
@@ -139,8 +141,10 @@ static void __remove_shared_vm_struct(struct vm_area_struct *vma,
 	if (vma->vm_flags & VM_SHARED)
 		mapping_unmap_writable(mapping);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_dcache_mmap_lock(mapping);
 	vma_interval_tree_remove(vma, &mapping->i_mmap);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_dcache_mmap_unlock(mapping);
 }
 
@@ -172,6 +176,7 @@ static struct vm_area_struct *remove_vma(struct vm_area_struct *vma)
 		vma->vm_ops->close(vma);
 	if (vma->vm_file)
 		fput(vma->vm_file);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mpol_put(vma_policy(vma));
 	kmem_cache_free(vm_area_cachep, vma);
 	return next;
@@ -190,7 +195,9 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)
 	LIST_HEAD(uf);
 
 	if (down_write_killable(&mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 
 #ifdef CONFIG_COMPAT_BRK
 	/*
@@ -245,7 +252,9 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)
 	up_write(&mm->mmap_sem);
 	userfaultfd_unmap_complete(mm, &uf);
 	if (populate)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mm_populate(oldbrk, newbrk - oldbrk);
+}
 	return brk;
 
 out:
@@ -266,6 +275,7 @@ static long vma_compute_subtree_gap(struct vm_area_struct *vma)
 	 */
 	max = vm_start_gap(vma);
 	if (vma->vm_prev) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		prev_end = vm_end_gap(vma->vm_prev);
 		if (max > prev_end)
 			max -= prev_end;
@@ -276,13 +286,17 @@ static long vma_compute_subtree_gap(struct vm_area_struct *vma)
 		subtree_gap = rb_entry(vma->vm_rb.rb_left,
 				struct vm_area_struct, vm_rb)->rb_subtree_gap;
 		if (subtree_gap > max)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			max = subtree_gap;
+}
 	}
 	if (vma->vm_rb.rb_right) {
 		subtree_gap = rb_entry(vma->vm_rb.rb_right,
 				struct vm_area_struct, vm_rb)->rb_subtree_gap;
 		if (subtree_gap > max)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			max = subtree_gap;
+}
 	}
 	return max;
 }
@@ -500,14 +514,18 @@ static int find_vma_links(struct mm_struct *mm, unsigned long addr,
 		struct vm_area_struct *vma_tmp;
 
 		__rb_parent = *__rb_link;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vma_tmp = rb_entry(__rb_parent, struct vm_area_struct, vm_rb);
 
 		if (vma_tmp->vm_end > addr) {
 			/* Fail if an existing vma overlaps the area */
 			if (vma_tmp->vm_start < end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENOMEM;
+}
 			__rb_link = &__rb_parent->rb_left;
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			rb_prev = __rb_parent;
 			__rb_link = &__rb_parent->rb_right;
 		}
@@ -530,7 +548,9 @@ static unsigned long count_vma_pages_range(struct mm_struct *mm,
 	/* Find first overlaping mapping */
 	vma = find_vma_intersection(mm, addr, end);
 	if (!vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	nr_pages = (min(end, vma->vm_end) -
 		max(addr, vma->vm_start)) >> PAGE_SHIFT;
@@ -586,8 +606,10 @@ static void __vma_link_file(struct vm_area_struct *vma)
 		if (vma->vm_flags & VM_SHARED)
 			atomic_inc(&mapping->i_mmap_writable);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_dcache_mmap_lock(mapping);
 		vma_interval_tree_insert(vma, &mapping->i_mmap);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_dcache_mmap_unlock(mapping);
 	}
 }
@@ -616,9 +638,12 @@ static void vma_link(struct mm_struct *mm, struct vm_area_struct *vma,
 	__vma_link_file(vma);
 
 	if (mapping)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		i_mmap_unlock_write(mapping);
+}
 
 	mm->map_count++;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	validate_mm(mm);
 }
 
@@ -668,6 +693,7 @@ static inline void __vma_unlink_prev(struct mm_struct *mm,
 				     struct vm_area_struct *vma,
 				     struct vm_area_struct *prev)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__vma_unlink_common(mm, vma, prev, true, vma);
 }
 
@@ -715,8 +741,10 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 				 */
 				remove_next = 3;
 				VM_WARN_ON(file != next->vm_file);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				swap(vma, next);
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				VM_WARN_ON(expand != vma);
 				/*
 				 * case 1, 6, 7, remove_next == 2 is case 6,
@@ -731,6 +759,7 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 				end = next->vm_end;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			exporter = next;
 			importer = vma;
 
@@ -773,7 +802,9 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 			importer->anon_vma = exporter->anon_vma;
 			error = anon_vma_clone(importer, exporter);
 			if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return error;
+}
 		}
 	}
 again:
@@ -787,6 +818,7 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 		if (adjust_next)
 			uprobe_munmap(next, next->vm_start, next->vm_end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		i_mmap_lock_write(mapping);
 		if (insert) {
 			/*
@@ -803,6 +835,7 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 	if (!anon_vma && adjust_next)
 		anon_vma = next->anon_vma;
 	if (anon_vma) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_WARN_ON(adjust_next && next->anon_vma &&
 			   anon_vma != next->anon_vma);
 		anon_vma_lock_write(anon_vma);
@@ -812,6 +845,7 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 	}
 
 	if (root) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_dcache_mmap_lock(mapping);
 		vma_interval_tree_remove(vma, root);
 		if (adjust_next)
@@ -836,6 +870,7 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 		if (adjust_next)
 			vma_interval_tree_insert(next, root);
 		vma_interval_tree_insert(vma, root);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_dcache_mmap_unlock(mapping);
 	}
 
@@ -881,10 +916,13 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 		anon_vma_interval_tree_post_update_vma(vma);
 		if (adjust_next)
 			anon_vma_interval_tree_post_update_vma(next);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		anon_vma_unlock_write(anon_vma);
 	}
 	if (mapping)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		i_mmap_unlock_write(mapping);
+}
 
 	if (root) {
 		uprobe_mmap(vma);
@@ -899,7 +937,9 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 			fput(file);
 		}
 		if (next->anon_vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			anon_vma_merge(vma, next);
+}
 		mm->map_count--;
 		mpol_put(vma_policy(next));
 		kmem_cache_free(vm_area_cachep, next);
@@ -930,6 +970,7 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 			next = vma;
 		}
 		if (remove_next == 2) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			remove_next = 1;
 			end = next->vm_end;
 			goto again;
@@ -962,8 +1003,10 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 	if (insert && file)
 		uprobe_mmap(insert);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	validate_mm(mm);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -984,13 +1027,22 @@ static inline int is_mergeable_vma(struct vm_area_struct *vma,
 	 * extended instead.
 	 */
 	if ((vma->vm_flags ^ vm_flags) & ~VM_SOFTDIRTY)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	if (vma->vm_file != file)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	if (vma->vm_ops && vma->vm_ops->close)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!is_mergeable_vm_userfaultfd_ctx(vma, vm_userfaultfd_ctx))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	return 1;
 }
 
@@ -1028,8 +1080,11 @@ can_vma_merge_before(struct vm_area_struct *vma, unsigned long vm_flags,
 	if (is_mergeable_vma(vma, file, vm_flags, vm_userfaultfd_ctx) &&
 	    is_mergeable_anon_vma(anon_vma, vma->anon_vma, vma)) {
 		if (vma->vm_pgoff == vm_pgoff)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 1;
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -1051,8 +1106,11 @@ can_vma_merge_after(struct vm_area_struct *vma, unsigned long vm_flags,
 		pgoff_t vm_pglen;
 		vm_pglen = vma_pages(vma);
 		if (vma->vm_pgoff + vm_pglen == vm_pgoff)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 1;
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -1112,12 +1170,15 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,
 	 * so this tests vma->vm_flags & VM_SPECIAL, too.
 	 */
 	if (vm_flags & VM_SPECIAL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (prev)
 		next = prev->vm_next;
 	else
 		next = mm->mmap;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	area = next;
 	if (area && area->vm_end == end)		/* cases 6, 7, 8 */
 		next = next->vm_next;
@@ -1154,7 +1215,9 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,
 			err = __vma_adjust(prev, prev->vm_start,
 					 end, prev->vm_pgoff, NULL, prev);
 		if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 		khugepaged_enter_vma_merge(prev, vm_flags);
 		return prev;
 	}
@@ -1181,11 +1244,14 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,
 			area = next;
 		}
 		if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 		khugepaged_enter_vma_merge(area, vm_flags);
 		return area;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1239,8 +1305,11 @@ static struct anon_vma *reusable_anon_vma(struct vm_area_struct *old, struct vm_
 		struct anon_vma *anon_vma = READ_ONCE(old->anon_vma);
 
 		if (anon_vma && list_is_singular(&old->anon_vma_chain))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return anon_vma;
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1263,7 +1332,9 @@ struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *vma)
 
 	anon_vma = reusable_anon_vma(near, vma, near);
 	if (anon_vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return anon_vma;
+}
 try_prev:
 	near = vma->vm_prev;
 	if (!near)
@@ -1271,7 +1342,9 @@ struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *vma)
 
 	anon_vma = reusable_anon_vma(near, near, vma);
 	if (anon_vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return anon_vma;
+}
 none:
 	/*
 	 * There's no absolute need to look only at touching neighbours:
@@ -1294,6 +1367,7 @@ static inline unsigned long round_hint_to_min(unsigned long hint)
 	if (((void *)hint != NULL) &&
 	    (hint < mmap_min_addr))
 		return PAGE_ALIGN(mmap_min_addr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return hint;
 }
 
@@ -1305,13 +1379,18 @@ static inline int mlock_future_check(struct mm_struct *mm,
 
 	/*  mlock MCL_FUTURE? */
 	if (flags & VM_LOCKED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		locked = len >> PAGE_SHIFT;
 		locked += mm->locked_vm;
 		lock_limit = rlimit(RLIMIT_MEMLOCK);
 		lock_limit >>= PAGE_SHIFT;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EAGAIN;
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -1330,7 +1409,9 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 	*populate = 0;
 
 	if (!len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	/*
 	 * Does the application expect PROT_READ to imply PROT_EXEC?
@@ -1348,27 +1429,38 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 	/* Careful about overflows.. */
 	len = PAGE_ALIGN(len);
 	if (!len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* offset overflow? */
 	if ((pgoff + (len >> PAGE_SHIFT)) < pgoff)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EOVERFLOW;
+}
 
 	/* Too many mappings? */
 	if (mm->map_count > sysctl_max_map_count)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* Obtain the address to map to. we verify (or select) it and ensure
 	 * that it represents a valid section of the address space.
 	 */
 	addr = get_unmapped_area(file, addr, len, pgoff, flags);
 	if (offset_in_page(addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 
 	if (prot == PROT_EXEC) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pkey = execute_only_pkey(mm);
 		if (pkey < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pkey = 0;
+}
 	}
 
 	/* Do simple checking here so the lower-level routines won't have
@@ -1379,32 +1471,45 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
 
 	if (flags & MAP_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!can_do_mlock())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EPERM;
+}
+}
 
 	if (mlock_future_check(mm, vm_flags, len))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EAGAIN;
+}
 
 	if (file) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		struct inode *inode = file_inode(file);
 
 		switch (flags & MAP_TYPE) {
 		case MAP_SHARED:
 			if ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EACCES;
+}
 
 			/*
 			 * Make sure we don't allow writing to an append-only
 			 * file..
 			 */
 			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EACCES;
+}
 
 			/*
 			 * Make sure there are no mandatory locks on the file.
 			 */
 			if (locks_verify_locked(file))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EAGAIN;
+}
 
 			vm_flags |= VM_SHARED | VM_MAYSHARE;
 			if (!(file->f_mode & FMODE_WRITE))
@@ -1413,17 +1518,25 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 			/* fall through */
 		case MAP_PRIVATE:
 			if (!(file->f_mode & FMODE_READ))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EACCES;
+}
 			if (path_noexec(&file->f_path)) {
 				if (vm_flags & VM_EXEC)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					return -EPERM;
+}
 				vm_flags &= ~VM_MAYEXEC;
 			}
 
 			if (!file->f_op->mmap)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENODEV;
+}
 			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EINVAL;
+}
 			break;
 
 		default:
@@ -1433,7 +1546,9 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 		switch (flags & MAP_TYPE) {
 		case MAP_SHARED:
 			if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -EINVAL;
+}
 			/*
 			 * Ignore pgoff.
 			 */
@@ -1462,7 +1577,9 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 
 		/* hugetlb applies strict overcommit unless MAP_NORESERVE */
 		if (file && is_file_hugepages(file))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vm_flags |= VM_NORESERVE;
+}
 	}
 
 	addr = mmap_region(file, addr, len, vm_flags, pgoff, uf);
@@ -1470,6 +1587,7 @@ unsigned long do_mmap(struct file *file, unsigned long addr,
 	    ((vm_flags & VM_LOCKED) ||
 	     (flags & (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))
 		*populate = len;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return addr;
 }
 
@@ -1484,9 +1602,12 @@ SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,
 		audit_mmap_fd(fd, flags);
 		file = fget(fd);
 		if (!file)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EBADF;
+}
 		if (is_file_hugepages(file))
 			len = ALIGN(len, huge_page_size(hstate_file(file)));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		retval = -EINVAL;
 		if (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))
 			goto out_fput;
@@ -1496,7 +1617,9 @@ SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,
 
 		hs = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
 		if (!hs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EINVAL;
+}
 
 		len = ALIGN(len, huge_page_size(hs));
 		/*
@@ -1510,7 +1633,9 @@ SYSCALL_DEFINE6(mmap_pgoff, unsigned long, addr, unsigned long, len,
 				&user, HUGETLB_ANONHUGE_INODE,
 				(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);
 		if (IS_ERR(file))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return PTR_ERR(file);
+}
 	}
 
 	flags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);
@@ -1559,11 +1684,15 @@ int vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)
 
 	/* If it was private or non-writable, the write bit is already clear */
 	if ((vm_flags & (VM_WRITE|VM_SHARED)) != ((VM_WRITE|VM_SHARED)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* The backer wishes to know when pages are first written to? */
 	if (vm_ops && (vm_ops->page_mkwrite || vm_ops->pfn_mkwrite))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	/* The open routine did something to the protections that pgprot_modify
 	 * won't preserve? */
@@ -1573,11 +1702,15 @@ int vma_wants_writenotify(struct vm_area_struct *vma, pgprot_t vm_page_prot)
 
 	/* Do we need to track softdirty? */
 	if (IS_ENABLED(CONFIG_MEM_SOFT_DIRTY) && !(vm_flags & VM_SOFTDIRTY))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	/* Specialty mapping? */
 	if (vm_flags & VM_PFNMAP)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Can the mapping track the dirty pages? */
 	return vma->vm_file && vma->vm_file->f_mapping &&
@@ -1595,7 +1728,9 @@ static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)
 	 * VM_HUGETLB may not be set yet so we cannot check for that flag.
 	 */
 	if (file && is_file_hugepages(file))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	return (vm_flags & (VM_NORESERVE | VM_SHARED | VM_WRITE)) == VM_WRITE;
 }
@@ -1629,16 +1764,21 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	while (find_vma_links(mm, addr, addr + len, &prev, &rb_link,
 			      &rb_parent)) {
 		if (do_munmap(mm, addr, len, uf))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 	}
 
 	/*
 	 * Private writable mapping: check memory availability
 	 */
 	if (accountable_mapping(file, vm_flags)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		charged = len >> PAGE_SHIFT;
 		if (security_vm_enough_memory_mm(mm, charged))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 		vm_flags |= VM_ACCOUNT;
 	}
 
@@ -1657,6 +1797,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 	 */
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 	if (!vma) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = -ENOMEM;
 		goto unacct_error;
 	}
@@ -1722,6 +1863,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 
 	vm_stat_account(mm, vm_flags, len >> PAGE_SHIFT);
 	if (vm_flags & VM_LOCKED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!((vm_flags & VM_SPECIAL) || is_vm_hugetlb_page(vma) ||
 					vma == get_gate_vma(current->mm)))
 			mm->locked_vm += (len >> PAGE_SHIFT);
@@ -1756,12 +1898,16 @@ unsigned long mmap_region(struct file *file, unsigned long addr,
 		mapping_unmap_writable(file->f_mapping);
 allow_write_and_free_vma:
 	if (vm_flags & VM_DENYWRITE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		allow_write_access(file);
+}
 free_vma:
 	kmem_cache_free(vm_area_cachep, vma);
 unacct_error:
 	if (charged)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vm_unacct_memory(charged);
+}
 	return error;
 }
 
@@ -1782,15 +1928,21 @@ unsigned long unmapped_area(struct vm_unmapped_area_info *info)
 	/* Adjust search length to account for worst case alignment overhead */
 	length = info->length + info->align_mask;
 	if (length < info->length)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* Adjust search limits by the desired length */
 	if (info->high_limit < length)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	high_limit = info->high_limit - length;
 
 	if (info->low_limit > high_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	low_limit = info->low_limit + length;
 
 	/* Check if rbtree root looks promising */
@@ -1800,6 +1952,7 @@ unsigned long unmapped_area(struct vm_unmapped_area_info *info)
 	if (vma->rb_subtree_gap < length)
 		goto check_highest;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (true) {
 		/* Visit left subtree if it looks promising */
 		gap_end = vm_start_gap(vma);
@@ -1808,6 +1961,7 @@ unsigned long unmapped_area(struct vm_unmapped_area_info *info)
 				rb_entry(vma->vm_rb.rb_left,
 					 struct vm_area_struct, vm_rb);
 			if (left->rb_subtree_gap >= length) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				vma = left;
 				continue;
 			}
@@ -1817,7 +1971,9 @@ unsigned long unmapped_area(struct vm_unmapped_area_info *info)
 check_current:
 		/* Check if current node has a suitable gap */
 		if (gap_start > high_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 		if (gap_end >= low_limit &&
 		    gap_end > gap_start && gap_end - gap_start >= length)
 			goto found;
@@ -1828,6 +1984,7 @@ unsigned long unmapped_area(struct vm_unmapped_area_info *info)
 				rb_entry(vma->vm_rb.rb_right,
 					 struct vm_area_struct, vm_rb);
 			if (right->rb_subtree_gap >= length) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				vma = right;
 				continue;
 			}
@@ -1853,12 +2010,16 @@ unsigned long unmapped_area(struct vm_unmapped_area_info *info)
 	gap_start = mm->highest_vm_end;
 	gap_end = ULONG_MAX;  /* Only for VM_BUG_ON below */
 	if (gap_start > high_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 found:
 	/* We found a suitable gap. Clip it with the original low_limit. */
 	if (gap_start < info->low_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		gap_start = info->low_limit;
+}
 
 	/* Adjust gap address to the desired alignment */
 	gap_start += (info->align_offset - gap_start) & info->align_mask;
@@ -1877,7 +2038,9 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 	/* Adjust search length to account for worst case alignment overhead */
 	length = info->length + info->align_mask;
 	if (length < info->length)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/*
 	 * Adjust search limits by the desired length.
@@ -1885,11 +2048,15 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 	 */
 	gap_end = info->high_limit;
 	if (gap_end < length)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	high_limit = gap_end - length;
 
 	if (info->low_limit > high_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	low_limit = info->low_limit + length;
 
 	/* Check highest gap, which does not precede any rbtree node */
@@ -1899,11 +2066,16 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 
 	/* Check if rbtree root looks promising */
 	if (RB_EMPTY_ROOT(&mm->mm_rb))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	vma = rb_entry(mm->mm_rb.rb_node, struct vm_area_struct, vm_rb);
 	if (vma->rb_subtree_gap < length)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (true) {
 		/* Visit right subtree if it looks promising */
 		gap_start = vma->vm_prev ? vm_end_gap(vma->vm_prev) : 0;
@@ -1912,6 +2084,7 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 				rb_entry(vma->vm_rb.rb_right,
 					 struct vm_area_struct, vm_rb);
 			if (right->rb_subtree_gap >= length) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				vma = right;
 				continue;
 			}
@@ -1921,7 +2094,9 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 		/* Check if current node has a suitable gap */
 		gap_end = vm_start_gap(vma);
 		if (gap_end < low_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 		if (gap_start <= high_limit &&
 		    gap_end > gap_start && gap_end - gap_start >= length)
 			goto found;
@@ -1932,6 +2107,7 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 				rb_entry(vma->vm_rb.rb_left,
 					 struct vm_area_struct, vm_rb);
 			if (left->rb_subtree_gap >= length) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				vma = left;
 				continue;
 			}
@@ -1941,7 +2117,9 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 		while (true) {
 			struct rb_node *prev = &vma->vm_rb;
 			if (!rb_parent(prev))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENOMEM;
+}
 			vma = rb_entry(rb_parent(prev),
 				       struct vm_area_struct, vm_rb);
 			if (prev == vma->vm_rb.rb_right) {
@@ -1955,7 +2133,9 @@ unsigned long unmapped_area_topdown(struct vm_unmapped_area_info *info)
 found:
 	/* We found a suitable gap. Clip it with the original high_limit. */
 	if (gap_end > info->high_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		gap_end = info->high_limit;
+}
 
 found_highest:
 	/* Compute highest gap address at the desired alignment */
@@ -2077,16 +2257,22 @@ get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
 
 	unsigned long error = arch_mmap_check(addr, len, flags);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	/* Careful about overflows.. */
 	if (len > TASK_SIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	get_area = current->mm->get_unmapped_area;
 	if (file) {
 		if (file->f_op->get_unmapped_area)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			get_area = file->f_op->get_unmapped_area;
+}
 	} else if (flags & MAP_SHARED) {
 		/*
 		 * mmap_region() will call shmem_zero_setup() to create a file,
@@ -2099,12 +2285,18 @@ get_unmapped_area(struct file *file, unsigned long addr, unsigned long len,
 
 	addr = get_area(file, addr, len, pgoff, flags);
 	if (IS_ERR_VALUE(addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 
 	if (addr > TASK_SIZE - len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	if (offset_in_page(addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	error = security_mmap_addr(addr);
 	return error ? error : addr;
@@ -2121,7 +2313,9 @@ struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)
 	/* Check the cache first. */
 	vma = vmacache_find(mm, addr);
 	if (likely(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return vma;
+}
 
 	rb_node = mm->mm_rb.rb_node;
 
@@ -2131,6 +2325,7 @@ struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)
 		tmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);
 
 		if (tmp->vm_end > addr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vma = tmp;
 			if (tmp->vm_start <= addr)
 				break;
@@ -2141,6 +2336,7 @@ struct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)
 
 	if (vma)
 		vmacache_update(addr, vma);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma;
 }
 
@@ -2161,7 +2357,9 @@ find_vma_prev(struct mm_struct *mm, unsigned long addr,
 	} else {
 		struct rb_node *rb_node = mm->mm_rb.rb_node;
 		*pprev = NULL;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		while (rb_node) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*pprev = rb_entry(rb_node, struct vm_area_struct, vm_rb);
 			rb_node = rb_node->rb_right;
 		}
@@ -2182,11 +2380,15 @@ static int acct_stack_growth(struct vm_area_struct *vma,
 
 	/* address space limit tests */
 	if (!may_expand_vm(mm, vma->vm_flags, grow))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* Stack limit test */
 	if (size > rlimit(RLIMIT_STACK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* mlock limit tests */
 	if (vma->vm_flags & VM_LOCKED) {
@@ -2195,22 +2397,29 @@ static int acct_stack_growth(struct vm_area_struct *vma,
 		locked = mm->locked_vm + grow;
 		limit = rlimit(RLIMIT_MEMLOCK);
 		limit >>= PAGE_SHIFT;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (locked > limit && !capable(CAP_IPC_LOCK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 	}
 
 	/* Check to ensure the stack will not grow into a hugetlb-only region */
 	new_start = (vma->vm_flags & VM_GROWSUP) ? vma->vm_start :
 			vma->vm_end - size;
 	if (is_hugepage_only_range(vma->vm_mm, new_start, size))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EFAULT;
+}
 
 	/*
 	 * Overcommit..  This must be the final test, as it will
 	 * update security statistics.
 	 */
 	if (security_vm_enough_memory_mm(mm, grow))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	return 0;
 }
@@ -2321,7 +2530,9 @@ int expand_downwards(struct vm_area_struct *vma,
 	address &= PAGE_MASK;
 	error = security_mmap_addr(address);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	/* Enforce stack_guard_gap */
 	prev = vma->vm_prev;
@@ -2329,12 +2540,16 @@ int expand_downwards(struct vm_area_struct *vma,
 	if (prev && !(prev->vm_flags & VM_GROWSDOWN) &&
 			(prev->vm_flags & (VM_WRITE|VM_READ|VM_EXEC))) {
 		if (address - prev->vm_end < stack_guard_gap)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 	}
 
 	/* We must make sure the anon_vma is allocated. */
 	if (unlikely(anon_vma_prepare(vma)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/*
 	 * vma->vm_start/vm_end cannot change under us because the caller
@@ -2367,7 +2582,9 @@ int expand_downwards(struct vm_area_struct *vma,
 				 */
 				spin_lock(&mm->page_table_lock);
 				if (vma->vm_flags & VM_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					mm->locked_vm += grow;
+}
 				vm_stat_account(mm, vma->vm_flags, grow);
 				anon_vma_interval_tree_pre_update_vma(vma);
 				vma->vm_start = address;
@@ -2382,6 +2599,7 @@ int expand_downwards(struct vm_area_struct *vma,
 	}
 	anon_vma_unlock_write(vma->anon_vma);
 	khugepaged_enter_vma_merge(vma, vma->vm_flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	validate_mm(mm);
 	return error;
 }
@@ -2396,7 +2614,9 @@ static int __init cmdline_parse_stack_guard_gap(char *p)
 
 	val = simple_strtoul(p, &endptr, 10);
 	if (!*endptr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		stack_guard_gap = val << PAGE_SHIFT;
+}
 
 	return 0;
 }
@@ -2438,16 +2658,28 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)
 	addr &= PAGE_MASK;
 	vma = find_vma(mm, addr);
 	if (!vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	if (vma->vm_start <= addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return vma;
+}
 	if (!(vma->vm_flags & VM_GROWSDOWN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	start = vma->vm_start;
 	if (expand_stack(vma, addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	if (vma->vm_flags & VM_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		populate_vma_page_range(vma, addr, start, NULL);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma;
 }
 #endif
@@ -2475,6 +2707,7 @@ static void remove_vma_list(struct mm_struct *mm, struct vm_area_struct *vma)
 		vma = remove_vma(vma);
 	} while (vma);
 	vm_unacct_memory(nr_accounted);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	validate_mm(mm);
 }
 
@@ -2523,7 +2756,9 @@ detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,
 		vma->vm_prev = prev;
 		vma_gap_update(vma);
 	} else
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mm->highest_vm_end = prev ? vm_end_gap(prev) : 0;
+}
 	tail_vma->vm_next = NULL;
 
 	/* Kill the cache */
@@ -2541,14 +2776,19 @@ int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	int err;
 
 	if (vma->vm_ops && vma->vm_ops->split) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = vma->vm_ops->split(vma, addr);
 		if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return err;
+}
 	}
 
 	new = kmem_cache_alloc(vm_area_cachep, GFP_KERNEL);
 	if (!new)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* most fields are the same, copy all, and then fixup */
 	*new = *vma;
@@ -2562,6 +2802,7 @@ int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 		new->vm_pgoff += ((addr - vma->vm_start) >> PAGE_SHIFT);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	err = vma_dup_policy(vma, new);
 	if (err)
 		goto out_free_vma;
@@ -2574,7 +2815,9 @@ int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 		get_file(new->vm_file);
 
 	if (new->vm_ops && new->vm_ops->open)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		new->vm_ops->open(new);
+}
 
 	if (new_below)
 		err = vma_adjust(vma, addr, vma->vm_end, vma->vm_pgoff +
@@ -2584,13 +2827,21 @@ int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 
 	/* Success. */
 	if (!err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Clean everything up if vma_adjust failed. */
 	if (new->vm_ops && new->vm_ops->close)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		new->vm_ops->close(new);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (new->vm_file)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		fput(new->vm_file);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unlink_anon_vmas(new);
  out_free_mpol:
 	mpol_put(vma_policy(new));
@@ -2607,7 +2858,9 @@ int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
 	      unsigned long addr, int new_below)
 {
 	if (mm->map_count >= sysctl_max_map_count)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	return __split_vma(mm, vma, addr, new_below);
 }
@@ -2624,23 +2877,31 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 	struct vm_area_struct *vma, *prev, *last;
 
 	if ((offset_in_page(start)) || start > TASK_SIZE || len > TASK_SIZE-start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	len = PAGE_ALIGN(len);
 	if (len == 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	/* Find the first overlapping VMA */
 	vma = find_vma(mm, start);
 	if (!vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	prev = vma->vm_prev;
 	/* we have  start < vma->vm_end  */
 
 	/* if it doesn't overlap, we have nothing.. */
 	end = start + len;
 	if (vma->vm_start >= end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * If we need to split any vma, do it now to save pain later.
@@ -2658,11 +2919,16 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 		 * its limit temporarily, to help free resources as expected.
 		 */
 		if (end < vma->vm_end && mm->map_count >= sysctl_max_map_count)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 
 		error = __split_vma(mm, vma, start, 0);
 		if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return error;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		prev = vma;
 	}
 
@@ -2671,7 +2937,9 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 	if (last && end > last->vm_start) {
 		int error = __split_vma(mm, last, end, 1);
 		if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return error;
+}
 	}
 	vma = prev ? prev->vm_next : mm->mmap;
 
@@ -2687,7 +2955,9 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 		 */
 		int error = userfaultfd_unmap_prep(vma, start, end, uf);
 		if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return error;
+}
 	}
 
 	/*
@@ -2697,6 +2967,7 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
 		struct vm_area_struct *tmp = vma;
 		while (tmp && tmp->vm_start < end) {
 			if (tmp->vm_flags & VM_LOCKED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				mm->locked_vm -= vma_pages(tmp);
 				munlock_vma_pages_all(tmp);
 			}
@@ -2725,7 +2996,9 @@ int vm_munmap(unsigned long start, size_t len)
 	LIST_HEAD(uf);
 
 	if (down_write_killable(&mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 
 	ret = do_munmap(mm, start, len, &uf);
 	up_write(&mm->mmap_sem);
@@ -2758,7 +3031,9 @@ SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,
 		     current->comm, current->pid);
 
 	if (prot)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 	start = start & PAGE_MASK;
 	size = size & PAGE_MASK;
 
@@ -2866,22 +3141,32 @@ static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long
 
 	len = PAGE_ALIGN(request);
 	if (len < request)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	if (!len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Until we need other flags, refuse anything except VM_EXEC. */
 	if ((flags & (~VM_EXEC)) != 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	flags |= VM_DATA_DEFAULT_FLAGS | VM_ACCOUNT | mm->def_flags;
 
 	error = get_unmapped_area(NULL, addr, len, 0, MAP_FIXED);
 	if (offset_in_page(error))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	error = mlock_future_check(mm, mm->def_flags, len);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	/*
 	 * mm->mmap_sem is required to protect against another thread
@@ -2894,19 +3179,28 @@ static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long
 	 */
 	while (find_vma_links(mm, addr, addr + len, &prev, &rb_link,
 			      &rb_parent)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (do_munmap(mm, addr, len, uf))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 	}
 
 	/* Check against address space limits *after* clearing old maps... */
 	if (!may_expand_vm(mm, flags, len >> PAGE_SHIFT))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (mm->map_count > sysctl_max_map_count)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (security_vm_enough_memory_mm(mm, len >> PAGE_SHIFT))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* Can we just expand an old private anonymous mapping? */
 	vma = vma_merge(mm, prev, addr, addr + len, flags,
@@ -2919,6 +3213,7 @@ static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long
 	 */
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 	if (!vma) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vm_unacct_memory(len >> PAGE_SHIFT);
 		return -ENOMEM;
 	}
@@ -2936,7 +3231,10 @@ static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long
 	mm->total_vm += len >> PAGE_SHIFT;
 	mm->data_vm += len >> PAGE_SHIFT;
 	if (flags & VM_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mm->locked_vm += (len >> PAGE_SHIFT);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	vma->vm_flags |= VM_SOFTDIRTY;
 	return 0;
 }
@@ -2954,20 +3252,26 @@ int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)
 	LIST_HEAD(uf);
 
 	if (down_write_killable(&mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 
 	ret = do_brk_flags(addr, len, flags, &uf);
 	populate = ((mm->def_flags & VM_LOCKED) != 0);
 	up_write(&mm->mmap_sem);
 	userfaultfd_unmap_complete(mm, &uf);
 	if (populate && !ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mm_populate(addr, len);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 EXPORT_SYMBOL(vm_brk_flags);
 
 int vm_brk(unsigned long addr, unsigned long len)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vm_brk_flags(addr, len, 0);
 }
 EXPORT_SYMBOL(vm_brk);
@@ -2983,21 +3287,31 @@ void exit_mmap(struct mm_struct *mm)
 	mmu_notifier_release(mm);
 
 	if (mm->locked_vm) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vma = mm->mmap;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		while (vma) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (vma->vm_flags & VM_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				munlock_vma_pages_all(vma);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vma = vma->vm_next;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	arch_exit_mmap(mm);
 
 	vma = mm->mmap;
 	if (!vma)	/* Can happen if dup_mmap() received an OOM */
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	lru_add_drain();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_cache_mm(mm);
 	tlb_gather_mmu(&tlb, mm, 0, -1);
 	/* update_hiwater_rss(mm) here? but nobody should be looking */
@@ -3161,7 +3475,9 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 bool may_expand_vm(struct mm_struct *mm, vm_flags_t flags, unsigned long npages)
 {
 	if (mm->total_vm + npages > rlimit(RLIMIT_AS) >> PAGE_SHIFT)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (is_data_mapping(flags) &&
 	    mm->data_vm + npages > rlimit(RLIMIT_DATA) >> PAGE_SHIFT) {
@@ -3169,7 +3485,9 @@ bool may_expand_vm(struct mm_struct *mm, vm_flags_t flags, unsigned long npages)
 		if (rlimit(RLIMIT_DATA) == 0 &&
 		    mm->data_vm + npages <= rlimit_max(RLIMIT_DATA) >> PAGE_SHIFT)
 			return true;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!ignore_rlimit_data) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_warn_once("%s (%d): VmData %lu exceed data ulimit %lu. Update limits or use boot option ignore_rlimit_data.\n",
 				     current->comm, current->pid,
 				     (mm->data_vm + npages) << PAGE_SHIFT,
@@ -3178,6 +3496,7 @@ bool may_expand_vm(struct mm_struct *mm, vm_flags_t flags, unsigned long npages)
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 
@@ -3211,6 +3530,7 @@ static int special_mapping_mremap(struct vm_area_struct *new_vma)
 {
 	struct vm_special_mapping *sm = new_vma->vm_private_data;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(current->mm != new_vma->vm_mm))
 		return -EFAULT;
 
@@ -3239,6 +3559,7 @@ static int special_mapping_fault(struct vm_fault *vmf)
 	struct page **pages;
 
 	if (vma->vm_ops == &legacy_special_mapping_vmops) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pages = vma->vm_private_data;
 	} else {
 		struct vm_special_mapping *sm = vma->vm_private_data;
@@ -3246,12 +3567,17 @@ static int special_mapping_fault(struct vm_fault *vmf)
 		if (sm->fault)
 			return sm->fault(sm, vmf->vma, vmf);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pages = sm->pages;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (pgoff = vmf->pgoff; pgoff && *pages; ++pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pgoff--;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (*pages) {
 		struct page *page = *pages;
 		get_page(page);
@@ -3259,6 +3585,7 @@ static int special_mapping_fault(struct vm_fault *vmf)
 		return 0;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return VM_FAULT_SIGBUS;
 }
 
@@ -3273,7 +3600,9 @@ static struct vm_area_struct *__install_special_mapping(
 
 	vma = kmem_cache_zalloc(vm_area_cachep, GFP_KERNEL);
 	if (unlikely(vma == NULL))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	INIT_LIST_HEAD(&vma->anon_vma_chain);
 	vma->vm_mm = mm;
@@ -3304,6 +3633,7 @@ static struct vm_area_struct *__install_special_mapping(
 bool vma_is_special_mapping(const struct vm_area_struct *vma,
 	const struct vm_special_mapping *sm)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma->vm_private_data == sm &&
 		(vma->vm_ops == &special_mapping_vmops ||
 		 vma->vm_ops == &legacy_special_mapping_vmops);
@@ -3331,6 +3661,7 @@ int install_special_mapping(struct mm_struct *mm,
 			    unsigned long addr, unsigned long len,
 			    unsigned long vm_flags, struct page **pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct vm_area_struct *vma = __install_special_mapping(
 		mm, addr, len, vm_flags, (void *)pages,
 		&legacy_special_mapping_vmops);
@@ -3342,6 +3673,7 @@ static DEFINE_MUTEX(mm_all_locks_mutex);
 
 static void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {
 		/*
 		 * The LSB of head.next can't change from under us
@@ -3365,6 +3697,7 @@ static void vm_lock_anon_vma(struct mm_struct *mm, struct anon_vma *anon_vma)
 
 static void vm_lock_mapping(struct mm_struct *mm, struct address_space *mapping)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {
 		/*
 		 * AS_MM_ALL_LOCKS can't change from under us because
@@ -3423,6 +3756,7 @@ int mm_take_all_locks(struct mm_struct *mm)
 	struct vm_area_struct *vma;
 	struct anon_vma_chain *avc;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(down_read_trylock(&mm->mmap_sem));
 
 	mutex_lock(&mm_all_locks_mutex);
@@ -3460,6 +3794,7 @@ int mm_take_all_locks(struct mm_struct *mm)
 
 static void vm_unlock_anon_vma(struct anon_vma *anon_vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(0, (unsigned long *) &anon_vma->root->rb_root.rb_root.rb_node)) {
 		/*
 		 * The LSB of head.next can't change to 0 from under
@@ -3482,6 +3817,7 @@ static void vm_unlock_anon_vma(struct anon_vma *anon_vma)
 
 static void vm_unlock_mapping(struct address_space *mapping)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(AS_MM_ALL_LOCKS, &mapping->flags)) {
 		/*
 		 * AS_MM_ALL_LOCKS can't change to 0 from under us
@@ -3503,6 +3839,7 @@ void mm_drop_all_locks(struct mm_struct *mm)
 	struct vm_area_struct *vma;
 	struct anon_vma_chain *avc;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(down_read_trylock(&mm->mmap_sem));
 	BUG_ON(!mutex_is_locked(&mm_all_locks_mutex));
 
@@ -3634,7 +3971,9 @@ static struct notifier_block reserve_mem_nb = {
 static int __meminit init_reserve_notifier(void)
 {
 	if (register_hotmemory_notifier(&reserve_mem_nb))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("Failed registering memory add/remove notifier for admin reserve\n");
+}
 
 	return 0;
 }
diff --git a/mm/mmzone.c b/mm/mmzone.c
index 4686fdc..b2e1e73 100644
--- a/mm/mmzone.c
+++ b/mm/mmzone.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * linux/mm/mmzone.c
@@ -17,10 +19,14 @@ struct pglist_data *first_online_pgdat(void)
 
 struct pglist_data *next_online_pgdat(struct pglist_data *pgdat)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int nid = next_online_node(pgdat->node_id);
 
 	if (nid == MAX_NUMNODES)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NODE_DATA(nid);
 }
 
@@ -34,9 +40,12 @@ struct zone *next_zone(struct zone *zone)
 	if (zone < pgdat->node_zones + MAX_NR_ZONES - 1)
 		zone++;
 	else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pgdat = next_online_pgdat(pgdat);
 		if (pgdat)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zone = pgdat->node_zones;
+}
 		else
 			zone = NULL;
 	}
diff --git a/mm/mprotect.c b/mm/mprotect.c
index 58b629b..b9c679c 100644
--- a/mm/mprotect.c
+++ b/mm/mprotect.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  mm/mprotect.c
@@ -51,7 +53,9 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 	 * and/or the other way around.
 	 */
 	if (pmd_trans_unstable(pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * The pmd points to a regular pte so the pmd can't change
@@ -81,6 +85,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				struct page *page;
 
 				page = vm_normal_page(vma, addr, oldpte);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (!page || PageKsm(page))
 					continue;
 
@@ -99,12 +104,15 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 			ptent = ptep_modify_prot_start(mm, addr, pte);
 			ptent = pte_modify(ptent, newprot);
 			if (preserve_write)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ptent = pte_mk_savedwrite(ptent);
+}
 
 			/* Avoid taking write faults for known dirty pages */
 			if (dirty_accountable && pte_dirty(ptent) &&
 					(pte_soft_dirty(ptent) ||
 					 !(vma->vm_flags & VM_SOFTDIRTY))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ptent = pte_mkwrite(ptent);
 			}
 			ptep_modify_prot_commit(mm, addr, pte, ptent);
@@ -121,12 +129,16 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				make_migration_entry_read(&entry);
 				newpte = swp_entry_to_pte(entry);
 				if (pte_swp_soft_dirty(oldpte))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					newpte = pte_swp_mksoft_dirty(newpte);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				set_pte_at(mm, addr, pte, newpte);
 
 				pages++;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (is_write_device_private_entry(entry)) {
 				pte_t newpte;
 
@@ -143,6 +155,7 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_unmap_unlock(pte - 1, ptl);
 
 	return pages;
@@ -170,19 +183,25 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 
 		/* invoke the mmu notifier if the pmd is populated */
 		if (!mni_start) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mni_start = addr;
 			mmu_notifier_invalidate_range_start(mm, mni_start, end);
 		}
 
 		if (is_swap_pmd(*pmd) || pmd_trans_huge(*pmd) || pmd_devmap(*pmd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (next - addr != HPAGE_PMD_SIZE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				__split_huge_pmd(vma, pmd, addr, false, NULL);
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				int nr_ptes = change_huge_pmd(vma, pmd, addr,
 						newprot, prot_numa);
 
 				if (nr_ptes) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					if (nr_ptes == HPAGE_PMD_NR) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 						pages += HPAGE_PMD_NR;
 						nr_huge_updates++;
 					}
@@ -203,8 +222,11 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,
 	if (mni_start)
 		mmu_notifier_invalidate_range_end(mm, mni_start, end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (nr_huge_updates)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		count_vm_numa_events(NUMA_HUGE_PTE_UPDATES, nr_huge_updates);
+}
 	return pages;
 }
 
@@ -238,11 +260,13 @@ static inline unsigned long change_p4d_range(struct vm_area_struct *vma,
 
 	p4d = p4d_offset(pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(p4d))
 			continue;
 		pages += change_pud_range(vma, p4d, addr, next, newprot,
 				 dirty_accountable, prot_numa);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (p4d++, addr = next, addr != end);
 
 	return pages;
@@ -260,6 +284,7 @@ static unsigned long change_protection_range(struct vm_area_struct *vma,
 
 	BUG_ON(addr >= end);
 	pgd = pgd_offset(mm, addr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_cache_range(vma, addr, end);
 	inc_tlb_flush_pending(mm);
 	do {
@@ -285,7 +310,9 @@ unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,
 	unsigned long pages;
 
 	if (is_vm_hugetlb_page(vma))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pages = hugetlb_change_protection(vma, start, end, newprot);
+}
 	else
 		pages = change_protection_range(vma, start, end, newprot, dirty_accountable, prot_numa);
 
@@ -322,9 +349,12 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 			return -ENOMEM;
 		if (!(oldflags & (VM_ACCOUNT|VM_WRITE|VM_HUGETLB|
 						VM_SHARED|VM_NORESERVE))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			charged = nrpages;
 			if (security_vm_enough_memory_mm(mm, charged))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return -ENOMEM;
+}
 			newflags |= VM_ACCOUNT;
 		}
 	}
@@ -337,6 +367,7 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
 			   vma->anon_vma, vma->vm_file, pgoff, vma_policy(vma),
 			   vma->vm_userfaultfd_ctx);
 	if (*pprev) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vma = *pprev;
 		VM_WARN_ON((vma->vm_flags ^ newflags) & ~VM_SOFTDIRTY);
 		goto success;
@@ -402,23 +433,36 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 
 	prot &= ~(PROT_GROWSDOWN|PROT_GROWSUP);
 	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can't be both */
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (start & ~PAGE_MASK)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	if (!len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	len = PAGE_ALIGN(len);
 	end = start + len;
 	if (end <= start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	if (!arch_validate_prot(prot))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	reqprot = prot;
 
 	if (down_write_killable(&current->mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 
 	/*
 	 * If userspace did not allocate the pkey, do not let
@@ -436,6 +480,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 	if (unlikely(grows & PROT_GROWSDOWN)) {
 		if (vma->vm_start >= end)
 			goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start = vma->vm_start;
 		error = -EINVAL;
 		if (!(vma->vm_flags & VM_GROWSDOWN))
@@ -444,6 +489,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 		if (vma->vm_start > start)
 			goto out;
 		if (unlikely(grows & PROT_GROWSUP)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			end = vma->vm_end;
 			error = -EINVAL;
 			if (!(vma->vm_flags & VM_GROWSUP))
@@ -453,6 +499,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 	if (start > vma->vm_start)
 		prev = vma;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (nstart = start ; ; ) {
 		unsigned long mask_off_old_flags;
 		unsigned long newflags;
@@ -478,6 +525,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 
 		/* newflags >> 4 shift VM_MAY% in place of VM_% */
 		if ((newflags & ~(newflags >> 4)) & (VM_READ | VM_WRITE | VM_EXEC)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EACCES;
 			goto out;
 		}
@@ -488,22 +536,29 @@ static int do_mprotect_pkey(unsigned long start, size_t len,
 
 		tmp = vma->vm_end;
 		if (tmp > end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			tmp = end;
+}
 		error = mprotect_fixup(vma, &prev, nstart, tmp, newflags);
 		if (error)
 			goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nstart = tmp;
 
 		if (nstart < prev->vm_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nstart = prev->vm_end;
+}
 		if (nstart >= end)
 			goto out;
 
 		vma = prev->vm_next;
 		if (!vma || vma->vm_start != nstart) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -ENOMEM;
 			goto out;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		prot = reqprot;
 	}
 out:
@@ -522,6 +577,7 @@ SYSCALL_DEFINE3(mprotect, unsigned long, start, size_t, len,
 SYSCALL_DEFINE4(pkey_mprotect, unsigned long, start, size_t, len,
 		unsigned long, prot, int, pkey)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return do_mprotect_pkey(start, len, prot, pkey);
 }
 
@@ -532,7 +588,9 @@ SYSCALL_DEFINE2(pkey_alloc, unsigned long, flags, unsigned long, init_val)
 
 	/* No flags supported yet. */
 	if (flags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	/* check for unsupported init values */
 	if (init_val & ~PKEY_ACCESS_MASK)
 		return -EINVAL;
diff --git a/mm/mremap.c b/mm/mremap.c
index 049470a..ca2e707 100644
--- a/mm/mremap.c
+++ b/mm/mremap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *	mm/mremap.c
@@ -39,20 +41,30 @@ static pmd_t *get_old_pmd(struct mm_struct *mm, unsigned long addr)
 
 	pgd = pgd_offset(mm, addr);
 	if (pgd_none_or_clear_bad(pgd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	p4d = p4d_offset(pgd, addr);
 	if (p4d_none_or_clear_bad(p4d))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	pud = pud_offset(p4d, addr);
 	if (pud_none_or_clear_bad(pud))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	pmd = pmd_offset(pud, addr);
 	if (pmd_none(*pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return pmd;
 }
 
@@ -67,15 +79,22 @@ static pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
 	pgd = pgd_offset(mm, addr);
 	p4d = p4d_alloc(mm, pgd, addr);
 	if (!p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	pud = pud_alloc(mm, p4d, addr);
 	if (!pud)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	pmd = pmd_alloc(mm, pud, addr);
 	if (!pmd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(pmd_trans_huge(*pmd));
 
 	return pmd;
@@ -83,6 +102,7 @@ static pmd_t *alloc_new_pmd(struct mm_struct *mm, struct vm_area_struct *vma,
 
 static void take_rmap_locks(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (vma->vm_file)
 		i_mmap_lock_write(vma->vm_file->f_mapping);
 	if (vma->anon_vma)
@@ -91,6 +111,7 @@ static void take_rmap_locks(struct vm_area_struct *vma)
 
 static void drop_rmap_locks(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (vma->anon_vma)
 		anon_vma_unlock_write(vma->anon_vma);
 	if (vma->vm_file)
@@ -142,7 +163,9 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
 	 *   order guarantees that we won't miss both the old and new ptes).
 	 */
 	if (need_rmap_locks)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		take_rmap_locks(vma);
+}
 
 	/*
 	 * We don't have to worry about the ordering of src and dst
@@ -161,6 +184,7 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
 		if (pte_none(*old_pte))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte = ptep_get_and_clear(mm, old_addr, old_pte);
 		/*
 		 * If we are remapping a dirty PTE, make sure
@@ -172,7 +196,10 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
 		 * dirty it after the check and before the removal.
 		 */
 		if (pte_present(pte) && pte_dirty(pte))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			force_flush = true;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte = move_pte(pte, new_vma->vm_page_prot, old_addr, new_addr);
 		pte = move_soft_dirty_pte(pte);
 		set_pte_at(mm, new_addr, new_pte, pte);
@@ -180,16 +207,22 @@ static void move_ptes(struct vm_area_struct *vma, pmd_t *old_pmd,
 
 	arch_leave_lazy_mmu_mode();
 	if (new_ptl != old_ptl)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(new_ptl);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_unmap(new_pte - 1);
 	if (force_flush)
 		flush_tlb_range(vma, old_end - len, old_end);
 	else
 		*need_flush = true;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_unmap_unlock(old_pte - 1, old_ptl);
 	if (need_rmap_locks)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		drop_rmap_locks(vma);
 }
+}
 
 #define LATENCY_LIMIT	(64 * PAGE_SIZE)
 
@@ -205,8 +238,10 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 	unsigned long mmun_end;		/* For mmu_notifiers */
 
 	old_end = old_addr + len;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_cache_range(vma, old_addr, old_end);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mmun_start = old_addr;
 	mmun_end   = old_end;
 	mmu_notifier_invalidate_range_start(vma->vm_mm, mmun_start, mmun_end);
@@ -217,7 +252,9 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 		/* even if next overflowed, extent below will be ok */
 		extent = next - old_addr;
 		if (extent > old_end - old_addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			extent = old_end - old_addr;
+}
 		old_pmd = get_old_pmd(vma->vm_mm, old_addr);
 		if (!old_pmd)
 			continue;
@@ -225,20 +262,29 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 		if (!new_pmd)
 			break;
 		if (is_swap_pmd(*old_pmd) || pmd_trans_huge(*old_pmd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (extent == HPAGE_PMD_SIZE) {
 				bool moved;
 				/* See comment in move_ptes() */
 				if (need_rmap_locks)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					take_rmap_locks(vma);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				moved = move_huge_pmd(vma, old_addr, new_addr,
 						    old_end, old_pmd, new_pmd,
 						    &need_flush);
 				if (need_rmap_locks)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					drop_rmap_locks(vma);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (moved)
 					continue;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			split_huge_pmd(vma, old_pmd, old_addr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (pmd_trans_unstable(old_pmd))
 				continue;
 		}
@@ -246,14 +292,20 @@ unsigned long move_page_tables(struct vm_area_struct *vma,
 			break;
 		next = (new_addr + PMD_SIZE) & PMD_MASK;
 		if (extent > next - new_addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			extent = next - new_addr;
+}
 		if (extent > LATENCY_LIMIT)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			extent = LATENCY_LIMIT;
+}
 		move_ptes(vma, old_pmd, old_addr, old_addr + extent, new_vma,
 			  new_pmd, new_addr, need_rmap_locks, &need_flush);
 	}
 	if (need_flush)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_tlb_range(vma, old_end-len, old_addr);
+}
 
 	mmu_notifier_invalidate_range_end(vma->vm_mm, mmun_start, mmun_end);
 
@@ -282,7 +334,9 @@ static unsigned long move_vma(struct vm_area_struct *vma,
 	 * which may split one vma into three before unmapping.
 	 */
 	if (mm->map_count >= sysctl_max_map_count - 3)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/*
 	 * Advise KSM to break any KSM pages in the area to be moved:
@@ -378,6 +432,7 @@ static unsigned long move_vma(struct vm_area_struct *vma,
 static struct vm_area_struct *vma_to_resize(unsigned long addr,
 	unsigned long old_len, unsigned long new_len, unsigned long *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct mm_struct *mm = current->mm;
 	struct vm_area_struct *vma = find_vma(mm, addr);
 	unsigned long pgoff;
@@ -446,6 +501,7 @@ static unsigned long mremap_to(unsigned long addr, unsigned long old_len,
 		struct list_head *uf_unmap_early,
 		struct list_head *uf_unmap)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct mm_struct *mm = current->mm;
 	struct vm_area_struct *vma;
 	unsigned long ret = -EINVAL;
@@ -504,7 +560,9 @@ static int vma_expandable(struct vm_area_struct *vma, unsigned long delta)
 {
 	unsigned long end = vma->vm_end + delta;
 	if (end < vma->vm_end) /* overflow */
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	if (vma->vm_next && vma->vm_next->vm_start < end) /* intersection */
 		return 0;
 	if (get_unmapped_area(NULL, vma->vm_start, end - vma->vm_start,
@@ -534,13 +592,19 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,
 	LIST_HEAD(uf_unmap);
 
 	if (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	if (flags & MREMAP_FIXED && !(flags & MREMAP_MAYMOVE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	if (offset_in_page(addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	old_len = PAGE_ALIGN(old_len);
 	new_len = PAGE_ALIGN(new_len);
@@ -551,12 +615,17 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,
 	 * a zero new-len is nonsensical.
 	 */
 	if (!new_len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	if (down_write_killable(&current->mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINTR;
+}
 
 	if (flags & MREMAP_FIXED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = mremap_to(addr, old_len, new_addr, new_len,
 				&locked, &uf, &uf_unmap_early, &uf_unmap);
 		goto out;
@@ -571,6 +640,7 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,
 		ret = do_munmap(mm, addr+new_len, old_len - new_len, &uf_unmap);
 		if (ret && old_len != new_len)
 			goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = addr;
 		goto out;
 	}
@@ -580,6 +650,7 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,
 	 */
 	vma = vma_to_resize(addr, old_len, new_len, &charged);
 	if (IS_ERR(vma)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = PTR_ERR(vma);
 		goto out;
 	}
@@ -593,16 +664,20 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,
 
 			if (vma_adjust(vma, vma->vm_start, addr + new_len,
 				       vma->vm_pgoff, NULL)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ret = -ENOMEM;
 				goto out;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vm_stat_account(mm, vma->vm_flags, pages);
 			if (vma->vm_flags & VM_LOCKED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				mm->locked_vm += pages;
 				locked = true;
 				new_addr = addr;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = addr;
 			goto out;
 		}
@@ -616,28 +691,37 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,
 	if (flags & MREMAP_MAYMOVE) {
 		unsigned long map_flags = 0;
 		if (vma->vm_flags & VM_MAYSHARE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			map_flags |= MAP_SHARED;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		new_addr = get_unmapped_area(vma->vm_file, 0, new_len,
 					vma->vm_pgoff +
 					((addr - vma->vm_start) >> PAGE_SHIFT),
 					map_flags);
 		if (offset_in_page(new_addr)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = new_addr;
 			goto out;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = move_vma(vma, addr, old_len, new_len, new_addr,
 			       &locked, &uf, &uf_unmap);
 	}
 out:
 	if (offset_in_page(ret)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vm_unacct_memory(charged);
 		locked = 0;
 	}
 	up_write(&current->mm->mmap_sem);
 	if (locked && new_len > old_len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mm_populate(new_addr + old_len, new_len - old_len);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	userfaultfd_unmap_complete(mm, &uf_unmap_early);
 	mremap_userfaultfd_complete(&uf, addr, new_addr, old_len);
 	userfaultfd_unmap_complete(mm, &uf_unmap);
diff --git a/mm/nobootmem.c b/mm/nobootmem.c
index 9b02fda..7f6feaa 100644
--- a/mm/nobootmem.c
+++ b/mm/nobootmem.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  bootmem - A boot-time physical memory allocator and configurator
@@ -45,23 +47,31 @@ static void * __init __alloc_memory_core_early(int nid, u64 size, u64 align,
 	ulong flags = choose_memblock_flags();
 
 	if (limit > memblock.current_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		limit = memblock.current_limit;
+}
 
 again:
 	addr = memblock_find_in_range_node(size, align, goal, limit, nid,
 					   flags);
 	if (!addr && (flags & MEMBLOCK_MIRROR)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flags &= ~MEMBLOCK_MIRROR;
 		pr_warn("Could not allocate %pap bytes of mirrored memory\n",
 			&size);
 		goto again;
 	}
 	if (!addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (memblock_reserve(addr, size))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ptr = phys_to_virt(addr);
 	memset(ptr, 0, size);
 	/*
@@ -90,6 +100,7 @@ void __init free_bootmem_late(unsigned long addr, unsigned long size)
 	cursor = PFN_UP(addr);
 	end = PFN_DOWN(addr + size);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (; cursor < end; cursor++) {
 		__free_pages_bootmem(pfn_to_page(cursor), cursor, 0);
 		totalram_pages++;
@@ -120,7 +131,9 @@ static unsigned long __init __free_memory_core(phys_addr_t start,
 				      PFN_DOWN(end), max_low_pfn);
 
 	if (start_pfn >= end_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	__free_pages_memory(start_pfn, end_pfn);
 
@@ -165,7 +178,9 @@ void __init reset_all_zones_managed_pages(void)
 	struct pglist_data *pgdat;
 
 	if (reset_managed_pages_done)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	for_each_online_pgdat(pgdat)
 		reset_node_managed_pages(pgdat);
@@ -203,6 +218,7 @@ unsigned long __init free_all_bootmem(void)
 void __init free_bootmem_node(pg_data_t *pgdat, unsigned long physaddr,
 			      unsigned long size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	memblock_free(physaddr, size);
 }
 
@@ -228,20 +244,27 @@ static void * __init ___alloc_bootmem_nopanic(unsigned long size,
 	void *ptr;
 
 	if (WARN_ON_ONCE(slab_is_available()))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return kzalloc(size, GFP_NOWAIT);
+}
 
 restart:
 
 	ptr = __alloc_memory_core_early(NUMA_NO_NODE, size, align, goal, limit);
 
 	if (ptr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ptr;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (goal != 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		goal = 0;
 		goto restart;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -272,12 +295,15 @@ static void * __init ___alloc_bootmem(unsigned long size, unsigned long align,
 	void *mem = ___alloc_bootmem_nopanic(size, align, goal, limit);
 
 	if (mem)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return mem;
+}
 	/*
 	 * Whoops, we cannot satisfy the allocation request.
 	 */
 	pr_alert("bootmem alloc of %lu bytes failed!\n", size);
 	panic("Out of memory");
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -332,6 +358,7 @@ void * __init ___alloc_bootmem_node_nopanic(pg_data_t *pgdat,
 void * __init __alloc_bootmem_node_nopanic(pg_data_t *pgdat, unsigned long size,
 				   unsigned long align, unsigned long goal)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(slab_is_available()))
 		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
 
@@ -346,7 +373,9 @@ static void * __init ___alloc_bootmem_node(pg_data_t *pgdat, unsigned long size,
 
 	ptr = ___alloc_bootmem_node_nopanic(pgdat, size, align, goal, limit);
 	if (ptr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ptr;
+}
 
 	pr_alert("bootmem alloc of %lu bytes failed!\n", size);
 	panic("Out of memory");
@@ -371,6 +400,7 @@ static void * __init ___alloc_bootmem_node(pg_data_t *pgdat, unsigned long size,
 void * __init __alloc_bootmem_node(pg_data_t *pgdat, unsigned long size,
 				   unsigned long align, unsigned long goal)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(slab_is_available()))
 		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
 
@@ -380,6 +410,7 @@ void * __init __alloc_bootmem_node(pg_data_t *pgdat, unsigned long size,
 void * __init __alloc_bootmem_node_high(pg_data_t *pgdat, unsigned long size,
 				   unsigned long align, unsigned long goal)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __alloc_bootmem_node(pgdat, size, align, goal);
 }
 
@@ -400,6 +431,7 @@ void * __init __alloc_bootmem_node_high(pg_data_t *pgdat, unsigned long size,
 void * __init __alloc_bootmem_low(unsigned long size, unsigned long align,
 				  unsigned long goal)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ___alloc_bootmem(size, align, goal, ARCH_LOW_ADDRESS_LIMIT);
 }
 
@@ -407,6 +439,7 @@ void * __init __alloc_bootmem_low_nopanic(unsigned long size,
 					  unsigned long align,
 					  unsigned long goal)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ___alloc_bootmem_nopanic(size, align, goal,
 					ARCH_LOW_ADDRESS_LIMIT);
 }
@@ -429,6 +462,7 @@ void * __init __alloc_bootmem_low_nopanic(unsigned long size,
 void * __init __alloc_bootmem_low_node(pg_data_t *pgdat, unsigned long size,
 				       unsigned long align, unsigned long goal)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(slab_is_available()))
 		return kzalloc_node(size, GFP_NOWAIT, pgdat->node_id);
 
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index 10aed8d..bf011ff 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/oom_kill.c
  * 
@@ -71,6 +73,7 @@ static bool has_intersects_mems_allowed(struct task_struct *start,
 	bool ret = false;
 
 	rcu_read_lock();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_thread(start, tsk) {
 		if (mask) {
 			/*
@@ -115,11 +118,14 @@ struct task_struct *find_lock_task_mm(struct task_struct *p)
 	rcu_read_lock();
 
 	for_each_thread(p, t) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		task_lock(t);
 		if (likely(t->mm))
 			goto found;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		task_unlock(t);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	t = NULL;
 found:
 	rcu_read_unlock();
@@ -133,11 +139,13 @@ struct task_struct *find_lock_task_mm(struct task_struct *p)
  */
 static inline bool is_sysrq_oom(struct oom_control *oc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return oc->order == -1;
 }
 
 static inline bool is_memcg_oom(struct oom_control *oc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return oc->memcg != NULL;
 }
 
@@ -145,6 +153,7 @@ static inline bool is_memcg_oom(struct oom_control *oc)
 static bool oom_unkillable_task(struct task_struct *p,
 		struct mem_cgroup *memcg, const nodemask_t *nodemask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_global_init(p))
 		return true;
 	if (p->flags & PF_KTHREAD)
@@ -177,7 +186,9 @@ unsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,
 	long adj;
 
 	if (oom_unkillable_task(p, memcg, nodemask))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	p = find_lock_task_mm(p);
 	if (!p)
@@ -241,6 +252,7 @@ static enum oom_constraint constrained_alloc(struct oom_control *oc)
 	int nid;
 
 	if (is_memcg_oom(oc)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		oc->totalpages = mem_cgroup_get_limit(oc->memcg) ?: 1;
 		return CONSTRAINT_MEMCG;
 	}
@@ -346,6 +358,7 @@ static int oom_evaluate_task(struct task_struct *task, void *arg)
  */
 static void select_bad_process(struct oom_control *oc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_memcg_oom(oc))
 		mem_cgroup_scan_tasks(oc->memcg, oom_evaluate_task, oc);
 	else {
@@ -379,6 +392,7 @@ static void dump_tasks(struct mem_cgroup *memcg, const nodemask_t *nodemask)
 
 	pr_info("[ pid ]   uid  tgid total_vm      rss nr_ptes nr_pmds swapents oom_score_adj name\n");
 	rcu_read_lock();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_process(p) {
 		if (oom_unkillable_task(p, memcg, nodemask))
 			continue;
@@ -407,6 +421,7 @@ static void dump_tasks(struct mem_cgroup *memcg, const nodemask_t *nodemask)
 
 static void dump_header(struct oom_control *oc, struct task_struct *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pr_warn("%s invoked oom-killer: gfp_mask=%#x(%pGg), nodemask=",
 		current->comm, oc->gfp_mask, &oc->gfp_mask);
 	if (oc->nodemask)
@@ -448,6 +463,7 @@ bool process_shares_mm(struct task_struct *p, struct mm_struct *mm)
 {
 	struct task_struct *t;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_thread(p, t) {
 		struct mm_struct *t_mm = READ_ONCE(t->mm);
 		if (t_mm)
@@ -490,6 +506,7 @@ static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)
 	mutex_lock(&oom_lock);
 
 	if (!down_read_trylock(&mm->mmap_sem)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = false;
 		trace_skip_task_reaping(tsk->pid);
 		goto unlock_oom;
@@ -599,26 +616,33 @@ static void oom_reap_task(struct task_struct *tsk)
 
 static int oom_reaper(void *unused)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (true) {
 		struct task_struct *tsk = NULL;
 
 		wait_event_freezable(oom_reaper_wait, oom_reaper_list != NULL);
 		spin_lock(&oom_reaper_lock);
 		if (oom_reaper_list != NULL) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			tsk = oom_reaper_list;
 			oom_reaper_list = tsk->oom_reaper_list;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(&oom_reaper_lock);
 
 		if (tsk)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			oom_reap_task(tsk);
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
 static void wake_oom_reaper(struct task_struct *tsk)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!oom_reaper_th)
 		return;
 
@@ -640,6 +664,7 @@ static int __init oom_init(void)
 {
 	oom_reaper_th = kthread_run(oom_reaper, NULL, "oom_reaper");
 	if (IS_ERR(oom_reaper_th)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("Unable to start OOM reaper %ld. Continuing regardless\n",
 				PTR_ERR(oom_reaper_th));
 		oom_reaper_th = NULL;
@@ -667,6 +692,7 @@ static void mark_oom_victim(struct task_struct *tsk)
 {
 	struct mm_struct *mm = tsk->mm;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON(oom_killer_disabled);
 	/* OOM killer might race with memcg OOM */
 	if (test_and_set_tsk_thread_flag(tsk, TIF_MEMDIE))
@@ -694,6 +720,7 @@ static void mark_oom_victim(struct task_struct *tsk)
  */
 void exit_oom_victim(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	clear_thread_flag(TIF_MEMDIE);
 
 	if (!atomic_dec_return(&oom_victims))
@@ -705,6 +732,7 @@ void exit_oom_victim(void)
  */
 void oom_killer_enable(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	oom_killer_disabled = false;
 	pr_info("OOM killer enabled.\n");
 }
@@ -733,7 +761,9 @@ bool oom_killer_disable(signed long timeout)
 	 * current is not killed (possibly due to sharing the victim's memory).
 	 */
 	if (mutex_lock_killable(&oom_lock))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 	oom_killer_disabled = true;
 	mutex_unlock(&oom_lock);
 
@@ -758,7 +788,9 @@ static inline bool __task_will_free_mem(struct task_struct *task)
 	 * and release memory.
 	 */
 	if (sig->flags & SIGNAL_GROUP_COREDUMP)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (sig->flags & SIGNAL_GROUP_EXIT)
 		return true;
@@ -788,7 +820,9 @@ static bool task_will_free_mem(struct task_struct *task)
 	 * on that for now. We can consider find_lock_task_mm in future.
 	 */
 	if (!mm)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (!__task_will_free_mem(task))
 		return false;
@@ -843,6 +877,7 @@ static void oom_kill_process(struct oom_control *oc, const char *message)
 	 */
 	task_lock(p);
 	if (task_will_free_mem(p)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mark_oom_victim(p);
 		wake_oom_reaper(p);
 		task_unlock(p);
@@ -964,6 +999,7 @@ static void oom_kill_process(struct oom_control *oc, const char *message)
 static void check_panic_on_oom(struct oom_control *oc,
 			       enum oom_constraint constraint)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (likely(!sysctl_panic_on_oom))
 		return;
 	if (sysctl_panic_on_oom != 2) {
@@ -987,12 +1023,14 @@ static BLOCKING_NOTIFIER_HEAD(oom_notify_list);
 
 int register_oom_notifier(struct notifier_block *nb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return blocking_notifier_chain_register(&oom_notify_list, nb);
 }
 EXPORT_SYMBOL_GPL(register_oom_notifier);
 
 int unregister_oom_notifier(struct notifier_block *nb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return blocking_notifier_chain_unregister(&oom_notify_list, nb);
 }
 EXPORT_SYMBOL_GPL(unregister_oom_notifier);
@@ -1012,7 +1050,9 @@ bool out_of_memory(struct oom_control *oc)
 	enum oom_constraint constraint = CONSTRAINT_NONE;
 
 	if (oom_killer_disabled)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (!is_memcg_oom(oc)) {
 		blocking_notifier_call_chain(&oom_notify_list, 0, &freed);
@@ -1093,7 +1133,9 @@ void pagefault_out_of_memory(void)
 	};
 
 	if (mem_cgroup_oom_synchronize(true))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (!mutex_trylock(&oom_lock))
 		return;
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 0b9c5cb..b86b226 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/page-writeback.c
  *
@@ -222,27 +224,32 @@ static void wb_min_max_ratio(struct bdi_writeback *wb,
 
 static bool mdtc_valid(struct dirty_throttle_control *dtc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
 static struct wb_domain *dtc_dom(struct dirty_throttle_control *dtc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return &global_wb_domain;
 }
 
 static struct dirty_throttle_control *mdtc_gdtc(struct dirty_throttle_control *mdtc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
 static struct fprop_local_percpu *wb_memcg_completions(struct bdi_writeback *wb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
 static void wb_min_max_ratio(struct bdi_writeback *wb,
 			     unsigned long *minp, unsigned long *maxp)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	*minp = wb->bdi->min_ratio;
 	*maxp = wb->bdi->max_ratio;
 }
@@ -279,6 +286,7 @@ static unsigned long node_dirtyable_memory(struct pglist_data *pgdat)
 	unsigned long nr_pages = 0;
 	int z;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (z = 0; z < MAX_NR_ZONES; z++) {
 		struct zone *zone = pgdat->node_zones + z;
 
@@ -375,7 +383,9 @@ static unsigned long global_dirtyable_memory(void)
 	x += global_node_page_state(NR_ACTIVE_FILE);
 
 	if (!vm_highmem_is_dirtyable)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		x -= highmem_dirtyable_memory(x);
+}
 
 	return x + 1;	/* Ensure that we never return 0 */
 }
@@ -415,28 +425,40 @@ static void domain_dirty_limits(struct dirty_throttle_control *dtc)
 		 * number of pages.
 		 */
 		if (bytes)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ratio = min(DIV_ROUND_UP(bytes, global_avail),
 				    PAGE_SIZE);
+}
 		if (bg_bytes)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bg_ratio = min(DIV_ROUND_UP(bg_bytes, global_avail),
 				       PAGE_SIZE);
+}
 		bytes = bg_bytes = 0;
 	}
 
 	if (bytes)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		thresh = DIV_ROUND_UP(bytes, PAGE_SIZE);
+}
 	else
 		thresh = (ratio * available_memory) / PAGE_SIZE;
 
 	if (bg_bytes)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bg_thresh = DIV_ROUND_UP(bg_bytes, PAGE_SIZE);
+}
 	else
 		bg_thresh = (bg_ratio * available_memory) / PAGE_SIZE;
 
 	if (bg_thresh >= thresh)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bg_thresh = thresh / 2;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	tsk = current;
 	if (tsk->flags & PF_LESS_THROTTLE || rt_task(tsk)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bg_thresh += bg_thresh / 4 + global_wb_domain.dirty_limit / 32;
 		thresh += thresh / 4 + global_wb_domain.dirty_limit / 32;
 	}
@@ -476,6 +498,7 @@ void global_dirty_limits(unsigned long *pbackground, unsigned long *pdirty)
  */
 static unsigned long node_dirty_limit(struct pglist_data *pgdat)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long node_memory = node_dirtyable_memory(pgdat);
 	struct task_struct *tsk = current;
 	unsigned long dirty;
@@ -501,6 +524,7 @@ static unsigned long node_dirty_limit(struct pglist_data *pgdat)
  */
 bool node_dirty_ok(struct pglist_data *pgdat)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long limit = node_dirty_limit(pgdat);
 	unsigned long nr_pages = 0;
 
@@ -518,6 +542,7 @@ int dirty_background_ratio_handler(struct ctl_table *table, int write,
 	int ret;
 
 	ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret == 0 && write)
 		dirty_background_bytes = 0;
 	return ret;
@@ -530,6 +555,7 @@ int dirty_background_bytes_handler(struct ctl_table *table, int write,
 	int ret;
 
 	ret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret == 0 && write)
 		dirty_background_ratio = 0;
 	return ret;
@@ -543,6 +569,7 @@ int dirty_ratio_handler(struct ctl_table *table, int write,
 	int ret;
 
 	ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret == 0 && write && vm_dirty_ratio != old_ratio) {
 		writeback_set_ratelimit();
 		vm_dirty_bytes = 0;
@@ -558,6 +585,7 @@ int dirty_bytes_handler(struct ctl_table *table, int write,
 	int ret;
 
 	ret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret == 0 && write && vm_dirty_bytes != old_bytes) {
 		writeback_set_ratelimit();
 		vm_dirty_ratio = 0;
@@ -567,6 +595,7 @@ int dirty_bytes_handler(struct ctl_table *table, int write,
 
 static unsigned long wp_next_time(unsigned long cur_time)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	cur_time += VM_COMPLETIONS_PERIOD_LEN;
 	/* 0 has a special meaning... */
 	if (!cur_time)
@@ -578,6 +607,7 @@ static void wb_domain_writeout_inc(struct wb_domain *dom,
 				   struct fprop_local_percpu *completions,
 				   unsigned int max_prop_frac)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__fprop_inc_percpu_max(&dom->completions, completions,
 			       max_prop_frac);
 	/* First event after period switching was turned off? */
@@ -607,14 +637,17 @@ static inline void __wb_writeout_inc(struct bdi_writeback *wb)
 
 	cgdom = mem_cgroup_wb_domain(wb);
 	if (cgdom)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		wb_domain_writeout_inc(cgdom, wb_memcg_completions(wb),
 				       wb->bdi->max_prop_frac);
 }
+}
 
 void wb_writeout_inc(struct bdi_writeback *wb)
 {
 	unsigned long flags;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	local_irq_save(flags);
 	__wb_writeout_inc(wb);
 	local_irq_restore(flags);
@@ -632,6 +665,7 @@ static void writeout_period(unsigned long t)
 						 VM_COMPLETIONS_PERIOD_LEN;
 
 	if (fprop_new_period(&dom->completions, miss_periods + 1)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		dom->period_time = wp_next_time(dom->period_time +
 				miss_periods * VM_COMPLETIONS_PERIOD_LEN);
 		mod_timer(&dom->period_timer, dom->period_time);
@@ -646,6 +680,7 @@ static void writeout_period(unsigned long t)
 
 int wb_domain_init(struct wb_domain *dom, gfp_t gfp)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	memset(dom, 0, sizeof(*dom));
 
 	spin_lock_init(&dom->lock);
@@ -679,6 +714,7 @@ int bdi_set_min_ratio(struct backing_dev_info *bdi, unsigned int min_ratio)
 
 	spin_lock_bh(&bdi_lock);
 	if (min_ratio > bdi->max_ratio) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = -EINVAL;
 	} else {
 		min_ratio -= bdi->min_ratio;
@@ -699,7 +735,9 @@ int bdi_set_max_ratio(struct backing_dev_info *bdi, unsigned max_ratio)
 	int ret = 0;
 
 	if (max_ratio > 100)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	spin_lock_bh(&bdi_lock);
 	if (bdi->min_ratio > max_ratio) {
@@ -717,12 +755,14 @@ EXPORT_SYMBOL(bdi_set_max_ratio);
 static unsigned long dirty_freerun_ceiling(unsigned long thresh,
 					   unsigned long bg_thresh)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (thresh + bg_thresh) / 2;
 }
 
 static unsigned long hard_dirty_limit(struct wb_domain *dom,
 				      unsigned long thresh)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return max(thresh, dom->dirty_limit);
 }
 
@@ -733,6 +773,7 @@ static unsigned long hard_dirty_limit(struct wb_domain *dom,
 static void mdtc_calc_avail(struct dirty_throttle_control *mdtc,
 			    unsigned long filepages, unsigned long headroom)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct dirty_throttle_control *gdtc = mdtc_gdtc(mdtc);
 	unsigned long clean = filepages - min(filepages, mdtc->dirty);
 	unsigned long global_clean = gdtc->avail - min(gdtc->avail, gdtc->dirty);
@@ -764,6 +805,7 @@ static void mdtc_calc_avail(struct dirty_throttle_control *mdtc,
  */
 static unsigned long __wb_calc_thresh(struct dirty_throttle_control *dtc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct wb_domain *dom = dtc_dom(dtc);
 	unsigned long thresh = dtc->thresh;
 	u64 wb_thresh;
@@ -824,6 +866,7 @@ static long long pos_ratio_polynom(unsigned long setpoint,
 	pos_ratio = pos_ratio * x >> RATELIMIT_CALC_SHIFT;
 	pos_ratio += 1 << RATELIMIT_CALC_SHIFT;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return clamp(pos_ratio, 0LL, 2LL << RATELIMIT_CALC_SHIFT);
 }
 
@@ -919,7 +962,9 @@ static void wb_position_ratio(struct dirty_throttle_control *dtc)
 	dtc->pos_ratio = 0;
 
 	if (unlikely(dtc->dirty >= limit))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * global setpoint
@@ -1085,6 +1130,7 @@ static void wb_update_write_bandwidth(struct bdi_writeback *wb,
 				      unsigned long elapsed,
 				      unsigned long written)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const unsigned long period = roundup_pow_of_two(3 * HZ);
 	unsigned long avg = wb->avg_write_bandwidth;
 	unsigned long old = wb->write_bandwidth;
@@ -1133,6 +1179,7 @@ static void wb_update_write_bandwidth(struct bdi_writeback *wb,
 
 static void update_dirty_limit(struct dirty_throttle_control *dtc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct wb_domain *dom = dtc_dom(dtc);
 	unsigned long thresh = dtc->thresh;
 	unsigned long limit = dom->dirty_limit;
@@ -1163,6 +1210,7 @@ static void update_dirty_limit(struct dirty_throttle_control *dtc)
 static void domain_update_bandwidth(struct dirty_throttle_control *dtc,
 				    unsigned long now)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct wb_domain *dom = dtc_dom(dtc);
 
 	/*
@@ -1252,7 +1300,9 @@ static void wb_update_dirty_ratelimit(struct dirty_throttle_control *dtc,
 	 * balanced_dirty_ratelimit ~= (write_bw / N) <= write_bw
 	 */
 	if (unlikely(balanced_dirty_ratelimit > write_bw))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		balanced_dirty_ratelimit = write_bw;
+}
 
 	/*
 	 * We could safely do this and return immediately:
@@ -1354,6 +1404,7 @@ static void __wb_update_bandwidth(struct dirty_throttle_control *gdtc,
 	unsigned long dirtied;
 	unsigned long written;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&wb->list_lock);
 
 	/*
@@ -1411,6 +1462,7 @@ void wb_update_bandwidth(struct bdi_writeback *wb, unsigned long start_time)
 static unsigned long dirty_poll_interval(unsigned long dirty,
 					 unsigned long thresh)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (thresh > dirty)
 		return 1UL << (ilog2(thresh - dirty) >> 1);
 
@@ -1442,6 +1494,7 @@ static long wb_min_pause(struct bdi_writeback *wb,
 			 unsigned long dirty_ratelimit,
 			 int *nr_dirtied_pause)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	long hi = ilog2(wb->avg_write_bandwidth);
 	long lo = ilog2(wb->dirty_ratelimit);
 	long t;		/* target pause */
@@ -1530,6 +1583,7 @@ static inline void wb_dirty_limits(struct dirty_throttle_control *dtc)
 	 *   at some rate <= (write_bw / 2) for bringing down wb_dirty.
 	 */
 	dtc->wb_thresh = __wb_calc_thresh(dtc);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	dtc->wb_bg_thresh = dtc->thresh ?
 		div_u64((u64)dtc->wb_thresh * dtc->bg_thresh, dtc->thresh) : 0;
 
@@ -1566,6 +1620,7 @@ static void balance_dirty_pages(struct address_space *mapping,
 	struct dirty_throttle_control gdtc_stor = { GDTC_INIT(wb) };
 	struct dirty_throttle_control mdtc_stor = { MDTC_INIT(wb, &gdtc_stor) };
 	struct dirty_throttle_control * const gdtc = &gdtc_stor;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct dirty_throttle_control * const mdtc = mdtc_valid(&mdtc_stor) ?
 						     &mdtc_stor : NULL;
 	struct dirty_throttle_control *sdtc;
@@ -1870,17 +1925,29 @@ void balance_dirty_pages_ratelimited(struct address_space *mapping)
 	int *p;
 
 	if (!bdi_cap_account_dirty(bdi))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (inode_cgwb_enabled(inode))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		wb = wb_get_create_current(bdi, GFP_KERNEL);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!wb)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		wb = &bdi->wb;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ratelimit = current->nr_dirtied_pause;
 	if (wb->dirty_exceeded)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ratelimit = min(ratelimit, 32 >> (PAGE_SHIFT - 10));
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	preempt_disable();
 	/*
 	 * This prevents one CPU to accumulate too many dirtied pages without
@@ -1890,8 +1957,12 @@ void balance_dirty_pages_ratelimited(struct address_space *mapping)
 	 */
 	p =  this_cpu_ptr(&bdp_ratelimits);
 	if (unlikely(current->nr_dirtied >= ratelimit))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*p = 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	else if (unlikely(*p >= ratelimit_pages)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*p = 0;
 		ratelimit = 0;
 	}
@@ -1901,17 +1972,23 @@ void balance_dirty_pages_ratelimited(struct address_space *mapping)
 	 * the dirty throttling and livelock other long-run dirtiers.
 	 */
 	p = this_cpu_ptr(&dirty_throttle_leaks);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (*p > 0 && current->nr_dirtied < ratelimit) {
 		unsigned long nr_pages_dirtied;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr_pages_dirtied = min(*p, ratelimit - current->nr_dirtied);
 		*p -= nr_pages_dirtied;
 		current->nr_dirtied += nr_pages_dirtied;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	preempt_enable();
 
 	if (unlikely(current->nr_dirtied >= ratelimit))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		balance_dirty_pages(mapping, wb, current->nr_dirtied);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	wb_put(wb);
 }
 EXPORT_SYMBOL(balance_dirty_pages_ratelimited);
@@ -1928,6 +2005,7 @@ bool wb_over_bg_thresh(struct bdi_writeback *wb)
 	struct dirty_throttle_control gdtc_stor = { GDTC_INIT(wb) };
 	struct dirty_throttle_control mdtc_stor = { MDTC_INIT(wb, &gdtc_stor) };
 	struct dirty_throttle_control * const gdtc = &gdtc_stor;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct dirty_throttle_control * const mdtc = mdtc_valid(&mdtc_stor) ?
 						     &mdtc_stor : NULL;
 
@@ -1972,6 +2050,7 @@ bool wb_over_bg_thresh(struct bdi_writeback *wb)
 int dirty_writeback_centisecs_handler(struct ctl_table *table, int write,
 	void __user *buffer, size_t *length, loff_t *ppos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	proc_dointvec(table, write, buffer, length, ppos);
 	return 0;
 }
@@ -1989,7 +2068,9 @@ void laptop_mode_timer_fn(unsigned long data)
 	 * threshold
 	 */
 	if (!bdi_has_dirty_io(q->backing_dev_info))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	rcu_read_lock();
 	list_for_each_entry_rcu(wb, &q->backing_dev_info->wb_list, bdi_node)
@@ -2006,6 +2087,7 @@ void laptop_mode_timer_fn(unsigned long data)
  */
 void laptop_io_completion(struct backing_dev_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mod_timer(&info->laptop_mode_wb_timer, jiffies + laptop_mode);
 }
 
@@ -2020,6 +2102,7 @@ void laptop_sync_completion(void)
 
 	rcu_read_lock();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry_rcu(bdi, &bdi_list, bdi_list)
 		del_timer(&bdi->laptop_mode_wb_timer);
 
@@ -2048,8 +2131,10 @@ void writeback_set_ratelimit(void)
 	dom->dirty_limit = dirty_thresh;
 	ratelimit_pages = dirty_thresh / (num_online_cpus() * 32);
 	if (ratelimit_pages < 16)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ratelimit_pages = 16;
 }
+}
 
 static int page_writeback_cpu_online(unsigned int cpu)
 {
@@ -2113,18 +2198,22 @@ void tag_pages_for_writeback(struct address_space *mapping,
 	spin_lock_irq(&mapping->tree_lock);
 	radix_tree_for_each_tagged(slot, &mapping->page_tree, &iter, start,
 							PAGECACHE_TAG_DIRTY) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (iter.index > end)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		radix_tree_iter_tag_set(&mapping->page_tree, &iter,
 							PAGECACHE_TAG_TOWRITE);
 		tagged++;
 		if ((tagged % WRITEBACK_TAG_BATCH) != 0)
 			continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		slot = radix_tree_iter_resume(slot, &iter);
 		spin_unlock_irq(&mapping->tree_lock);
 		cond_resched();
 		spin_lock_irq(&mapping->tree_lock);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock_irq(&mapping->tree_lock);
 }
 EXPORT_SYMBOL(tag_pages_for_writeback);
@@ -2169,22 +2258,31 @@ int write_cache_pages(struct address_space *mapping,
 
 	pagevec_init(&pvec, 0);
 	if (wbc->range_cyclic) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		writeback_index = mapping->writeback_index; /* prev offset */
 		index = writeback_index;
 		if (index == 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			cycled = 1;
+}
 		else
 			cycled = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		end = -1;
 	} else {
 		index = wbc->range_start >> PAGE_SHIFT;
 		end = wbc->range_end >> PAGE_SHIFT;
 		if (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			range_whole = 1;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		cycled = 1; /* ignore range_cyclic tests */
 	}
 	if (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		tag = PAGECACHE_TAG_TOWRITE;
+}
 	else
 		tag = PAGECACHE_TAG_DIRTY;
 retry:
@@ -2199,6 +2297,7 @@ int write_cache_pages(struct address_space *mapping,
 		if (nr_pages == 0)
 			break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for (i = 0; i < nr_pages; i++) {
 			struct page *page = pvec.pages[i];
 
@@ -2218,6 +2317,7 @@ int write_cache_pages(struct address_space *mapping,
 				break;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			done_index = page->index;
 
 			lock_page(page);
@@ -2236,26 +2336,36 @@ int write_cache_pages(struct address_space *mapping,
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!PageDirty(page)) {
 				/* someone wrote it for us */
 				goto continue_unlock;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (PageWriteback(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (wbc->sync_mode != WB_SYNC_NONE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					wait_on_page_writeback(page);
+}
 				else
 					goto continue_unlock;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			BUG_ON(PageWriteback(page));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!clear_page_dirty_for_io(page))
 				goto continue_unlock;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			trace_wbc_writepage(wbc, inode_to_bdi(mapping->host));
 			ret = (*writepage)(page, wbc, data);
 			if (unlikely(ret)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (ret == AOP_WRITEPAGE_ACTIVATE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					unlock_page(page);
 					ret = 0;
 				} else {
@@ -2282,10 +2392,12 @@ int write_cache_pages(struct address_space *mapping,
 			 */
 			if (--wbc->nr_to_write <= 0 &&
 			    wbc->sync_mode == WB_SYNC_NONE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				done = 1;
 				break;
 			}
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pagevec_release(&pvec);
 		cond_resched();
 	}
@@ -2336,7 +2448,9 @@ int generic_writepages(struct address_space *mapping,
 
 	/* deal with chardevs and other special file */
 	if (!mapping->a_ops->writepage)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	blk_start_plug(&plug);
 	ret = write_cache_pages(mapping, wbc, __writepage, mapping);
@@ -2351,7 +2465,10 @@ int do_writepages(struct address_space *mapping, struct writeback_control *wbc)
 	int ret;
 
 	if (wbc->nr_to_write <= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (1) {
 		if (mapping->a_ops->writepages)
 			ret = mapping->a_ops->writepages(mapping, wbc);
@@ -2359,9 +2476,11 @@ int do_writepages(struct address_space *mapping, struct writeback_control *wbc)
 			ret = generic_writepages(mapping, wbc);
 		if ((ret != -ENOMEM) || (wbc->sync_mode != WB_SYNC_ALL))
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		cond_resched();
 		congestion_wait(BLK_RW_ASYNC, HZ/50);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -2383,6 +2502,7 @@ int write_one_page(struct page *page)
 		.nr_to_write = 1,
 	};
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!PageLocked(page));
 
 	wait_on_page_writeback(page);
@@ -2410,6 +2530,7 @@ int __set_page_dirty_no_writeback(struct page *page)
 {
 	if (!PageDirty(page))
 		return !TestSetPageDirty(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -2439,6 +2560,7 @@ void account_page_dirtied(struct page *page, struct address_space *mapping)
 		inc_wb_stat(wb, WB_DIRTIED);
 		task_io_account_write(PAGE_SIZE);
 		current->nr_dirtied++;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		this_cpu_inc(bdp_ratelimits);
 	}
 }
@@ -2452,6 +2574,7 @@ EXPORT_SYMBOL(account_page_dirtied);
 void account_page_cleaned(struct page *page, struct address_space *mapping,
 			  struct bdi_writeback *wb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (mapping_cap_account_dirty(mapping)) {
 		dec_lruvec_page_state(page, NR_FILE_DIRTY);
 		dec_zone_page_state(page, NR_ZONE_WRITE_PENDING);
@@ -2474,6 +2597,7 @@ void account_page_cleaned(struct page *page, struct address_space *mapping,
  */
 int __set_page_dirty_nobuffers(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lock_page_memcg(page);
 	if (!TestSetPageDirty(page)) {
 		struct address_space *mapping = page_mapping(page);
@@ -2515,6 +2639,7 @@ void account_page_redirty(struct page *page)
 {
 	struct address_space *mapping = page->mapping;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (mapping && mapping_cap_account_dirty(mapping)) {
 		struct inode *inode = mapping->host;
 		struct bdi_writeback *wb;
@@ -2574,17 +2699,24 @@ int set_page_dirty(struct page *page)
 		 * process. But it's a trivial problem.
 		 */
 		if (PageReclaim(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ClearPageReclaim(page);
+}
 #ifdef CONFIG_BLOCK
 		if (!spd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spd = __set_page_dirty_buffers;
+}
 #endif
 		return (*spd)(page);
 	}
 	if (!PageDirty(page)) {
 		if (!TestSetPageDirty(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 1;
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 EXPORT_SYMBOL(set_page_dirty);
@@ -2636,11 +2768,14 @@ void cancel_dirty_page(struct page *page)
 		wb = unlocked_inode_to_wb_begin(inode, &locked);
 
 		if (TestClearPageDirty(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			account_page_cleaned(page, mapping, wb);
+}
 
 		unlocked_inode_to_wb_end(inode, locked);
 		unlock_page_memcg(page);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ClearPageDirty(page);
 	}
 }
@@ -2662,6 +2797,7 @@ EXPORT_SYMBOL(cancel_dirty_page);
  */
 int clear_page_dirty_for_io(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct address_space *mapping = page_mapping(page);
 	int ret = 0;
 
@@ -2723,6 +2859,7 @@ EXPORT_SYMBOL(clear_page_dirty_for_io);
 
 int test_clear_page_writeback(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct address_space *mapping = page_mapping(page);
 	struct mem_cgroup *memcg;
 	struct lruvec *lruvec;
@@ -2774,6 +2911,7 @@ int test_clear_page_writeback(struct page *page)
 
 int __test_set_page_writeback(struct page *page, bool keep_write)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct address_space *mapping = page_mapping(page);
 	int ret;
 
@@ -2833,6 +2971,7 @@ EXPORT_SYMBOL(__test_set_page_writeback);
  */
 int mapping_tagged(struct address_space *mapping, int tag)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return radix_tree_tagged(&mapping->page_tree, tag);
 }
 EXPORT_SYMBOL(mapping_tagged);
@@ -2847,6 +2986,7 @@ EXPORT_SYMBOL(mapping_tagged);
  */
 void wait_for_stable_page(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (bdi_cap_stable_pages_required(inode_to_bdi(page->mapping->host)))
 		wait_on_page_writeback(page);
 }
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 6627cae..9bdecbd 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/page_alloc.c
  *
@@ -162,6 +164,7 @@ static gfp_t saved_gfp_mask;
 
 void pm_restore_gfp_mask(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON(!mutex_is_locked(&pm_mutex));
 	if (saved_gfp_mask) {
 		gfp_allowed_mask = saved_gfp_mask;
@@ -171,6 +174,7 @@ void pm_restore_gfp_mask(void)
 
 void pm_restrict_gfp_mask(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON(!mutex_is_locked(&pm_mutex));
 	WARN_ON(saved_gfp_mask);
 	saved_gfp_mask = gfp_allowed_mask;
@@ -179,6 +183,7 @@ void pm_restrict_gfp_mask(void)
 
 bool pm_suspended_storage(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((gfp_allowed_mask & (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))
 		return false;
 	return true;
@@ -361,6 +366,7 @@ static inline void reset_deferred_meminit(pg_data_t *pgdat)
 
 static inline bool early_page_uninitialised(unsigned long pfn)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -368,6 +374,7 @@ static inline bool update_defer_init(pg_data_t *pgdat,
 				unsigned long pfn, unsigned long zone_end,
 				unsigned long *nr_initialised)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 #endif
@@ -451,6 +458,7 @@ void set_pfnblock_flags_mask(struct page *page, unsigned long flags,
 	unsigned long bitidx, word_bitidx;
 	unsigned long old_word, word;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG_ON(NR_PAGEBLOCK_BITS != 4);
 
 	bitmap = get_pageblock_bitmap(page, pfn);
@@ -469,6 +477,7 @@ void set_pfnblock_flags_mask(struct page *page, unsigned long flags,
 		old_word = cmpxchg(&bitmap[word_bitidx], word, (word & ~mask) | flags);
 		if (word == old_word)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		word = old_word;
 	}
 }
@@ -531,6 +540,7 @@ static int __maybe_unused bad_range(struct zone *zone, struct page *page)
 #else
 static inline int __maybe_unused bad_range(struct zone *zone, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 #endif
@@ -547,6 +557,7 @@ static void bad_page(struct page *page, const char *reason,
 	 * or allow a steady drip of one report per second.
 	 */
 	if (nr_shown == 60) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (time_before(jiffies, resume)) {
 			nr_unshown++;
 			goto out;
@@ -613,6 +624,7 @@ void prep_compound_page(struct page *page, unsigned int order)
 		p->mapping = TAIL_MAPPING;
 		set_compound_head(p, page);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	atomic_set(compound_mapcount_ptr(page), -1);
 }
 
@@ -732,6 +744,7 @@ static inline void set_page_order(struct page *page, unsigned int order)
 
 static inline void rmv_page_order(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__ClearPageBuddy(page);
 	set_page_private(page, 0);
 }
@@ -754,10 +767,15 @@ static inline void rmv_page_order(struct page *page)
 static inline int page_is_buddy(struct page *page, struct page *buddy,
 							unsigned int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (page_is_guard(buddy) && page_order(buddy) == order) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (page_zone_id(page) != page_zone_id(buddy))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
 
 		return 1;
@@ -770,12 +788,16 @@ static inline int page_is_buddy(struct page *page, struct page *buddy,
 		 * never merge.
 		 */
 		if (page_zone_id(page) != page_zone_id(buddy))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);
 
 		return 1;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -814,6 +836,7 @@ static inline void __free_one_page(struct page *page,
 	struct page *buddy;
 	unsigned int max_order;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	max_order = min_t(unsigned int, MAX_ORDER, pageblock_order + 1);
 
 	VM_BUG_ON(!zone_is_initialized(zone));
@@ -823,11 +846,13 @@ static inline void __free_one_page(struct page *page,
 	if (likely(!is_migrate_isolate(migratetype)))
 		__mod_zone_freepage_state(zone, 1 << order, migratetype);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(pfn & ((1 << order) - 1), page);
 	VM_BUG_ON_PAGE(bad_range(zone, page), page);
 
 continue_merging:
 	while (order < max_order - 1) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		buddy_pfn = __find_buddy_pfn(pfn, order);
 		buddy = page + (buddy_pfn - pfn);
 
@@ -840,6 +865,7 @@ static inline void __free_one_page(struct page *page,
 		 * merge with it and move up one order.
 		 */
 		if (page_is_guard(buddy)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			clear_page_guard(zone, buddy, order, migratetype);
 		} else {
 			list_del(&buddy->lru);
@@ -915,7 +941,9 @@ static inline bool page_expected_state(struct page *page,
 					unsigned long check_flags)
 {
 	if (unlikely(atomic_read(&page->_mapcount) != -1))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (unlikely((unsigned long)page->mapping |
 			page_ref_count(page) |
@@ -937,7 +965,9 @@ static void free_pages_check_bad(struct page *page)
 	bad_flags = 0;
 
 	if (unlikely(atomic_read(&page->_mapcount) != -1))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bad_reason = "nonzero mapcount";
+}
 	if (unlikely(page->mapping != NULL))
 		bad_reason = "non-NULL mapping";
 	if (unlikely(page_ref_count(page) != 0))
@@ -956,7 +986,9 @@ static void free_pages_check_bad(struct page *page)
 static inline int free_pages_check(struct page *page)
 {
 	if (likely(page_expected_state(page, PAGE_FLAGS_CHECK_AT_FREE)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Something has gone sideways, find it */
 	free_pages_check_bad(page);
@@ -973,14 +1005,18 @@ static int free_tail_pages_check(struct page *head_page, struct page *page)
 	 */
 	BUILD_BUG_ON((unsigned long)LIST_POISON1 & 1);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!IS_ENABLED(CONFIG_DEBUG_VM)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = 0;
 		goto out;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	switch (page - head_page) {
 	case 1:
 		/* the first tail page: ->mapping is compound_mapcount() */
 		if (unlikely(compound_mapcount(page))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bad_page(page, "nonzero compound_mapcount", 0);
 			goto out;
 		}
@@ -993,19 +1029,25 @@ static int free_tail_pages_check(struct page *head_page, struct page *page)
 		break;
 	default:
 		if (page->mapping != TAIL_MAPPING) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bad_page(page, "corrupted mapping in tail page", 0);
 			goto out;
 		}
 		break;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(!PageTail(page))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bad_page(page, "PageTail not set", 0);
 		goto out;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(compound_head(page) != head_page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bad_page(page, "compound_head not consistent", 0);
 		goto out;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = 0;
 out:
 	page->mapping = NULL;
@@ -1027,6 +1069,7 @@ static __always_inline bool free_pages_prepare(struct page *page,
 	 * avoid checking PageCompound for order-0 pages.
 	 */
 	if (unlikely(order)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bool compound = PageCompound(page);
 		int i;
 
@@ -1038,6 +1081,7 @@ static __always_inline bool free_pages_prepare(struct page *page,
 			if (compound)
 				bad += free_tail_pages_check(page, page + i);
 			if (unlikely(free_pages_check(page + i))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				bad++;
 				continue;
 			}
@@ -1048,21 +1092,27 @@ static __always_inline bool free_pages_prepare(struct page *page,
 		page->mapping = NULL;
 	if (memcg_kmem_enabled() && PageKmemcg(page))
 		memcg_kmem_uncharge(page, order);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (check_free)
 		bad += free_pages_check(page);
 	if (bad)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	page_cpupid_reset_last(page);
 	page->flags &= ~PAGE_FLAGS_CHECK_AT_PREP;
 	reset_page_owner(page, order);
 
 	if (!PageHighMem(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		debug_check_no_locks_freed(page_address(page),
 					   PAGE_SIZE << order);
 		debug_check_no_obj_freed(page_address(page),
 					   PAGE_SIZE << order);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	arch_free_page(page, order);
 	kernel_poison_pages(page, 1 << order, 0);
 	kernel_map_pages(page, 1 << order, 0);
@@ -1128,13 +1178,17 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 		do {
 			batch_free++;
 			if (++migratetype == MIGRATE_PCPTYPES)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				migratetype = 0;
+}
 			list = &pcp->lists[migratetype];
 		} while (list_empty(list));
 
 		/* This is the only non-empty list. Free them all. */
 		if (batch_free == MIGRATE_PCPTYPES)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			batch_free = count;
+}
 
 		do {
 			int mt;	/* migratetype of the to-be-freed page */
@@ -1148,7 +1202,9 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 			VM_BUG_ON_PAGE(is_migrate_isolate(mt), page);
 			/* Pageblock could have been isolated meanwhile */
 			if (unlikely(isolated_pageblocks))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				mt = get_pageblock_migratetype(page);
+}
 
 			if (bulkfree_pcp_prepare(page))
 				continue;
@@ -1157,6 +1213,7 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 			trace_mm_page_pcpu_drain(page, 0, mt);
 		} while (--count && --batch_free && !list_empty(list));
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&zone->lock);
 }
 
@@ -1165,9 +1222,11 @@ static void free_one_page(struct zone *zone,
 				unsigned int order,
 				int migratetype)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&zone->lock);
 	if (unlikely(has_isolate_pageblock(zone) ||
 		is_migrate_isolate(migratetype))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		migratetype = get_pfnblock_migratetype(page, pfn);
 	}
 	__free_one_page(page, pfn, zone, order, migratetype);
@@ -1254,8 +1313,11 @@ static void __free_pages_ok(struct page *page, unsigned int order)
 	unsigned long pfn = page_to_pfn(page);
 
 	if (!free_pages_prepare(page, order, true))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	migratetype = get_pfnblock_migratetype(page, pfn);
 	local_irq_save(flags);
 	__count_vm_events(PGFREE, 1 << order);
@@ -1275,6 +1337,7 @@ static void __init __free_pages_boot_core(struct page *page, unsigned int order)
 		__ClearPageReserved(p);
 		set_page_count(p, 0);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__ClearPageReserved(p);
 	set_page_count(p, 0);
 
@@ -1296,7 +1359,10 @@ int __meminit early_pfn_to_nid(unsigned long pfn)
 	spin_lock(&early_pfn_lock);
 	nid = __early_pfn_to_nid(pfn, &early_pfnnid_cache);
 	if (nid < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nid = first_online_node;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&early_pfn_lock);
 
 	return nid;
@@ -1311,6 +1377,7 @@ meminit_pfn_in_nid(unsigned long pfn, int node,
 	int nid;
 
 	nid = __early_pfn_to_nid(pfn, state);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (nid >= 0 && nid != node)
 		return false;
 	return true;
@@ -1319,6 +1386,7 @@ meminit_pfn_in_nid(unsigned long pfn, int node,
 /* Only safe to use early in boot when initialisation is single-threaded */
 static inline bool __meminit early_pfn_in_nid(unsigned long pfn, int node)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return meminit_pfn_in_nid(pfn, node, &early_pfnnid_cache);
 }
 
@@ -1340,8 +1408,11 @@ meminit_pfn_in_nid(unsigned long pfn, int node,
 void __init __free_pages_bootmem(struct page *page, unsigned long pfn,
 							unsigned int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (early_page_uninitialised(pfn))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	return __free_pages_boot_core(page, order);
 }
 
@@ -1372,20 +1443,28 @@ struct page *__pageblock_pfn_to_page(unsigned long start_pfn,
 	end_pfn--;
 
 	if (!pfn_valid(start_pfn) || !pfn_valid(end_pfn))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	start_page = pfn_to_online_page(start_pfn);
 	if (!start_page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (page_zone(start_page) != zone)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	end_page = pfn_to_page(end_pfn);
 
 	/* This gives a shorter code than deriving page_zone(end_page) */
 	if (page_zone_id(start_page) != page_zone_id(end_page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	return start_page;
 }
@@ -1413,6 +1492,7 @@ void set_zone_contiguous(struct zone *zone)
 
 void clear_zone_contiguous(struct zone *zone)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	zone->contiguous = false;
 }
 
@@ -1680,7 +1760,9 @@ static void check_new_page_bad(struct page *page)
 	unsigned long bad_flags = 0;
 
 	if (unlikely(atomic_read(&page->_mapcount) != -1))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bad_reason = "nonzero mapcount";
+}
 	if (unlikely(page->mapping != NULL))
 		bad_reason = "non-NULL mapping";
 	if (unlikely(page_ref_count(page) != 0))
@@ -1712,12 +1794,14 @@ static inline int check_new_page(struct page *page)
 				PAGE_FLAGS_CHECK_AT_PREP|__PG_HWPOISON)))
 		return 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	check_new_page_bad(page);
 	return 1;
 }
 
 static inline bool free_pages_prezeroed(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return IS_ENABLED(CONFIG_PAGE_POISONING_ZERO) &&
 		page_poisoning_enabled();
 }
@@ -1739,6 +1823,7 @@ static bool check_pcp_refill(struct page *page)
 }
 static bool check_new_pcp(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 #endif /* CONFIG_DEBUG_VM */
@@ -1750,9 +1835,12 @@ static bool check_new_pages(struct page *page, unsigned int order)
 		struct page *p = page + i;
 
 		if (unlikely(check_new_page(p)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return true;
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -1790,7 +1878,9 @@ static void prep_new_page(struct page *page, unsigned int order, gfp_t gfp_flags
 	 * being used for !PFMEMALLOC purposes.
 	 */
 	if (alloc_flags & ALLOC_NO_WATERMARKS)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set_page_pfmemalloc(page);
+}
 	else
 		clear_page_pfmemalloc(page);
 }
@@ -1822,6 +1912,7 @@ struct page *__rmqueue_smallest(struct zone *zone, unsigned int order,
 		return page;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1878,7 +1969,9 @@ static int move_freepages(struct zone *zone,
 #endif
 
 	if (num_movable)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*num_movable = 0;
+}
 
 	for (page = start_page; page <= end_page;) {
 		if (!pfn_valid_within(page_to_pfn(page))) {
@@ -1927,7 +2020,9 @@ int move_freepages_block(struct zone *zone, struct page *page,
 
 	/* Do not cross zone boundaries */
 	if (!zone_spans_pfn(zone, start_pfn))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start_page = page;
+}
 	if (!zone_spans_pfn(zone, end_pfn))
 		return 0;
 
@@ -1968,14 +2063,18 @@ static bool can_steal_fallback(unsigned int order, int start_mt)
 	 * so could be changed anytime.
 	 */
 	if (order >= pageblock_order)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (order >= pageblock_order / 2 ||
 		start_mt == MIGRATE_RECLAIMABLE ||
 		start_mt == MIGRATE_UNMOVABLE ||
 		page_group_by_mobility_disabled)
 		return true;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -1990,6 +2089,7 @@ static bool can_steal_fallback(unsigned int order, int start_mt)
 static void steal_suitable_fallback(struct zone *zone, struct page *page,
 					int start_type, bool whole_block)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned int current_order = page_order(page);
 	struct free_area *area;
 	int free_pages, movable_pages, alike_pages;
@@ -2014,6 +2114,7 @@ static void steal_suitable_fallback(struct zone *zone, struct page *page,
 	if (!whole_block)
 		goto single_page;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	free_pages = move_freepages_block(zone, page, start_type,
 						&movable_pages);
 	/*
@@ -2022,6 +2123,7 @@ static void steal_suitable_fallback(struct zone *zone, struct page *page,
 	 * we just obtained. For other types it's a bit more tricky.
 	 */
 	if (start_type == MIGRATE_MOVABLE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		alike_pages = movable_pages;
 	} else {
 		/*
@@ -2032,8 +2134,10 @@ static void steal_suitable_fallback(struct zone *zone, struct page *page,
 		 * exact migratetype of non-movable pages.
 		 */
 		if (old_block_type == MIGRATE_MOVABLE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			alike_pages = pageblock_nr_pages
 						- (free_pages + movable_pages);
+}
 		else
 			alike_pages = 0;
 	}
@@ -2050,6 +2154,7 @@ static void steal_suitable_fallback(struct zone *zone, struct page *page,
 			page_group_by_mobility_disabled)
 		set_pageblock_migratetype(page, start_type);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return;
 
 single_page:
@@ -2070,7 +2175,9 @@ int find_suitable_fallback(struct free_area *area, unsigned int order,
 	int fallback_mt;
 
 	if (area->nr_free == 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	*can_steal = false;
 	for (i = 0;; i++) {
@@ -2085,12 +2192,18 @@ int find_suitable_fallback(struct free_area *area, unsigned int order,
 			*can_steal = true;
 
 		if (!only_stealable)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return fallback_mt;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (*can_steal)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return fallback_mt;
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return -1;
 }
 
@@ -2110,7 +2223,9 @@ static void reserve_highatomic_pageblock(struct page *page, struct zone *zone,
 	 */
 	max_managed = (zone->managed_pages / 100) + pageblock_nr_pages;
 	if (zone->nr_reserved_highatomic >= max_managed)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	spin_lock_irqsave(&zone->lock, flags);
 
@@ -2151,6 +2266,7 @@ static bool unreserve_highatomic_pageblock(const struct alloc_context *ac,
 	int order;
 	bool ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_zone_zonelist_nodemask(zone, z, zonelist, ac->high_zoneidx,
 								ac->nodemask) {
 		/*
@@ -2261,11 +2377,13 @@ __rmqueue_fallback(struct zone *zone, int order, int start_migratetype)
 		goto do_steal;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 
 find_smallest:
 	for (current_order = order; current_order < MAX_ORDER;
 							current_order++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		area = &(zone->free_area[current_order]);
 		fallback_mt = find_suitable_fallback(area, current_order,
 				start_migratetype, false, &can_steal);
@@ -2305,7 +2423,9 @@ static struct page *__rmqueue(struct zone *zone, unsigned int order,
 	page = __rmqueue_smallest(zone, order, migratetype);
 	if (unlikely(!page)) {
 		if (migratetype == MIGRATE_MOVABLE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = __rmqueue_cma_fallback(zone, order);
+}
 
 		if (!page && __rmqueue_fallback(zone, order, migratetype))
 			goto retry;
@@ -2351,8 +2471,10 @@ static int rmqueue_bulk(struct zone *zone, unsigned int order,
 		list = &page->lru;
 		alloced++;
 		if (is_migrate_cma(get_pcppage_migratetype(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__mod_zone_page_state(zone, NR_FREE_CMA_PAGES,
 					      -(1 << order));
+}
 	}
 
 	/*
@@ -2380,6 +2502,7 @@ void drain_zone_pages(struct zone *zone, struct per_cpu_pages *pcp)
 	unsigned long flags;
 	int to_drain, batch;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	local_irq_save(flags);
 	batch = READ_ONCE(pcp->batch);
 	to_drain = min(pcp->count, batch);
@@ -2404,6 +2527,7 @@ static void drain_pages_zone(unsigned int cpu, struct zone *zone)
 	struct per_cpu_pageset *pset;
 	struct per_cpu_pages *pcp;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	local_irq_save(flags);
 	pset = per_cpu_ptr(zone->pageset, cpu);
 
@@ -2426,6 +2550,7 @@ static void drain_pages(unsigned int cpu)
 {
 	struct zone *zone;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_populated_zone(zone) {
 		drain_pages_zone(cpu, zone);
 	}
@@ -2439,6 +2564,7 @@ static void drain_pages(unsigned int cpu)
  */
 void drain_local_pages(struct zone *zone)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int cpu = smp_processor_id();
 
 	if (zone)
@@ -2553,7 +2679,9 @@ void mark_free_pages(struct zone *zone)
 	struct page *page;
 
 	if (zone_is_empty(zone))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	spin_lock_irqsave(&zone->lock, flags);
 
@@ -2599,6 +2727,7 @@ void mark_free_pages(struct zone *zone)
  */
 void free_hot_cold_page(struct page *page, bool cold)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct zone *zone = page_zone(page);
 	struct per_cpu_pages *pcp;
 	unsigned long flags;
@@ -2606,11 +2735,15 @@ void free_hot_cold_page(struct page *page, bool cold)
 	int migratetype;
 
 	if (!free_pcp_prepare(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	migratetype = get_pfnblock_migratetype(page, pfn);
 	set_pcppage_migratetype(page, migratetype);
 	local_irq_save(flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__count_vm_event(PGFREE);
 
 	/*
@@ -2621,10 +2754,13 @@ void free_hot_cold_page(struct page *page, bool cold)
 	 * excessively into the page allocator
 	 */
 	if (migratetype >= MIGRATE_PCPTYPES) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(is_migrate_isolate(migratetype))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			free_one_page(zone, page, pfn, 0, migratetype);
 			goto out;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		migratetype = MIGRATE_MOVABLE;
 	}
 
@@ -2674,6 +2810,7 @@ void split_page(struct page *page, unsigned int order)
 
 	for (i = 1; i < (1 << order); i++)
 		set_page_refcounted(page + i);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	split_page_owner(page, order);
 }
 EXPORT_SYMBOL_GPL(split_page);
@@ -2684,6 +2821,7 @@ int __isolate_free_page(struct page *page, unsigned int order)
 	struct zone *zone;
 	int mt;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!PageBuddy(page));
 
 	zone = page_zone(page);
@@ -2738,7 +2876,9 @@ static inline void zone_statistics(struct zone *preferred_zone, struct zone *z)
 	enum numa_stat_item local_stat = NUMA_LOCAL;
 
 	if (z->node != numa_node_id())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		local_stat = NUMA_OTHER;
+}
 
 	if (z->node == preferred_zone->node)
 		__inc_numa_state(z, NUMA_HIT);
@@ -2763,7 +2903,9 @@ static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
 					pcp->batch, list,
 					migratetype, cold);
 			if (unlikely(list_empty(list)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return NULL;
+}
 		}
 
 		if (cold)
@@ -2773,6 +2915,7 @@ static struct page *__rmqueue_pcplist(struct zone *zone, int migratetype,
 
 		list_del(&page->lru);
 		pcp->count--;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (check_new_pcp(page));
 
 	return page;
@@ -2827,15 +2970,20 @@ struct page *rmqueue(struct zone *preferred_zone,
 	spin_lock_irqsave(&zone->lock, flags);
 
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = NULL;
 		if (alloc_flags & ALLOC_HARDER) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = __rmqueue_smallest(zone, order, MIGRATE_HIGHATOMIC);
 			if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				trace_mm_page_alloc_zone_locked(page, order, migratetype);
+}
 		}
 		if (!page)
 			page = __rmqueue(zone, order, migratetype);
 	} while (page && check_new_pages(page, order));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&zone->lock);
 	if (!page)
 		goto failed;
@@ -2852,6 +3000,7 @@ struct page *rmqueue(struct zone *preferred_zone,
 
 failed:
 	local_irq_restore(flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -2928,6 +3077,7 @@ late_initcall(fail_page_alloc_debugfs);
 
 static inline bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -2951,7 +3101,9 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 	free_pages -= (1 << order) - 1;
 
 	if (alloc_flags & ALLOC_HIGH)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		min -= min / 2;
+}
 
 	/*
 	 * If the caller does not have rights to ALLOC_HARDER then subtract
@@ -2968,7 +3120,9 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 		 * makes during the free path will be small and short-lived.
 		 */
 		if (alloc_flags & ALLOC_OOM)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			min -= min / 2;
+}
 		else
 			min -= min / 4;
 	}
@@ -2986,11 +3140,15 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 	 * even if a suitable page happened to be free.
 	 */
 	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	/* If this is an order-0 request then the watermark is fine */
 	if (!order)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
 	/* For a high-order request, check at least one suitable page is free */
 	for (o = order; o < MAX_ORDER; o++) {
@@ -3002,7 +3160,9 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 
 		for (mt = 0; mt < MIGRATE_PCPTYPES; mt++) {
 			if (!list_empty(&area->free_list[mt]))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return true;
+}
 		}
 
 #ifdef CONFIG_CMA
@@ -3015,6 +3175,7 @@ bool __zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 			!list_empty(&area->free_list[MIGRATE_HIGHATOMIC]))
 			return true;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -3028,6 +3189,7 @@ bool zone_watermark_ok(struct zone *z, unsigned int order, unsigned long mark,
 static inline bool zone_watermark_fast(struct zone *z, unsigned int order,
 		unsigned long mark, int classzone_idx, unsigned int alloc_flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	long free_pages = zone_page_state(z, NR_FREE_PAGES);
 	long cma_pages = 0;
 
@@ -3045,7 +3207,9 @@ static inline bool zone_watermark_fast(struct zone *z, unsigned int order,
 	 * list. That corner case is then slower but it is harmless.
 	 */
 	if (!order && (free_pages - cma_pages) > mark + z->lowmem_reserve[classzone_idx])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
 	return __zone_watermark_ok(z, order, mark, classzone_idx, alloc_flags,
 					free_pages);
@@ -3054,10 +3218,13 @@ static inline bool zone_watermark_fast(struct zone *z, unsigned int order,
 bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
 			unsigned long mark, int classzone_idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	long free_pages = zone_page_state(z, NR_FREE_PAGES);
 
 	if (z->percpu_drift_mark && free_pages < z->percpu_drift_mark)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		free_pages = zone_page_state_snapshot(z, NR_FREE_PAGES);
+}
 
 	return __zone_watermark_ok(z, order, mark, classzone_idx, 0,
 								free_pages);
@@ -3066,6 +3233,7 @@ bool zone_watermark_ok_safe(struct zone *z, unsigned int order,
 #ifdef CONFIG_NUMA
 static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return node_distance(zone_to_nid(local_zone), zone_to_nid(zone)) <=
 				RECLAIM_DISTANCE;
 }
@@ -3121,10 +3289,13 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 		 * dirty-throttling and the flusher threads.
 		 */
 		if (ac->spread_dirty_pages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (last_pgdat_dirty_limit == zone->zone_pgdat)
 				continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!node_dirty_ok(zone->zone_pgdat)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				last_pgdat_dirty_limit = zone->zone_pgdat;
 				continue;
 			}
@@ -3140,10 +3311,12 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 			if (alloc_flags & ALLOC_NO_WATERMARKS)
 				goto try_this_zone;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (node_reclaim_mode == 0 ||
 			    !zone_allows_reclaim(ac->preferred_zoneref->zone, zone))
 				continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = node_reclaim(zone->zone_pgdat, gfp_mask, order);
 			switch (ret) {
 			case NODE_RECLAIM_NOSCAN:
@@ -3173,12 +3346,16 @@ get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 			 * if the pageblock should be reserved for the future
 			 */
 			if (unlikely(order && (alloc_flags & ALLOC_HARDER)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				reserve_highatomic_pageblock(page, zone, order);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -3201,6 +3378,7 @@ static void warn_alloc_show_mem(gfp_t gfp_mask, nodemask_t *nodemask)
 	unsigned int filter = SHOW_MEM_FILTER_NODES;
 	static DEFINE_RATELIMIT_STATE(show_mem_rs, HZ, 1);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (should_suppress_show_mem() || !__ratelimit(&show_mem_rs))
 		return;
 
@@ -3226,6 +3404,7 @@ void warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...)
 	static DEFINE_RATELIMIT_STATE(nopage_rs, DEFAULT_RATELIMIT_INTERVAL,
 				      DEFAULT_RATELIMIT_BURST);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((gfp_mask & __GFP_NOWARN) || !__ratelimit(&nopage_rs))
 		return;
 
@@ -3263,8 +3442,10 @@ __alloc_pages_cpuset_fallback(gfp_t gfp_mask, unsigned int order,
 	 * are depleted
 	 */
 	if (!page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = get_page_from_freelist(gfp_mask, order,
 				alloc_flags, ac);
+}
 
 	return page;
 }
@@ -3289,6 +3470,7 @@ __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,
 	 * making progress for us.
 	 */
 	if (!mutex_trylock(&oom_lock)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*did_some_progress = 1;
 		schedule_timeout_uninterruptible(1);
 		return NULL;
@@ -3374,7 +3556,9 @@ __alloc_pages_direct_compact(gfp_t gfp_mask, unsigned int order,
 	unsigned int noreclaim_flag;
 
 	if (!order)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	noreclaim_flag = memalloc_noreclaim_save();
 	*compact_result = try_to_compact_pages(gfp_mask, order, alloc_flags, ac,
@@ -3425,7 +3609,9 @@ should_compact_retry(struct alloc_context *ac, int order, int alloc_flags,
 	enum compact_priority priority = *compact_priority;
 
 	if (!order)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (compaction_made_progress(compact_result))
 		(*compaction_retries)++;
@@ -3601,7 +3787,9 @@ __alloc_pages_direct_reclaim(gfp_t gfp_mask, unsigned int order,
 
 	*did_some_progress = __perform_reclaim(gfp_mask, order, ac);
 	if (unlikely(!(*did_some_progress)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 retry:
 	page = get_page_from_freelist(gfp_mask, order, alloc_flags, ac);
@@ -3627,6 +3815,7 @@ static void wake_all_kswapds(unsigned int order, const struct alloc_context *ac)
 	struct zone *zone;
 	pg_data_t *last_pgdat = NULL;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_zone_zonelist_nodemask(zone, z, ac->zonelist,
 					ac->high_zoneidx, ac->nodemask) {
 		if (last_pgdat != zone->zone_pgdat)
@@ -3675,6 +3864,7 @@ gfp_to_alloc_flags(gfp_t gfp_mask)
 
 static bool oom_reserves_allowed(struct task_struct *tsk)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!tsk_is_oom_victim(tsk))
 		return false;
 
@@ -3694,6 +3884,7 @@ static bool oom_reserves_allowed(struct task_struct *tsk)
  */
 static inline int __gfp_pfmemalloc_flags(gfp_t gfp_mask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(gfp_mask & __GFP_NOMEMALLOC))
 		return 0;
 	if (gfp_mask & __GFP_MEMALLOC)
@@ -3712,6 +3903,7 @@ static inline int __gfp_pfmemalloc_flags(gfp_t gfp_mask)
 
 bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return !!__gfp_pfmemalloc_flags(gfp_mask);
 }
 
@@ -3874,6 +4066,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 	 * too large.
 	 */
 	if (order >= MAX_ORDER) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON_ONCE(!(gfp_mask & __GFP_NOWARN));
 		return NULL;
 	}
@@ -4138,17 +4331,25 @@ static inline bool prepare_alloc_pages(gfp_t gfp_mask, unsigned int order,
 			*alloc_flags |= ALLOC_CPUSET;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	fs_reclaim_acquire(gfp_mask);
 	fs_reclaim_release(gfp_mask);
 
 	might_sleep_if(gfp_mask & __GFP_DIRECT_RECLAIM);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (should_fail_alloc_page(gfp_mask, order))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (IS_ENABLED(CONFIG_CMA) && ac->migratetype == MIGRATE_MOVABLE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*alloc_flags |= ALLOC_CMA;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 
@@ -4183,7 +4384,9 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	gfp_mask &= gfp_allowed_mask;
 	alloc_mask = gfp_mask;
 	if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, &ac, &alloc_mask, &alloc_flags))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	finalise_ac(gfp_mask, order, &ac);
 
@@ -4206,13 +4409,17 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
 	 */
 	if (unlikely(ac.nodemask != nodemask))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ac.nodemask = nodemask;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	page = __alloc_pages_slowpath(alloc_mask, order, &ac);
 
 out:
 	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
 	    unlikely(memcg_kmem_charge(page, gfp_mask, order) != 0)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__free_pages(page, order);
 		page = NULL;
 	}
@@ -4238,7 +4445,9 @@ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)
 
 	page = alloc_pages(gfp_mask, order);
 	if (!page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	return (unsigned long) page_address(page);
 }
 EXPORT_SYMBOL(__get_free_pages);
@@ -4264,6 +4473,7 @@ EXPORT_SYMBOL(__free_pages);
 void free_pages(unsigned long addr, unsigned int order)
 {
 	if (addr != 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON(!virt_addr_valid((void *)addr));
 		__free_pages(virt_to_page((void *)addr), order);
 	}
@@ -4296,7 +4506,9 @@ static struct page *__page_frag_cache_refill(struct page_frag_cache *nc,
 	nc->size = page ? PAGE_FRAG_CACHE_MAX_SIZE : PAGE_SIZE;
 #endif
 	if (unlikely(!page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = alloc_pages_node(NUMA_NO_NODE, gfp, 0);
+}
 
 	nc->va = page ? page_address(page) : NULL;
 
@@ -4305,6 +4517,7 @@ static struct page *__page_frag_cache_refill(struct page_frag_cache *nc,
 
 void __page_frag_cache_drain(struct page *page, unsigned int count)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(page_ref_count(page) == 0, page);
 
 	if (page_ref_sub_and_test(page, count)) {
@@ -4329,7 +4542,9 @@ void *page_frag_alloc(struct page_frag_cache *nc,
 refill:
 		page = __page_frag_cache_refill(nc, gfp_mask);
 		if (!page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 
 #if (PAGE_SIZE < PAGE_FRAG_CACHE_MAX_SIZE)
 		/* if size can vary use size else just use PAGE_SIZE */
@@ -4348,6 +4563,7 @@ void *page_frag_alloc(struct page_frag_cache *nc,
 
 	offset = nc->offset - fragsz;
 	if (unlikely(offset < 0)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = virt_to_page(nc->va);
 
 		if (!page_ref_sub_and_test(page, nc->pagecnt_bias))
@@ -4380,8 +4596,10 @@ void page_frag_free(void *addr)
 	struct page *page = virt_to_head_page(addr);
 
 	if (unlikely(put_page_testzero(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__free_pages_ok(page, compound_order(page));
 }
+}
 EXPORT_SYMBOL(page_frag_free);
 
 static void *make_alloc_exact(unsigned long addr, unsigned int order,
@@ -4435,6 +4653,7 @@ EXPORT_SYMBOL(alloc_pages_exact);
  */
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned int order = get_order(size);
 	struct page *p = alloc_pages_node(nid, gfp_mask, order);
 	if (!p)
@@ -4454,6 +4673,7 @@ void free_pages_exact(void *virt, size_t size)
 	unsigned long addr = (unsigned long)virt;
 	unsigned long end = addr + PAGE_ALIGN(size);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (addr < end) {
 		free_page(addr);
 		addr += PAGE_SIZE;
@@ -4516,6 +4736,7 @@ unsigned long nr_free_pagecache_pages(void)
 
 static inline void show_node(struct zone *zone)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (IS_ENABLED(CONFIG_NUMA))
 		printk("Node %d ", zone_to_nid(zone));
 }
@@ -4559,7 +4780,9 @@ long si_mem_available(void)
 			 wmark_low);
 
 	if (available < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		available = 0;
+}
 	return available;
 }
 EXPORT_SYMBOL_GPL(si_mem_available);
@@ -4586,6 +4809,7 @@ void si_meminfo_node(struct sysinfo *val, int nid)
 	unsigned long free_highpages = 0;
 	pg_data_t *pgdat = NODE_DATA(nid);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zone_type = 0; zone_type < MAX_NR_ZONES; zone_type++)
 		managed_pages += pgdat->node_zones[zone_type].managed_pages;
 	val->totalram = managed_pages;
@@ -4616,6 +4840,7 @@ void si_meminfo_node(struct sysinfo *val, int nid)
  */
 static bool show_mem_node_skip(unsigned int flags, int nid, nodemask_t *nodemask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!(flags & SHOW_MEM_FILTER_NODES))
 		return false;
 
@@ -4650,6 +4875,7 @@ static void show_migration_types(unsigned char type)
 	char *p = tmp;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < MIGRATE_TYPES; i++) {
 		if (type & (1 << i))
 			*p++ = types[i];
@@ -4675,6 +4901,7 @@ void show_free_areas(unsigned int filter, nodemask_t *nodemask)
 	struct zone *zone;
 	pg_data_t *pgdat;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_populated_zone(zone) {
 		if (show_mem_node_skip(filter, zone_to_nid(zone), nodemask))
 			continue;
@@ -4906,6 +5133,7 @@ static int __parse_numa_zonelist_order(char *s)
 
 static __init int setup_numa_zonelist_order(char *s)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!s)
 		return 0;
 
@@ -4926,7 +5154,9 @@ int numa_zonelist_order_handler(struct ctl_table *table, int write,
 	int ret;
 
 	if (!write)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return proc_dostring(table, write, buffer, length, ppos);
+}
 	str = memdup_user_nul(buffer, 16);
 	if (IS_ERR(str))
 		return PTR_ERR(str);
@@ -5014,6 +5244,7 @@ static void build_zonelists_in_node_order(pg_data_t *pgdat, int *node_order,
 
 	zonerefs = pgdat->node_zonelists[ZONELIST_FALLBACK]._zonerefs;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < nr_nodes; i++) {
 		int nr_zones;
 
@@ -5062,6 +5293,7 @@ static void build_zonelists(pg_data_t *pgdat)
 	nodes_clear(used_mask);
 
 	memset(node_order, 0, sizeof(node_order));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while ((node = find_next_best_node(local_node, &used_mask)) >= 0) {
 		/*
 		 * We don't want to pressure a particular node.
@@ -5179,6 +5411,7 @@ static void __build_all_zonelists(void *data)
 	 * building zonelists is fine - no need to touch other nodes.
 	 */
 	if (self && !node_online(self->node_id)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		build_zonelists(self);
 	} else {
 		for_each_online_node(nid) {
@@ -5201,6 +5434,7 @@ static void __build_all_zonelists(void *data)
 #endif
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&lock);
 }
 
@@ -5242,6 +5476,7 @@ void __ref build_all_zonelists(pg_data_t *pgdat)
 	if (system_state == SYSTEM_BOOTING) {
 		build_all_zonelists_init();
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__build_all_zonelists(pgdat);
 		/* cpuset refresh routine should be here */
 	}
@@ -5254,7 +5489,9 @@ void __ref build_all_zonelists(pg_data_t *pgdat)
 	 * disabled and enable it later
 	 */
 	if (vm_total_pages < (pageblock_nr_pages * MIGRATE_TYPES))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page_group_by_mobility_disabled = 1;
+}
 	else
 		page_group_by_mobility_disabled = 0;
 
@@ -5292,7 +5529,9 @@ void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
 	 * memory
 	 */
 	if (altmap && start_pfn == altmap->base_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start_pfn += altmap->reserve;
+}
 
 	for (pfn = start_pfn; pfn < end_pfn; pfn++) {
 		/*
@@ -5313,8 +5552,10 @@ void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
 #endif
 			continue;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!early_pfn_in_nid(pfn, nid))
 			continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!update_defer_init(pgdat, pfn, end_pfn, &nr_initialised))
 			break;
 
@@ -5325,12 +5566,16 @@ void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
 		 * mirrored, it's an overlapped memmap init. skip it.
 		 */
 		if (mirrored_kernelcore && zone == ZONE_MOVABLE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!r || pfn >= memblock_region_memory_end_pfn(r)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				for_each_memblock(memory, tmp)
 					if (pfn < memblock_region_memory_end_pfn(tmp))
 						break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				r = tmp;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (pfn >= memblock_region_memory_base_pfn(r) &&
 			    memblock_is_mirror(r)) {
 				/* already initialized as NORMAL */
@@ -5392,10 +5637,14 @@ static int zone_batchsize(struct zone *zone)
 	 */
 	batch = zone->managed_pages / 1024;
 	if (batch * PAGE_SIZE > 512 * 1024)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		batch = (512 * 1024) / PAGE_SIZE;
+}
 	batch /= 4;		/* We effectively *= 4 below */
 	if (batch < 1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		batch = 1;
+}
 
 	/*
 	 * Clamp the batch to a 2^n - 1 value. Having a power
@@ -5488,6 +5737,7 @@ static void setup_pageset(struct per_cpu_pageset *p, unsigned long batch)
 static void pageset_set_high(struct per_cpu_pageset *p,
 				unsigned long high)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long batch = max(1UL, high / 4);
 	if ((high / 4) > (PAGE_SHIFT * 8))
 		batch = PAGE_SHIFT * 8;
@@ -5499,9 +5749,11 @@ static void pageset_set_high_and_batch(struct zone *zone,
 				       struct per_cpu_pageset *pcp)
 {
 	if (percpu_pagelist_fraction)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pageset_set_high(pcp,
 			(zone->managed_pages /
 				percpu_pagelist_fraction));
+}
 	else
 		pageset_set_batch(pcp, zone_batchsize(zone));
 }
@@ -5587,7 +5839,9 @@ int __meminit __early_pfn_to_nid(unsigned long pfn,
 	int nid;
 
 	if (state->last_start <= pfn && pfn < state->last_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return state->last_nid;
+}
 
 	nid = memblock_search_pfn_nid(pfn, &start_pfn, &end_pfn);
 	if (nid != -1) {
@@ -5596,6 +5850,7 @@ int __meminit __early_pfn_to_nid(unsigned long pfn,
 		state->last_nid = nid;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return nid;
 }
 #endif /* CONFIG_HAVE_ARCH_EARLY_PFN_TO_NID */
@@ -5614,6 +5869,7 @@ void __init free_bootmem_with_active_regions(int nid, unsigned long max_low_pfn)
 	unsigned long start_pfn, end_pfn;
 	int i, this_nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, &this_nid) {
 		start_pfn = min(start_pfn, max_low_pfn);
 		end_pfn = min(end_pfn, max_low_pfn);
@@ -5667,8 +5923,10 @@ void __meminit get_pfn_range_for_nid(unsigned int nid,
 	}
 
 	if (*start_pfn == -1UL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*start_pfn = 0;
 }
+}
 
 /*
  * This finds a zone that can be used for ZONE_MOVABLE pages. The
@@ -5687,6 +5945,7 @@ static void __init find_usable_zone_for_movable(void)
 			break;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(zone_index == -1);
 	movable_zone = zone_index;
 }
@@ -5712,7 +5971,9 @@ static void __meminit adjust_zone_range_for_zone_movable(int nid,
 	if (zone_movable_pfn[nid]) {
 		/* Size ZONE_MOVABLE */
 		if (zone_type == ZONE_MOVABLE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*zone_start_pfn = zone_movable_pfn[nid];
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*zone_end_pfn = min(node_end_pfn,
 				arch_zone_highest_possible_pfn[movable_zone]);
 
@@ -5720,11 +5981,14 @@ static void __meminit adjust_zone_range_for_zone_movable(int nid,
 		} else if (!mirrored_kernelcore &&
 			*zone_start_pfn < zone_movable_pfn[nid] &&
 			*zone_end_pfn > zone_movable_pfn[nid]) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*zone_end_pfn = zone_movable_pfn[nid];
 
 		/* Check if this whole range is within ZONE_MOVABLE */
 		} else if (*zone_start_pfn >= zone_movable_pfn[nid])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*zone_start_pfn = *zone_end_pfn;
+}
 	}
 }
 
@@ -5742,7 +6006,9 @@ static unsigned long __meminit zone_spanned_pages_in_node(int nid,
 {
 	/* When hotadd a new node from cpu_up(), the node should be empty */
 	if (!node_start_pfn && !node_end_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Get the start and end of the zone */
 	*zone_start_pfn = arch_zone_lowest_possible_pfn[zone_type];
@@ -5753,7 +6019,9 @@ static unsigned long __meminit zone_spanned_pages_in_node(int nid,
 
 	/* Check that this node has pages within the zone's required range */
 	if (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Move the zone boundaries inside the node if necessary */
 	*zone_end_pfn = min(*zone_end_pfn, node_end_pfn);
@@ -5793,6 +6061,7 @@ unsigned long __meminit __absent_pages_in_range(int nid,
 unsigned long __init absent_pages_in_range(unsigned long start_pfn,
 							unsigned long end_pfn)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __absent_pages_in_range(MAX_NUMNODES, start_pfn, end_pfn);
 }
 
@@ -5810,7 +6079,9 @@ static unsigned long __meminit zone_absent_pages_in_node(int nid,
 
 	/* When hotadd a new node from cpu_up(), the node should be empty */
 	if (!node_start_pfn && !node_end_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	zone_start_pfn = clamp(node_start_pfn, zone_low, zone_high);
 	zone_end_pfn = clamp(node_end_pfn, zone_low, zone_high);
@@ -5829,9 +6100,12 @@ static unsigned long __meminit zone_absent_pages_in_node(int nid,
 		unsigned long start_pfn, end_pfn;
 		struct memblock_region *r;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for_each_memblock(memory, r) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			start_pfn = clamp(memblock_region_memory_base_pfn(r),
 					  zone_start_pfn, zone_end_pfn);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			end_pfn = clamp(memblock_region_memory_end_pfn(r),
 					zone_start_pfn, zone_end_pfn);
 
@@ -5839,12 +6113,14 @@ static unsigned long __meminit zone_absent_pages_in_node(int nid,
 			    memblock_is_mirror(r))
 				nr_absent += end_pfn - start_pfn;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (zone_type == ZONE_NORMAL &&
 			    !memblock_is_mirror(r))
 				nr_absent += end_pfn - start_pfn;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return nr_absent;
 }
 
@@ -6074,8 +6350,10 @@ static void __paginginit free_area_init_core(struct pglist_data *pgdat)
 					       "  %s zone: %lu pages used for memmap\n",
 					       zone_names[j], memmap_pages);
 			} else
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				pr_warn("  %s zone: %lu pages exceeds freesize %lu\n",
 					zone_names[j], memmap_pages, freesize);
+}
 		}
 
 		/* Account for reserved pages */
@@ -6085,11 +6363,14 @@ static void __paginginit free_area_init_core(struct pglist_data *pgdat)
 					zone_names[0], dma_reserve);
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!is_highmem_idx(j))
 			nr_kernel_pages += freesize;
 		/* Charge for highmem memmap if there are enough kernel pages */
 		else if (nr_kernel_pages > memmap_pages * 2)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nr_kernel_pages -= memmap_pages;
+}
 		nr_all_pages += freesize;
 
 		/*
@@ -6104,12 +6385,14 @@ static void __paginginit free_area_init_core(struct pglist_data *pgdat)
 		zone->name = zone_names[j];
 		zone->zone_pgdat = pgdat;
 		spin_lock_init(&zone->lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		zone_seqlock_init(zone);
 		zone_pcp_init(zone);
 
 		if (!size)
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set_pageblock_order();
 		setup_usemap(pgdat, zone, zone_start_pfn, size);
 		init_currently_empty_zone(zone, zone_start_pfn, size);
@@ -6124,7 +6407,9 @@ static void __ref alloc_node_mem_map(struct pglist_data *pgdat)
 
 	/* Skip empty nodes */
 	if (!pgdat->node_spanned_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 #ifdef CONFIG_FLAT_NODE_MEM_MAP
 	start = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);
@@ -6239,6 +6524,7 @@ unsigned long __init node_map_pfn_alignment(void)
 	int last_nid = -1;
 	int i, nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_mem_pfn_range(i, MAX_NUMNODES, &start, &end, &nid) {
 		if (!start || last_nid < 0 || last_nid == nid) {
 			last_nid = nid;
@@ -6274,10 +6560,12 @@ static unsigned long __init find_min_pfn_for_node(int nid)
 		min_pfn = min(min_pfn, start_pfn);
 
 	if (min_pfn == ULONG_MAX) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("Could not find start_pfn for node %d\n", nid);
 		return 0;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return min_pfn;
 }
 
@@ -6308,7 +6596,9 @@ static unsigned long __init early_calculate_totalpages(void)
 
 		totalpages += pages;
 		if (pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			node_set_state(nid, N_MEMORY);
+}
 	}
 	return totalpages;
 }
@@ -6338,13 +6628,17 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 	 * options.
 	 */
 	if (movable_node_is_enabled()) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for_each_memblock(memory, r) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!memblock_is_hotpluggable(r))
 				continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nid = r->nid;
 
 			usable_startpfn = PFN_DOWN(r->base);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zone_movable_pfn[nid] = zone_movable_pfn[nid] ?
 				min(usable_startpfn, zone_movable_pfn[nid]) :
 				usable_startpfn;
@@ -6359,26 +6653,34 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 	if (mirrored_kernelcore) {
 		bool mem_below_4gb_not_mirrored = false;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for_each_memblock(memory, r) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (memblock_is_mirror(r))
 				continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nid = r->nid;
 
 			usable_startpfn = memblock_region_memory_base_pfn(r);
 
 			if (usable_startpfn < 0x100000) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				mem_below_4gb_not_mirrored = true;
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zone_movable_pfn[nid] = zone_movable_pfn[nid] ?
 				min(usable_startpfn, zone_movable_pfn[nid]) :
 				usable_startpfn;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (mem_below_4gb_not_mirrored)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_warn("This configuration results in unmirrored kernel memory.");
+}
 
 		goto out2;
 	}
@@ -6400,9 +6702,11 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 		 */
 		required_movablecore =
 			roundup(required_movablecore, MAX_ORDER_NR_PAGES);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		required_movablecore = min(totalpages, required_movablecore);
 		corepages = totalpages - required_movablecore;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		required_kernelcore = max(required_kernelcore, corepages);
 	}
 
@@ -6419,6 +6723,7 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 restart:
 	/* Spread kernelcore memory as evenly as possible throughout nodes */
 	kernelcore_node = required_kernelcore / usable_nodes;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node_state(nid, N_MEMORY) {
 		unsigned long start_pfn, end_pfn;
 
@@ -6428,7 +6733,9 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 		 * amount of memory for the kernel
 		 */
 		if (required_kernelcore < kernelcore_node)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			kernelcore_node = required_kernelcore / usable_nodes;
+}
 
 		/*
 		 * As the map is walked, we track how much memory is usable
@@ -6441,6 +6748,7 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 		for_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {
 			unsigned long size_pages;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			start_pfn = max(start_pfn, zone_movable_pfn[nid]);
 			if (start_pfn >= end_pfn)
 				continue;
@@ -6448,11 +6756,14 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 			/* Account for what is only usable for kernelcore */
 			if (start_pfn < usable_startpfn) {
 				unsigned long kernel_pages;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				kernel_pages = min(end_pfn, usable_startpfn)
 								- start_pfn;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				kernelcore_remaining -= min(kernel_pages,
 							kernelcore_remaining);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				required_kernelcore -= min(kernel_pages,
 							required_kernelcore);
 
@@ -6468,6 +6779,7 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 					zone_movable_pfn[nid] = end_pfn;
 					continue;
 				}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				start_pfn = usable_startpfn;
 			}
 
@@ -6478,7 +6790,10 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 			 */
 			size_pages = end_pfn - start_pfn;
 			if (size_pages > kernelcore_remaining)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				size_pages = kernelcore_remaining;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zone_movable_pfn[nid] = start_pfn + size_pages;
 
 			/*
@@ -6501,6 +6816,7 @@ static void __init find_zone_movable_pfns_for_nodes(void)
 	 * satisfied
 	 */
 	usable_nodes--;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (usable_nodes && required_kernelcore > usable_nodes)
 		goto restart;
 
@@ -6521,11 +6837,15 @@ static void check_for_memory(pg_data_t *pgdat, int nid)
 	enum zone_type zone_type;
 
 	if (N_MEMORY == N_NORMAL_MEMORY)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zone_type = 0; zone_type <= ZONE_MOVABLE - 1; zone_type++) {
 		struct zone *zone = &pgdat->node_zones[zone_type];
 		if (populated_zone(zone)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			node_set_state(nid, N_HIGH_MEMORY);
 			if (N_NORMAL_MEMORY != N_HIGH_MEMORY &&
 			    zone_type <= ZONE_NORMAL)
@@ -6597,8 +6917,10 @@ void __init free_area_init_nodes(unsigned long *max_zone_pfn)
 	pr_info("Movable zone start for each node\n");
 	for (i = 0; i < MAX_NUMNODES; i++) {
 		if (zone_movable_pfn[i])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_info("  Node %d: %#018Lx\n", i,
 			       (u64)zone_movable_pfn[i] << PAGE_SHIFT);
+}
 	}
 
 	/* Print out the early node map */
@@ -6618,7 +6940,9 @@ void __init free_area_init_nodes(unsigned long *max_zone_pfn)
 
 		/* Any memory on that node */
 		if (pgdat->node_present_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			node_set_state(nid, N_MEMORY);
+}
 		check_for_memory(pgdat, nid);
 	}
 }
@@ -6627,7 +6951,9 @@ static int __init cmdline_parse_core(char *p, unsigned long *core)
 {
 	unsigned long long coremem;
 	if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	coremem = memparse(p, &p);
 	*core = coremem >> PAGE_SHIFT;
@@ -6659,6 +6985,7 @@ static int __init cmdline_parse_kernelcore(char *p)
  */
 static int __init cmdline_parse_movablecore(char *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return cmdline_parse_core(p, &required_movablecore);
 }
 
@@ -6669,6 +6996,7 @@ early_param("movablecore", cmdline_parse_movablecore);
 
 void adjust_managed_page_count(struct page *page, long count)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&managed_page_count_lock);
 	page_zone(page)->managed_pages += count;
 	totalram_pages += count;
@@ -6689,7 +7017,9 @@ unsigned long free_reserved_area(void *start, void *end, int poison, char *s)
 	end = (void *)((unsigned long)end & PAGE_MASK);
 	for (pos = start; pos < end; pos += PAGE_SIZE, pages++) {
 		if ((unsigned int)poison <= 0xFF)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			memset(pos, poison, PAGE_SIZE);
+}
 		free_reserved_page(virt_to_page(pos));
 	}
 
@@ -6789,6 +7119,7 @@ void __init free_area_init(unsigned long *zones_size)
 static int page_alloc_cpu_dead(unsigned int cpu)
 {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lru_add_drain_cpu(cpu);
 	drain_pages(cpu);
 
@@ -6842,14 +7173,18 @@ static void calculate_totalreserve_pages(void)
 			/* Find valid and maximum lowmem_reserve in the zone */
 			for (j = i; j < MAX_NR_ZONES; j++) {
 				if (zone->lowmem_reserve[j] > max)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					max = zone->lowmem_reserve[j];
+}
 			}
 
 			/* we treat the high watermark as reserved pages. */
 			max += high_wmark_pages(zone);
 
 			if (max > zone->managed_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				max = zone->managed_pages;
+}
 
 			pgdat->totalreserve_pages += max;
 
@@ -6907,6 +7242,7 @@ static void __setup_per_zone_wmarks(void)
 
 	/* Calculate total number of !ZONE_HIGHMEM pages */
 	for_each_zone(zone) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!is_highmem(zone))
 			lowmem_pages += zone->managed_pages;
 	}
@@ -6930,6 +7266,7 @@ static void __setup_per_zone_wmarks(void)
 			unsigned long min_pages;
 
 			min_pages = zone->managed_pages / 1024;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			min_pages = clamp(min_pages, SWAP_CLUSTER_MAX, 128UL);
 			zone->watermark[WMARK_MIN] = min_pages;
 		} else {
@@ -7010,10 +7347,15 @@ int __meminit init_per_zone_wmark_min(void)
 	if (new_min_free_kbytes > user_min_free_kbytes) {
 		min_free_kbytes = new_min_free_kbytes;
 		if (min_free_kbytes < 128)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			min_free_kbytes = 128;
+}
 		if (min_free_kbytes > 65536)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			min_free_kbytes = 65536;
+}
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("min_free_kbytes is not updated to %d because user defined value %d is preferred\n",
 				new_min_free_kbytes, user_min_free_kbytes);
 	}
@@ -7042,7 +7384,9 @@ int min_free_kbytes_sysctl_handler(struct ctl_table *table, int write,
 
 	rc = proc_dointvec_minmax(table, write, buffer, length, ppos);
 	if (rc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return rc;
+}
 
 	if (write) {
 		user_min_free_kbytes = min_free_kbytes;
@@ -7058,7 +7402,9 @@ int watermark_scale_factor_sysctl_handler(struct ctl_table *table, int write,
 
 	rc = proc_dointvec_minmax(table, write, buffer, length, ppos);
 	if (rc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return rc;
+}
 
 	if (write)
 		setup_per_zone_wmarks();
@@ -7072,6 +7418,7 @@ static void setup_min_unmapped_ratio(void)
 	pg_data_t *pgdat;
 	struct zone *zone;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_online_pgdat(pgdat)
 		pgdat->min_unmapped_pages = 0;
 
@@ -7088,7 +7435,9 @@ int sysctl_min_unmapped_ratio_sysctl_handler(struct ctl_table *table, int write,
 
 	rc = proc_dointvec_minmax(table, write, buffer, length, ppos);
 	if (rc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return rc;
+}
 
 	setup_min_unmapped_ratio();
 
@@ -7100,6 +7449,7 @@ static void setup_min_slab_ratio(void)
 	pg_data_t *pgdat;
 	struct zone *zone;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_online_pgdat(pgdat)
 		pgdat->min_slab_pages = 0;
 
@@ -7115,7 +7465,9 @@ int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,
 
 	rc = proc_dointvec_minmax(table, write, buffer, length, ppos);
 	if (rc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return rc;
+}
 
 	setup_min_slab_ratio();
 
@@ -7135,6 +7487,7 @@ int sysctl_min_slab_ratio_sysctl_handler(struct ctl_table *table, int write,
 int lowmem_reserve_ratio_sysctl_handler(struct ctl_table *table, int write,
 	void __user *buffer, size_t *length, loff_t *ppos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	proc_dointvec_minmax(table, write, buffer, length, ppos);
 	setup_per_zone_lowmem_reserve();
 	return 0;
@@ -7156,6 +7509,7 @@ int percpu_pagelist_fraction_sysctl_handler(struct ctl_table *table, int write,
 	old_percpu_pagelist_fraction = percpu_pagelist_fraction;
 
 	ret = proc_dointvec_minmax(table, write, buffer, length, ppos);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!write || ret < 0)
 		goto out;
 
@@ -7188,6 +7542,7 @@ int hashdist = HASHDIST_DEFAULT;
 
 static int __init set_hashdist(char *str)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!str)
 		return 0;
 	hashdist = simple_strtoul(str, &str, 0);
@@ -7203,6 +7558,7 @@ __setup("hashdist=", set_hashdist);
  */
 static unsigned long __init arch_reserved_kernel_pages(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 #endif
@@ -7259,6 +7615,7 @@ void *__init alloc_large_system_hash(const char *tablename,
 
 			for (adapt = ADAPT_SCALE_NPAGES; adapt < numentries;
 			     adapt <<= ADAPT_SCALE_SHIFT)
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				scale++;
 		}
 #endif
@@ -7274,11 +7631,15 @@ void *__init alloc_large_system_hash(const char *tablename,
 			/* Makes no sense without HASH_EARLY */
 			WARN_ON(!(flags & HASH_EARLY));
 			if (!(numentries >> *_hash_shift)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				numentries = 1UL << *_hash_shift;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				BUG_ON(!numentries);
 			}
 		} else if (unlikely((numentries * bucketsize) < PAGE_SIZE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			numentries = PAGE_SIZE / bucketsize;
+}
 	}
 	numentries = roundup_pow_of_two(numentries);
 
@@ -7290,9 +7651,13 @@ void *__init alloc_large_system_hash(const char *tablename,
 	max = min(max, 0x80000000ULL);
 
 	if (numentries < low_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		numentries = low_limit;
+}
 	if (numentries > max)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		numentries = max;
+}
 
 	log2qty = ilog2(numentries);
 
@@ -7305,8 +7670,11 @@ void *__init alloc_large_system_hash(const char *tablename,
 		size = bucketsize << log2qty;
 		if (flags & HASH_EARLY)
 			table = memblock_virt_alloc_nopanic(size, 0);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		else if (hashdist)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			table = __vmalloc(size, gfp_flags, PAGE_KERNEL);
+}
 		else {
 			/*
 			 * If bucketsize is not a power-of-two, we may free
@@ -7321,7 +7689,9 @@ void *__init alloc_large_system_hash(const char *tablename,
 	} while (!table && size > PAGE_SIZE && --log2qty);
 
 	if (!table)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("Failed to allocate %s hash table\n", tablename);
+}
 
 	pr_info("%s hash table entries: %ld (order: %d, %lu bytes)\n",
 		tablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size);
@@ -7354,7 +7724,9 @@ bool has_unmovable_pages(struct zone *zone, struct page *page, int count,
 	 * If ZONE_MOVABLE, the zone never contains unmovable pages
 	 */
 	if (zone_idx(zone) == ZONE_MOVABLE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 	mt = get_pageblock_migratetype(page);
 	if (mt == MIGRATE_MOVABLE || is_migrate_cma(mt))
 		return false;
@@ -7434,7 +7806,9 @@ bool is_pageblock_removable_nolock(struct page *page)
 	 * its NODE_DATA will be NULL - see page_zone.
 	 */
 	if (!node_online(page_to_nid(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	zone = page_zone(page);
 	pfn = page_to_pfn(page);
@@ -7764,6 +8138,7 @@ __offline_isolated_pages(unsigned long start_pfn, unsigned long end_pfn)
 
 bool is_free_buddy_page(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct zone *zone = page_zone(page);
 	unsigned long pfn = page_to_pfn(page);
 	unsigned long flags;
diff --git a/mm/page_counter.c b/mm/page_counter.c
index 2a8df3a..553e2b5 100644
--- a/mm/page_counter.c
+++ b/mm/page_counter.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Lockless hierarchical page accounting & limiting
diff --git a/mm/page_ext.c b/mm/page_ext.c
index 2c16216..5c875b1 100644
--- a/mm/page_ext.c
+++ b/mm/page_ext.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/mm.h>
 #include <linux/mmzone.h>
diff --git a/mm/page_poison.c b/mm/page_poison.c
index e83fd44..8d2725a 100644
--- a/mm/page_poison.c
+++ b/mm/page_poison.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/kernel.h>
 #include <linux/string.h>
diff --git a/mm/pagewalk.c b/mm/pagewalk.c
index 23a3e41..1187ffe 100644
--- a/mm/pagewalk.c
+++ b/mm/pagewalk.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 #include <linux/mm.h>
 #include <linux/highmem.h>
@@ -12,6 +14,7 @@ static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
 
 	pte = pte_offset_map(pmd, addr);
 	for (;;) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = walk->pte_entry(pte, addr, addr + PAGE_SIZE, walk);
 		if (err)
 		       break;
@@ -38,7 +41,9 @@ static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,
 		next = pmd_addr_end(addr, end);
 		if (pmd_none(*pmd) || !walk->vma) {
 			if (walk->pte_hole)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				err = walk->pte_hole(addr, next, walk);
+}
 			if (err)
 				break;
 			continue;
@@ -59,9 +64,12 @@ static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,
 		if (!walk->pte_entry)
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		split_huge_pmd(walk->vma, pmd, addr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (pmd_trans_unstable(pmd))
 			goto again;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = walk_pte_range(pmd, addr, next, walk);
 		if (err)
 			break;
@@ -83,16 +91,20 @@ static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,
 		next = pud_addr_end(addr, end);
 		if (pud_none(*pud) || !walk->vma) {
 			if (walk->pte_hole)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				err = walk->pte_hole(addr, next, walk);
+}
 			if (err)
 				break;
 			continue;
 		}
 
 		if (walk->pud_entry) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spinlock_t *ptl = pud_trans_huge_lock(pud, walk->vma);
 
 			if (ptl) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				err = walk->pud_entry(pud, addr, next, walk);
 				spin_unlock(ptl);
 				if (err)
@@ -123,10 +135,15 @@ static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,
 
 	p4d = p4d_offset(pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(p4d)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (walk->pte_hole)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				err = walk->pte_hole(addr, next, walk);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (err)
 				break;
 			continue;
@@ -135,6 +152,7 @@ static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,
 			err = walk_pud_range(p4d, addr, next, walk);
 		if (err)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (p4d++, addr = next, addr != end);
 
 	return err;
@@ -151,8 +169,12 @@ static int walk_pgd_range(unsigned long addr, unsigned long end,
 	do {
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (walk->pte_hole)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				err = walk->pte_hole(addr, next, walk);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (err)
 				break;
 			continue;
@@ -170,6 +192,7 @@ static int walk_pgd_range(unsigned long addr, unsigned long end,
 static unsigned long hugetlb_entry_end(struct hstate *h, unsigned long addr,
 				       unsigned long end)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long boundary = (addr & huge_page_mask(h)) + huge_page_size(h);
 	return boundary < end ? boundary : end;
 }
@@ -186,6 +209,7 @@ static int walk_hugetlb_range(unsigned long addr, unsigned long end,
 	int err = 0;
 
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = hugetlb_entry_end(h, addr, end);
 		pte = huge_pte_offset(walk->mm, addr & hmask, sz);
 
@@ -222,7 +246,9 @@ static int walk_page_test(unsigned long start, unsigned long end,
 	struct vm_area_struct *vma = walk->vma;
 
 	if (walk->test_walk)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return walk->test_walk(start, end, walk);
+}
 
 	/*
 	 * vma(VM_PFNMAP) doesn't have any valid struct pages behind VM_PFNMAP
@@ -235,9 +261,13 @@ static int walk_page_test(unsigned long start, unsigned long end,
 	if (vma->vm_flags & VM_PFNMAP) {
 		int err = 1;
 		if (walk->pte_hole)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			err = walk->pte_hole(start, end, walk);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return err ? err : 1;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -248,8 +278,11 @@ static int __walk_page_range(unsigned long start, unsigned long end,
 	struct vm_area_struct *vma = walk->vma;
 
 	if (vma && is_vm_hugetlb_page(vma)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (walk->hugetlb_entry)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			err = walk_hugetlb_range(start, end, walk);
+}
 	} else
 		err = walk_pgd_range(start, end, walk);
 
@@ -294,20 +327,28 @@ int walk_page_range(unsigned long start, unsigned long end,
 	struct vm_area_struct *vma;
 
 	if (start >= end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (!walk->mm)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_MM(!rwsem_is_locked(&walk->mm->mmap_sem), walk->mm);
 
 	vma = find_vma(walk->mm, start);
 	do {
 		if (!vma) { /* after the last vma */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			walk->vma = NULL;
 			next = end;
 		} else if (start < vma->vm_start) { /* outside vma */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			walk->vma = NULL;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			next = min(end, vma->vm_start);
 		} else { /* inside vma */
 			walk->vma = vma;
@@ -332,6 +373,7 @@ int walk_page_range(unsigned long start, unsigned long end,
 		if (err)
 			break;
 	} while (start = next, start < end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return err;
 }
 
@@ -340,7 +382,9 @@ int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk)
 	int err;
 
 	if (!walk->mm)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	VM_BUG_ON(!rwsem_is_locked(&walk->mm->mmap_sem));
 	VM_BUG_ON(!vma);
diff --git a/mm/percpu-internal.h b/mm/percpu-internal.h
index b1739dc..90e2e1e 100644
--- a/mm/percpu-internal.h
+++ b/mm/percpu-internal.h
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _MM_PERCPU_INTERNAL_H
 #define _MM_PERCPU_INTERNAL_H
diff --git a/mm/percpu-vm.c b/mm/percpu-vm.c
index 15dab69..a113748 100644
--- a/mm/percpu-vm.c
+++ b/mm/percpu-vm.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/percpu-vm.c - vmalloc area based chunk allocation
  *
diff --git a/mm/percpu.c b/mm/percpu.c
index a0e0c82..e1e261b 100644
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/percpu.c - percpu memory allocator
  *
@@ -198,7 +200,9 @@ static bool pcpu_addr_in_chunk(struct pcpu_chunk *chunk, void *addr)
 	void *start_addr, *end_addr;
 
 	if (!chunk)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	start_addr = chunk->base_addr + chunk->start_offset;
 	end_addr = chunk->base_addr + chunk->nr_pages * PAGE_SIZE -
@@ -209,6 +213,7 @@ static bool pcpu_addr_in_chunk(struct pcpu_chunk *chunk, void *addr)
 
 static int __pcpu_size_to_slot(int size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int highbit = fls(size);	/* size is in bytes */
 	return max(highbit - PCPU_SLOT_BASE_SHIFT + 2, 1);
 }
@@ -223,7 +228,9 @@ static int pcpu_size_to_slot(int size)
 static int pcpu_chunk_slot(const struct pcpu_chunk *chunk)
 {
 	if (chunk->free_bytes < PCPU_MIN_ALLOC_SIZE || chunk->contig_bits == 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	return pcpu_size_to_slot(chunk->free_bytes);
 }
@@ -265,6 +272,7 @@ static void pcpu_next_unpop(unsigned long *bitmap, int *rs, int *re, int end)
 
 static void pcpu_next_pop(unsigned long *bitmap, int *rs, int *re, int end)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	*rs = find_next_bit(bitmap, end, *rs);
 	*re = find_next_zero_bit(bitmap, end, *rs + 1);
 }
@@ -335,6 +343,7 @@ static void pcpu_next_md_free_region(struct pcpu_chunk *chunk, int *bit_off,
 			*bits += block->left_free;
 			if (block->left_free == PCPU_BITMAP_BLOCK_BITS)
 				continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
 		}
 
@@ -389,7 +398,9 @@ static void pcpu_next_fit_region(struct pcpu_chunk *chunk, int alloc_bits,
 		if (*bits) {
 			*bits += block->left_free;
 			if (*bits >= alloc_bits)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return;
+}
 			if (block->left_free == PCPU_BITMAP_BLOCK_BITS)
 				continue;
 		}
@@ -417,7 +428,9 @@ static void pcpu_next_fit_region(struct pcpu_chunk *chunk, int alloc_bits,
 		*bits = PCPU_BITMAP_BLOCK_BITS - *bit_off;
 		*bit_off = pcpu_block_off_to_off(i, *bit_off);
 		if (*bits >= alloc_bits)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
 	}
 
 	/* no valid offsets were found - fail condition */
@@ -461,7 +474,9 @@ static void pcpu_next_fit_region(struct pcpu_chunk *chunk, int alloc_bits,
 static void *pcpu_mem_zalloc(size_t size)
 {
 	if (WARN_ON_ONCE(!slab_is_available()))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (size <= PAGE_SIZE)
 		return kzalloc(size, GFP_KERNEL);
@@ -477,6 +492,7 @@ static void *pcpu_mem_zalloc(size_t size)
  */
 static void pcpu_mem_free(void *ptr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kvfree(ptr);
 }
 
@@ -525,7 +541,9 @@ static inline int pcpu_cnt_pop_pages(struct pcpu_chunk *chunk, int bit_off,
 	int page_end = PFN_DOWN((bit_off + bits) * PCPU_MIN_ALLOC_SIZE);
 
 	if (page_start >= page_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * bitmap_weight counts the number of bits set in a bitmap up to
@@ -736,6 +754,7 @@ static void pcpu_block_update_hint_alloc(struct pcpu_chunk *chunk, int bit_off,
 				/* contig hint is broken - scan to fix it */
 				pcpu_block_refresh_hint(chunk, e_index);
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				e_block->left_free = 0;
 				e_block->right_free =
 					min_t(int, e_block->right_free,
@@ -745,6 +764,7 @@ static void pcpu_block_update_hint_alloc(struct pcpu_chunk *chunk, int bit_off,
 
 		/* update in-between md_blocks */
 		for (block = s_block + 1; block < e_block; block++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			block->contig_hint = 0;
 			block->left_free = 0;
 			block->right_free = 0;
@@ -812,6 +832,7 @@ static void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,
 	 */
 	start = s_off;
 	if (s_off == s_block->contig_hint + s_block->contig_hint_start) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start = s_block->contig_hint_start;
 	} else {
 		/*
@@ -825,6 +846,7 @@ static void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,
 		start = (start == l_bit) ? 0 : l_bit + 1;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	end = e_off;
 	if (e_off == e_block->contig_hint_start)
 		end = e_block->contig_hint_start + e_block->contig_hint;
@@ -843,6 +865,7 @@ static void pcpu_block_update_hint_free(struct pcpu_chunk *chunk, int bit_off,
 
 		/* reset md_blocks in the middle */
 		for (block = s_block + 1; block < e_block; block++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			block->first_free = 0;
 			block->contig_hint_start = 0;
 			block->contig_hint = PCPU_BITMAP_BLOCK_BITS;
@@ -890,8 +913,11 @@ static bool pcpu_is_populated(struct pcpu_chunk *chunk, int bit_off, int bits,
 	rs = page_start;
 	pcpu_next_unpop(chunk->populated, &rs, &re, page_end);
 	if (rs >= page_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	*next_off = re * PAGE_SIZE / PCPU_MIN_ALLOC_SIZE;
 	return false;
 }
@@ -929,7 +955,9 @@ static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
 	bit_off = ALIGN(chunk->contig_bits_start, align) -
 		  chunk->contig_bits_start;
 	if (bit_off + alloc_bits > chunk->contig_bits)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	bit_off = chunk->first_bit;
 	bits = 0;
@@ -938,12 +966,15 @@ static int pcpu_find_block_fit(struct pcpu_chunk *chunk, int alloc_bits,
 						   &next_off))
 			break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bit_off = next_off;
 		bits = 0;
 	}
 
 	if (bit_off == pcpu_chunk_map_bits(chunk))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	return bit_off;
 }
@@ -973,6 +1004,7 @@ static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
 	size_t align_mask = (align) ? (align - 1) : 0;
 	int bit_off, end, oslot;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&pcpu_lock);
 
 	oslot = pcpu_chunk_slot(chunk);
@@ -984,7 +1016,9 @@ static int pcpu_alloc_area(struct pcpu_chunk *chunk, int alloc_bits,
 	bit_off = bitmap_find_next_zero_area(chunk->alloc_map, end, start,
 					     alloc_bits, align_mask);
 	if (bit_off >= end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	/* update alloc map */
 	bitmap_set(chunk->alloc_map, bit_off, alloc_bits);
@@ -1022,7 +1056,9 @@ static void pcpu_free_area(struct pcpu_chunk *chunk, int off)
 {
 	int bit_off, bits, end, oslot;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&pcpu_lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pcpu_stats_area_dealloc(chunk);
 
 	oslot = pcpu_chunk_slot(chunk);
@@ -1161,7 +1197,9 @@ static struct pcpu_chunk *pcpu_alloc_chunk(void)
 
 	chunk = pcpu_mem_zalloc(pcpu_chunk_struct_size);
 	if (!chunk)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	INIT_LIST_HEAD(&chunk->list);
 	chunk->nr_pages = pcpu_unit_pages;
@@ -1202,6 +1240,7 @@ static struct pcpu_chunk *pcpu_alloc_chunk(void)
 
 static void pcpu_free_chunk(struct pcpu_chunk *chunk)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!chunk)
 		return;
 	pcpu_mem_free(chunk->bound_map);
@@ -1228,6 +1267,7 @@ static void pcpu_chunk_populated(struct pcpu_chunk *chunk, int page_start,
 {
 	int nr = page_end - page_start;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&pcpu_lock);
 
 	bitmap_set(chunk->populated, page_start, nr);
@@ -1254,6 +1294,7 @@ static void pcpu_chunk_depopulated(struct pcpu_chunk *chunk,
 {
 	int nr = page_end - page_start;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&pcpu_lock);
 
 	bitmap_clear(chunk->populated, page_start, nr);
@@ -1304,11 +1345,15 @@ static struct pcpu_chunk *pcpu_chunk_addr_search(void *addr)
 {
 	/* is it in the dynamic region (first chunk)? */
 	if (pcpu_addr_in_chunk(pcpu_first_chunk, addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return pcpu_first_chunk;
+}
 
 	/* is it in the reserved region? */
 	if (pcpu_addr_in_chunk(pcpu_reserved_chunk, addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return pcpu_reserved_chunk;
+}
 
 	/*
 	 * The address is relative to unit0 which might be unused and
@@ -1356,7 +1401,9 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 	 * of up to PCPU_MIN_ALLOC_SIZE - 1 bytes.
 	 */
 	if (unlikely(align < PCPU_MIN_ALLOC_SIZE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		align = PCPU_MIN_ALLOC_SIZE;
+}
 
 	size = ALIGN(size, PCPU_MIN_ALLOC_SIZE);
 	bits = size >> PCPU_MIN_ALLOC_SHIFT;
@@ -1364,6 +1411,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 
 	if (unlikely(!size || size > PCPU_MIN_UNIT_SIZE || align > PAGE_SIZE ||
 		     !is_power_of_2(align))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN(do_warn, "illegal size (%zu) or align (%zu) for percpu allocation\n",
 		     size, align);
 		return NULL;
@@ -1376,18 +1424,22 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 
 	/* serve reserved allocations from the reserved chunk if available */
 	if (reserved && pcpu_reserved_chunk) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		chunk = pcpu_reserved_chunk;
 
 		off = pcpu_find_block_fit(chunk, bits, bit_align, is_atomic);
 		if (off < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			err = "alloc from reserved chunk failed";
 			goto fail_unlock;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		off = pcpu_alloc_area(chunk, bits, bit_align, off);
 		if (off >= 0)
 			goto area_found;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = "alloc from reserved chunk failed";
 		goto fail_unlock;
 	}
@@ -1408,6 +1460,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock_irqrestore(&pcpu_lock, flags);
 
 	/*
@@ -1416,6 +1469,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 	 * there's still no empty chunk after grabbing the mutex.
 	 */
 	if (is_atomic) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = "atomic alloc failed, no space left";
 		goto fail;
 	}
@@ -1423,6 +1477,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 	if (list_empty(&pcpu_slot[pcpu_nr_slots - 1])) {
 		chunk = pcpu_create_chunk();
 		if (!chunk) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			err = "failed to allocate new chunk";
 			goto fail;
 		}
@@ -1430,6 +1485,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 		spin_lock_irqsave(&pcpu_lock, flags);
 		pcpu_chunk_relocate(chunk, -1);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irqsave(&pcpu_lock, flags);
 	}
 
@@ -1454,6 +1510,7 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 
 			spin_lock_irqsave(&pcpu_lock, flags);
 			if (ret) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				pcpu_free_area(chunk, off);
 				err = "failed to populate";
 				goto fail_unlock;
@@ -1485,20 +1542,27 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 fail:
 	trace_percpu_alloc_percpu_fail(reserved, is_atomic, size, align);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!is_atomic && do_warn && warn_limit) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("allocation failed, size=%zu align=%zu atomic=%d, %s\n",
 			size, align, is_atomic, err);
 		dump_stack();
 		if (!--warn_limit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_info("limit reached, disable warning\n");
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_atomic) {
 		/* see the flag handling in pcpu_blance_workfn() */
 		pcpu_atomic_alloc_failed = true;
 		pcpu_schedule_balance_work();
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mutex_unlock(&pcpu_alloc_mutex);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1554,6 +1618,7 @@ EXPORT_SYMBOL_GPL(__alloc_percpu);
  */
 void __percpu *__alloc_reserved_percpu(size_t size, size_t align)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return pcpu_alloc(size, align, true, GFP_KERNEL);
 }
 
@@ -1578,27 +1643,33 @@ static void pcpu_balance_workfn(struct work_struct *work)
 	spin_lock_irq(&pcpu_lock);
 
 	list_for_each_entry_safe(chunk, next, free_head, list) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON(chunk->immutable);
 
 		/* spare the first one */
 		if (chunk == list_first_entry(free_head, struct pcpu_chunk, list))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		list_move(&chunk->list, &to_free);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock_irq(&pcpu_lock);
 
 	list_for_each_entry_safe(chunk, next, &to_free, list) {
 		int rs, re;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pcpu_for_each_pop_region(chunk->populated, rs, re, 0,
 					 chunk->nr_pages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pcpu_depopulate_chunk(chunk, rs, re);
 			spin_lock_irq(&pcpu_lock);
 			pcpu_chunk_depopulated(chunk, rs, re);
 			spin_unlock_irq(&pcpu_lock);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pcpu_destroy_chunk(chunk);
 	}
 
@@ -1614,6 +1685,7 @@ static void pcpu_balance_workfn(struct work_struct *work)
 	 */
 retry_pop:
 	if (pcpu_atomic_alloc_failed) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr_to_pop = PCPU_EMPTY_POP_PAGES_HIGH;
 		/* best effort anyway, don't worry about synchronization */
 		pcpu_atomic_alloc_failed = false;
@@ -1629,12 +1701,14 @@ static void pcpu_balance_workfn(struct work_struct *work)
 		if (!nr_to_pop)
 			break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irq(&pcpu_lock);
 		list_for_each_entry(chunk, &pcpu_slot[slot], list) {
 			nr_unpop = chunk->nr_pages - chunk->nr_populated;
 			if (nr_unpop)
 				break;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irq(&pcpu_lock);
 
 		if (!nr_unpop)
@@ -1652,6 +1726,7 @@ static void pcpu_balance_workfn(struct work_struct *work)
 				pcpu_chunk_populated(chunk, rs, rs + nr, false);
 				spin_unlock_irq(&pcpu_lock);
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nr_to_pop = 0;
 			}
 
@@ -1664,6 +1739,7 @@ static void pcpu_balance_workfn(struct work_struct *work)
 		/* ran out of chunks to populate, create a new one and retry */
 		chunk = pcpu_create_chunk();
 		if (chunk) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_lock_irq(&pcpu_lock);
 			pcpu_chunk_relocate(chunk, -1);
 			spin_unlock_irq(&pcpu_lock);
@@ -1691,8 +1767,11 @@ void free_percpu(void __percpu *ptr)
 	int off;
 
 	if (!ptr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kmemleak_free_percpu(ptr);
 
 	addr = __pcpu_ptr_to_addr(ptr);
@@ -1708,8 +1787,10 @@ void free_percpu(void __percpu *ptr)
 	if (chunk->free_bytes == pcpu_unit_size) {
 		struct pcpu_chunk *pos;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		list_for_each_entry(pos, &pcpu_slot[pcpu_nr_slots - 1], list)
 			if (pos != chunk) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				pcpu_schedule_balance_work();
 				break;
 			}
@@ -1728,6 +1809,7 @@ bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr)
 	void __percpu *base = __addr_to_pcpu_ptr(pcpu_base_addr);
 	unsigned int cpu;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_possible_cpu(cpu) {
 		void *start = per_cpu_ptr(base, cpu);
 		void *va = (void *)addr;
@@ -1759,6 +1841,7 @@ bool __is_kernel_percpu_address(unsigned long addr, unsigned long *can_addr)
  */
 bool is_kernel_percpu_address(unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __is_kernel_percpu_address(addr, NULL);
 }
 
@@ -1812,6 +1895,7 @@ phys_addr_t per_cpu_ptr_to_phys(void *addr)
 			void *start = per_cpu_ptr(base, cpu);
 
 			if (addr >= start && addr < start + pcpu_unit_size) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				in_first_chunk = true;
 				break;
 			}
@@ -1825,9 +1909,11 @@ phys_addr_t per_cpu_ptr_to_phys(void *addr)
 			return page_to_phys(vmalloc_to_page(addr)) +
 			       offset_in_page(addr);
 	} else
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return page_to_phys(pcpu_addr_to_page(addr)) +
 		       offset_in_page(addr);
 }
+}
 
 /**
  * pcpu_alloc_alloc_info - allocate percpu allocation info
@@ -1858,7 +1944,10 @@ struct pcpu_alloc_info * __init pcpu_alloc_alloc_info(int nr_groups,
 
 	ptr = memblock_virt_alloc_nopanic(PFN_ALIGN(ai_size), 0);
 	if (!ptr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ai = ptr;
 	ptr += base_size;
 
@@ -2034,12 +2123,15 @@ int __init pcpu_setup_first_chunk(const struct pcpu_alloc_info *ai,
 	PCPU_SETUP_BUG_ON(ai->unit_size < size_sum);
 	PCPU_SETUP_BUG_ON(offset_in_page(ai->unit_size));
 	PCPU_SETUP_BUG_ON(ai->unit_size < PCPU_MIN_UNIT_SIZE);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	PCPU_SETUP_BUG_ON(!IS_ALIGNED(ai->unit_size, PCPU_BITMAP_BLOCK_SIZE));
 	PCPU_SETUP_BUG_ON(ai->dyn_size < PERCPU_DYNAMIC_EARLY_SIZE);
 	PCPU_SETUP_BUG_ON(!ai->dyn_size);
 	PCPU_SETUP_BUG_ON(!IS_ALIGNED(ai->reserved_size, PCPU_MIN_ALLOC_SIZE));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	PCPU_SETUP_BUG_ON(!(IS_ALIGNED(PCPU_BITMAP_BLOCK_SIZE, PAGE_SIZE) ||
 			    IS_ALIGNED(PAGE_SIZE, PCPU_BITMAP_BLOCK_SIZE)));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	PCPU_SETUP_BUG_ON(pcpu_verify_alloc_info(ai) < 0);
 
 	/* process group information and build config tables accordingly */
@@ -2175,6 +2267,7 @@ enum pcpu_fc pcpu_chosen_fc __initdata = PCPU_FC_AUTO;
 
 static int __init percpu_alloc_setup(char *str)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!str)
 		return -EINVAL;
 
@@ -2271,19 +2364,24 @@ static struct pcpu_alloc_info * __init pcpu_build_alloc_info(
 	upa = alloc_size / min_unit_size;
 	while (alloc_size % upa || (offset_in_page(alloc_size / upa)))
 		upa--;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	max_upa = upa;
 
 	/* group cpus according to their proximity */
 	for_each_possible_cpu(cpu) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		group = 0;
 	next_group:
 		for_each_possible_cpu(tcpu) {
 			if (cpu == tcpu)
 				break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (group_map[tcpu] == group && cpu_distance_fn &&
 			    (cpu_distance_fn(cpu, tcpu) > LOCAL_DISTANCE ||
 			     cpu_distance_fn(tcpu, cpu) > LOCAL_DISTANCE)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				group++;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nr_groups = max(nr_groups, group + 1);
 				goto next_group;
 			}
@@ -2321,9 +2419,11 @@ static struct pcpu_alloc_info * __init pcpu_build_alloc_info(
 		/* and then don't consume more memory */
 		if (allocs > last_allocs)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		last_allocs = allocs;
 		best_upa = upa;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	upa = best_upa;
 
 	/* allocate and fill alloc_info */
@@ -2332,7 +2432,9 @@ static struct pcpu_alloc_info * __init pcpu_build_alloc_info(
 
 	ai = pcpu_alloc_alloc_info(nr_groups, nr_units);
 	if (!ai)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 	cpu_map = ai->groups[0].cpu_map;
 
 	for (group = 0; group < nr_groups; group++) {
@@ -2365,6 +2467,7 @@ static struct pcpu_alloc_info * __init pcpu_build_alloc_info(
 	}
 	BUG_ON(unit != nr_units);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ai;
 }
 #endif /* BUILD_EMBED_FIRST_CHUNK || BUILD_PAGE_FIRST_CHUNK */
@@ -2418,13 +2521,16 @@ int __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,
 	ai = pcpu_build_alloc_info(reserved_size, dyn_size, atom_size,
 				   cpu_distance_fn);
 	if (IS_ERR(ai))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PTR_ERR(ai);
+}
 
 	size_sum = ai->static_size + ai->reserved_size + ai->dyn_size;
 	areas_size = PFN_ALIGN(ai->nr_groups * sizeof(void *));
 
 	areas = memblock_virt_alloc_nopanic(areas_size, 0);
 	if (!areas) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rc = -ENOMEM;
 		goto out_free;
 	}
@@ -2443,6 +2549,7 @@ int __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,
 		/* allocate space for the whole group */
 		ptr = alloc_fn(cpu, gi->nr_units * ai->unit_size, atom_size);
 		if (!ptr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			rc = -ENOMEM;
 			goto out_free_areas;
 		}
@@ -2452,13 +2559,16 @@ int __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,
 
 		base = min(ptr, base);
 		if (ptr > areas[highest_group])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			highest_group = group;
+}
 	}
 	max_distance = areas[highest_group] - base;
 	max_distance += ai->unit_size * ai->groups[highest_group].nr_units;
 
 	/* warn if maximum distance is further than 75% of vmalloc space */
 	if (max_distance > VMALLOC_TOTAL * 3 / 4) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("max_distance=0x%lx too large for vmalloc space 0x%lx\n",
 				max_distance, VMALLOC_TOTAL);
 #ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK
@@ -2503,13 +2613,18 @@ int __init pcpu_embed_first_chunk(size_t reserved_size, size_t dyn_size,
 
 out_free_areas:
 	for (group = 0; group < ai->nr_groups; group++)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (areas[group])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			free_fn(areas[group],
 				ai->groups[group].nr_units * ai->unit_size);
+}
 out_free:
 	pcpu_free_alloc_info(ai);
+}
 	if (areas)
 		memblock_free_early(__pa(areas), areas_size);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return rc;
 }
 #endif /* BUILD_EMBED_FIRST_CHUNK */
@@ -2550,7 +2665,9 @@ int __init pcpu_page_first_chunk(size_t reserved_size,
 
 	ai = pcpu_build_alloc_info(reserved_size, 0, PAGE_SIZE, NULL);
 	if (IS_ERR(ai))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PTR_ERR(ai);
+}
 	BUG_ON(ai->nr_groups != 1);
 	upa = ai->alloc_size/ai->unit_size;
 	nr_g0_units = roundup(num_possible_cpus(), upa);
diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c
index 1e4ee76..4777336 100644
--- a/mm/pgtable-generic.c
+++ b/mm/pgtable-generic.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  mm/pgtable-generic.c
@@ -19,24 +21,28 @@
 
 void pgd_clear_bad(pgd_t *pgd)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pgd_ERROR(*pgd);
 	pgd_clear(pgd);
 }
 
 void p4d_clear_bad(p4d_t *p4d)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	p4d_ERROR(*p4d);
 	p4d_clear(p4d);
 }
 
 void pud_clear_bad(pud_t *pud)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pud_ERROR(*pud);
 	pud_clear(pud);
 }
 
 void pmd_clear_bad(pmd_t *pmd)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pmd_ERROR(*pmd);
 	pmd_clear(pmd);
 }
diff --git a/mm/readahead.c b/mm/readahead.c
index c4ca702..e524c31 100644
--- a/mm/readahead.c
+++ b/mm/readahead.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/readahead.c - address_space-level file readahead.
  *
@@ -44,6 +46,7 @@ EXPORT_SYMBOL_GPL(file_ra_state_init);
 static void read_cache_pages_invalidate_page(struct address_space *mapping,
 					     struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (page_has_private(page)) {
 		if (!trylock_page(page))
 			BUG();
@@ -63,6 +66,7 @@ static void read_cache_pages_invalidate_pages(struct address_space *mapping,
 {
 	struct page *victim;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!list_empty(pages)) {
 		victim = lru_to_page(pages);
 		list_del(&victim->lru);
@@ -86,6 +90,7 @@ int read_cache_pages(struct address_space *mapping, struct list_head *pages,
 	struct page *page;
 	int ret = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!list_empty(pages)) {
 		page = lru_to_page(pages);
 		list_del(&page->lru);
@@ -124,13 +129,19 @@ static int read_pages(struct address_space *mapping, struct file *filp,
 		goto out;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (page_idx = 0; page_idx < nr_pages; page_idx++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		struct page *page = lru_to_page(pages);
 		list_del(&page->lru);
 		if (!add_to_page_cache_lru(page, mapping, page->index, gfp))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mapping->a_ops->readpage(filp, page);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_page(page);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = 0;
 
 out:
@@ -174,6 +185,7 @@ int __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 		if (page_offset > end_index)
 			break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rcu_read_lock();
 		page = radix_tree_lookup(&mapping->page_tree, page_offset);
 		rcu_read_unlock();
@@ -186,7 +198,9 @@ int __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 		page->index = page_offset;
 		list_add(&page->lru, &page_pool);
 		if (page_idx == nr_to_read - lookahead_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			SetPageReadahead(page);
+}
 		ret++;
 	}
 
@@ -209,6 +223,7 @@ int __do_page_cache_readahead(struct address_space *mapping, struct file *filp,
 int force_page_cache_readahead(struct address_space *mapping, struct file *filp,
 			       pgoff_t offset, unsigned long nr_to_read)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct backing_dev_info *bdi = inode_to_bdi(mapping->host);
 	struct file_ra_state *ra = &filp->f_ra;
 	unsigned long max_pages;
@@ -253,7 +268,9 @@ static unsigned long get_init_ra_size(unsigned long size, unsigned long max)
 	if (newsize <= max / 32)
 		newsize = newsize * 4;
 	else if (newsize <= max / 4)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		newsize = newsize * 2;
+}
 	else
 		newsize = max;
 
@@ -271,7 +288,9 @@ static unsigned long get_next_ra_size(struct file_ra_state *ra,
 	unsigned long newsize;
 
 	if (cur < max / 16)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		newsize = 4 * cur;
+}
 	else
 		newsize = 2 * cur;
 
@@ -353,16 +372,22 @@ static int try_context_readahead(struct address_space *mapping,
 	 * it could be a random read
 	 */
 	if (size <= req_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * starts from beginning of file:
 	 * it is a strong indication of long-run stream (or whole-file-read)
 	 */
 	if (size >= offset)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		size *= 2;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ra->start = offset;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ra->size = min(size + req_size, max);
 	ra->async_size = 1;
 
@@ -420,9 +445,13 @@ ondemand_readahead(struct address_space *mapping,
 		start = page_cache_next_hole(mapping, offset + 1, max_pages);
 		rcu_read_unlock();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!start || start - offset > max_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ra->start = start;
 		ra->size = start - offset;	/* old async_size */
 		ra->size += req_size;
@@ -498,10 +527,13 @@ void page_cache_sync_readahead(struct address_space *mapping,
 {
 	/* no read-ahead */
 	if (!ra->ra_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/* be dumb */
 	if (filp && (filp->f_mode & FMODE_RANDOM)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		force_page_cache_readahead(mapping, filp, offset, req_size);
 		return;
 	}
@@ -534,21 +566,28 @@ page_cache_async_readahead(struct address_space *mapping,
 {
 	/* no read-ahead */
 	if (!ra->ra_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * Same bit is used for PG_readahead and PG_reclaim.
 	 */
 	if (PageWriteback(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ClearPageReadahead(page);
 
 	/*
 	 * Defer asynchronous read-ahead on IO congestion.
 	 */
 	if (inode_read_congested(mapping->host))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/* do read-ahead */
 	ondemand_readahead(mapping, ra, filp, true, offset, req_size);
@@ -559,6 +598,7 @@ static ssize_t
 do_readahead(struct address_space *mapping, struct file *filp,
 	     pgoff_t index, unsigned long nr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!mapping || !mapping->a_ops)
 		return -EINVAL;
 
@@ -581,6 +621,7 @@ SYSCALL_DEFINE3(readahead, int, fd, loff_t, offset, size_t, count)
 	ret = -EBADF;
 	f = fdget(fd);
 	if (f.file) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (f.file->f_mode & FMODE_READ) {
 			struct address_space *mapping = f.file->f_mapping;
 			pgoff_t start = offset >> PAGE_SHIFT;
diff --git a/mm/rmap.c b/mm/rmap.c
index b874c47..c6698ec 100644
--- a/mm/rmap.c
+++ b/mm/rmap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/rmap.c - physical to virtual reverse mappings
  *
@@ -80,6 +82,7 @@ static inline struct anon_vma *anon_vma_alloc(void)
 
 	anon_vma = kmem_cache_alloc(anon_vma_cachep, GFP_KERNEL);
 	if (anon_vma) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		atomic_set(&anon_vma->refcount, 1);
 		anon_vma->degree = 1;	/* Reference for first vma */
 		anon_vma->parent = anon_vma;
@@ -95,6 +98,7 @@ static inline struct anon_vma *anon_vma_alloc(void)
 
 static inline void anon_vma_free(struct anon_vma *anon_vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(atomic_read(&anon_vma->refcount));
 
 	/*
@@ -116,6 +120,7 @@ static inline void anon_vma_free(struct anon_vma *anon_vma)
 	 */
 	might_sleep();
 	if (rwsem_is_locked(&anon_vma->root->rwsem)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		anon_vma_lock_write(anon_vma);
 		anon_vma_unlock_write(anon_vma);
 	}
@@ -189,9 +194,11 @@ int __anon_vma_prepare(struct vm_area_struct *vma)
 		anon_vma = anon_vma_alloc();
 		if (unlikely(!anon_vma))
 			goto out_enomem_free_avc;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		allocated = anon_vma;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	anon_vma_lock_write(anon_vma);
 	/* page_table_lock to protect against threads */
 	spin_lock(&mm->page_table_lock);
@@ -203,14 +210,20 @@ int __anon_vma_prepare(struct vm_area_struct *vma)
 		allocated = NULL;
 		avc = NULL;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&mm->page_table_lock);
 	anon_vma_unlock_write(anon_vma);
 
 	if (unlikely(allocated))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_anon_vma(allocated);
+}
 	if (unlikely(avc))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		anon_vma_chain_free(avc);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
  out_enomem_free_avc:
@@ -232,7 +245,10 @@ static inline struct anon_vma *lock_anon_vma_root(struct anon_vma *root, struct
 	struct anon_vma *new_root = anon_vma->root;
 	if (new_root != root) {
 		if (WARN_ON_ONCE(root))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			up_write(&root->rwsem);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		root = new_root;
 		down_write(&root->rwsem);
 	}
@@ -267,6 +283,7 @@ int anon_vma_clone(struct vm_area_struct *dst, struct vm_area_struct *src)
 
 		avc = anon_vma_chain_alloc(GFP_NOWAIT | __GFP_NOWARN);
 		if (unlikely(!avc)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_anon_vma_root(root);
 			root = NULL;
 			avc = anon_vma_chain_alloc(GFP_KERNEL);
@@ -319,7 +336,9 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 
 	/* Don't bother if the parent process has no anon_vma here. */
 	if (!pvma->anon_vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Drop inherited anon_vma, we'll reuse existing or allocate new. */
 	vma->anon_vma = NULL;
@@ -330,11 +349,15 @@ int anon_vma_fork(struct vm_area_struct *vma, struct vm_area_struct *pvma)
 	 */
 	error = anon_vma_clone(vma, pvma);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	/* An existing anon_vma has been reused, all done then. */
 	if (vma->anon_vma)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Then add our own anon_vma. */
 	anon_vma = anon_vma_alloc();
@@ -424,6 +447,7 @@ static void anon_vma_ctor(void *data)
 	struct anon_vma *anon_vma = data;
 
 	init_rwsem(&anon_vma->rwsem);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	atomic_set(&anon_vma->refcount, 0);
 	anon_vma->rb_root = RB_ROOT_CACHED;
 }
@@ -466,6 +490,7 @@ struct anon_vma *page_get_anon_vma(struct page *page)
 	unsigned long anon_mapping;
 
 	rcu_read_lock();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	anon_mapping = (unsigned long)READ_ONCE(page->mapping);
 	if ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)
 		goto out;
@@ -510,6 +535,7 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 	unsigned long anon_mapping;
 
 	rcu_read_lock();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	anon_mapping = (unsigned long)READ_ONCE(page->mapping);
 	if ((anon_mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)
 		goto out;
@@ -567,6 +593,7 @@ struct anon_vma *page_lock_anon_vma_read(struct page *page)
 
 void page_unlock_anon_vma_read(struct anon_vma *anon_vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	anon_vma_unlock_read(anon_vma);
 }
 
@@ -579,6 +606,7 @@ void page_unlock_anon_vma_read(struct anon_vma *anon_vma)
  */
 void try_to_unmap_flush(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;
 
 	if (!tlb_ubc->flush_required)
@@ -592,6 +620,7 @@ void try_to_unmap_flush(void)
 /* Flush iff there are potentially writable TLB entries that can race with IO */
 void try_to_unmap_flush_dirty(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;
 
 	if (tlb_ubc->writable)
@@ -600,6 +629,7 @@ void try_to_unmap_flush_dirty(void)
 
 static void set_tlb_ubc_flush_pending(struct mm_struct *mm, bool writable)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct tlbflush_unmap_batch *tlb_ubc = &current->tlb_ubc;
 
 	arch_tlbbatch_add_mm(&tlb_ubc->arch, mm);
@@ -630,7 +660,9 @@ static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)
 	bool should_defer = false;
 
 	if (!(flags & TTU_BATCH_FLUSH))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	/* If remote CPUs need to be flushed then defer batch the flush */
 	if (cpumask_any_but(mm_cpumask(mm), get_cpu()) < nr_cpu_ids)
@@ -658,6 +690,7 @@ static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)
 void flush_tlb_batched_pending(struct mm_struct *mm)
 {
 	if (mm->tlb_flush_batched) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_tlb_mm(mm);
 
 		/*
@@ -687,6 +720,7 @@ unsigned long page_address_in_vma(struct page *page, struct vm_area_struct *vma)
 {
 	unsigned long address;
 	if (PageAnon(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		struct anon_vma *page__anon_vma = page_anon_vma(page);
 		/*
 		 * Note: swapoff's unuse_vma() is more efficient with this
@@ -718,6 +752,7 @@ pmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)
 	if (!pgd_present(*pgd))
 		goto out;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	p4d = p4d_offset(pgd, address);
 	if (!p4d_present(*p4d))
 		goto out;
@@ -735,7 +770,9 @@ pmd_t *mm_find_pmd(struct mm_struct *mm, unsigned long address)
 	pmde = *pmd;
 	barrier();
 	if (!pmd_present(pmde) || pmd_trans_huge(pmde))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pmd = NULL;
+}
 out:
 	return pmd;
 }
@@ -760,6 +797,7 @@ static bool page_referenced_one(struct page *page, struct vm_area_struct *vma,
 	};
 	int referenced = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (page_vma_mapped_walk(&pvmw)) {
 		address = pvmw.address;
 
@@ -817,7 +855,9 @@ static bool invalid_page_referenced_vma(struct vm_area_struct *vma, void *arg)
 	struct mem_cgroup *memcg = pra->memcg;
 
 	if (!mm_match_cgroup(vma->vm_mm, memcg))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
 	return false;
 }
@@ -850,7 +890,9 @@ int page_referenced(struct page *page,
 
 	*vm_flags = 0;
 	if (!page_mapped(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (!page_rmapping(page))
 		return 0;
@@ -952,6 +994,7 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,
 
 static bool invalid_mkclean_vma(struct vm_area_struct *vma, void *arg)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (vma->vm_flags & VM_SHARED)
 		return false;
 
@@ -968,6 +1011,7 @@ int page_mkclean(struct page *page)
 		.invalid_vma = invalid_mkclean_vma,
 	};
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!PageLocked(page));
 
 	if (!page_mapped(page))
@@ -1026,7 +1070,9 @@ static void __page_set_anon_rmap(struct page *page,
 	BUG_ON(!anon_vma);
 
 	if (PageAnon(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * If the page isn't exclusively mapped into this vma,
@@ -1034,7 +1080,9 @@ static void __page_set_anon_rmap(struct page *page,
 	 * page mapping!
 	 */
 	if (!exclusive)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		anon_vma = anon_vma->root;
+}
 
 	anon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;
 	page->mapping = (struct address_space *) anon_vma;
@@ -1083,6 +1131,7 @@ static void __page_check_anon_rmap(struct page *page,
 void page_add_anon_rmap(struct page *page,
 	struct vm_area_struct *vma, unsigned long address, bool compound)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	do_page_add_anon_rmap(page, vma, address, compound ? RMAP_COMPOUND : 0);
 }
 
@@ -1104,6 +1153,7 @@ void do_page_add_anon_rmap(struct page *page,
 		mapcount = compound_mapcount_ptr(page);
 		first = atomic_inc_and_test(mapcount);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		first = atomic_inc_and_test(&page->_mapcount);
 	}
 
@@ -1151,6 +1201,7 @@ void page_add_new_anon_rmap(struct page *page,
 	VM_BUG_ON_VMA(address < vma->vm_start || address >= vma->vm_end, vma);
 	__SetPageSwapBacked(page);
 	if (compound) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(!PageTransHuge(page), page);
 		/* increment count (starts at -1) */
 		atomic_set(compound_mapcount_ptr(page), 0);
@@ -1178,21 +1229,30 @@ void page_add_file_rmap(struct page *page, bool compound)
 	VM_BUG_ON_PAGE(compound && !PageTransHuge(page), page);
 	lock_page_memcg(page);
 	if (compound && PageTransHuge(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for (i = 0, nr = 0; i < HPAGE_PMD_NR; i++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (atomic_inc_and_test(&page[i]._mapcount))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nr++;
+}
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!atomic_inc_and_test(compound_mapcount_ptr(page)))
 			goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(!PageSwapBacked(page), page);
 		__inc_node_page_state(page, NR_SHMEM_PMDMAPPED);
 	} else {
 		if (PageTransCompound(page) && page_mapping(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			VM_WARN_ON_ONCE(!PageLocked(page));
 
 			SetPageDoubleMap(compound_head(page));
 			if (PageMlocked(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				clear_page_mlock(compound_head(page));
+}
 		}
 		if (!atomic_inc_and_test(&page->_mapcount))
 			goto out;
@@ -1218,12 +1278,18 @@ static void page_remove_file_rmap(struct page *page, bool compound)
 
 	/* page still mapped by someone else? */
 	if (compound && PageTransHuge(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for (i = 0, nr = 0; i < HPAGE_PMD_NR; i++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (atomic_add_negative(-1, &page[i]._mapcount))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nr++;
+}
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!atomic_add_negative(-1, compound_mapcount_ptr(page)))
 			goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON_PAGE(!PageSwapBacked(page), page);
 		__dec_node_page_state(page, NR_SHMEM_PMDMAPPED);
 	} else {
@@ -1239,7 +1305,9 @@ static void page_remove_file_rmap(struct page *page, bool compound)
 	__mod_lruvec_page_state(page, NR_FILE_MAPPED, -nr);
 
 	if (unlikely(PageMlocked(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		clear_page_mlock(page);
+}
 out:
 	unlock_page_memcg(page);
 }
@@ -1249,14 +1317,21 @@ static void page_remove_anon_compound_rmap(struct page *page)
 	int i, nr;
 
 	if (!atomic_add_negative(-1, compound_mapcount_ptr(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/* Hugepages are not counted in NR_ANON_PAGES for now. */
 	if (unlikely(PageHuge(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!IS_ENABLED(CONFIG_TRANSPARENT_HUGEPAGE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	__dec_node_page_state(page, NR_ANON_THPS);
 
@@ -1266,15 +1341,21 @@ static void page_remove_anon_compound_rmap(struct page *page)
 		 * themi are still mapped.
 		 */
 		for (i = 0, nr = 0; i < HPAGE_PMD_NR; i++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (atomic_add_negative(-1, &page[i]._mapcount))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nr++;
+}
 		}
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr = HPAGE_PMD_NR;
 	}
 
 	if (unlikely(PageMlocked(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		clear_page_mlock(page);
+}
 
 	if (nr) {
 		__mod_node_page_state(page_pgdat(page), NR_ANON_MAPPED, -nr);
@@ -1299,7 +1380,9 @@ void page_remove_rmap(struct page *page, bool compound)
 
 	/* page still mapped by someone else? */
 	if (!atomic_add_negative(-1, &page->_mapcount))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * We use the irq-unsafe __{inc|mod}_zone_page_stat because
@@ -1309,7 +1392,9 @@ void page_remove_rmap(struct page *page, bool compound)
 	__dec_node_page_state(page, NR_ANON_MAPPED);
 
 	if (unlikely(PageMlocked(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		clear_page_mlock(page);
+}
 
 	if (PageTransCompound(page))
 		deferred_split_huge_page(compound_head(page));
@@ -1568,7 +1653,9 @@ bool is_vma_temporary_stack(struct vm_area_struct *vma)
 	int maybe_stack = vma->vm_flags & (VM_GROWSDOWN | VM_GROWSUP);
 
 	if (!maybe_stack)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if ((vma->vm_flags & VM_STACK_INCOMPLETE_SETUP) ==
 						VM_STACK_INCOMPLETE_SETUP)
@@ -1579,11 +1666,13 @@ bool is_vma_temporary_stack(struct vm_area_struct *vma)
 
 static bool invalid_migration_vma(struct vm_area_struct *vma, void *arg)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return is_vma_temporary_stack(vma);
 }
 
 static int page_mapcount_is_zero(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return !total_mapcount(page);
 }
 
@@ -1628,6 +1717,7 @@ bool try_to_unmap(struct page *page, enum ttu_flags flags)
 
 static int page_not_mapped(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return !page_mapped(page);
 };
 
@@ -1671,7 +1761,9 @@ static struct anon_vma *rmap_walk_anon_lock(struct page *page,
 	struct anon_vma *anon_vma;
 
 	if (rwc->anon_lock)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return rwc->anon_lock(page);
+}
 
 	/*
 	 * Note: remove_migration_ptes() cannot use page_lock_anon_vma_read()
@@ -1709,6 +1801,7 @@ static void rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc,
 	struct anon_vma_chain *avc;
 
 	if (locked) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		anon_vma = page_anon_vma(page);
 		/* anon_vma disappear under us? */
 		VM_BUG_ON_PAGE(!anon_vma, page);
@@ -1756,6 +1849,7 @@ static void rmap_walk_anon(struct page *page, struct rmap_walk_control *rwc,
 static void rmap_walk_file(struct page *page, struct rmap_walk_control *rwc,
 		bool locked)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct address_space *mapping = page_mapping(page);
 	pgoff_t pgoff_start, pgoff_end;
 	struct vm_area_struct *vma;
@@ -1797,6 +1891,7 @@ static void rmap_walk_file(struct page *page, struct rmap_walk_control *rwc,
 
 void rmap_walk(struct page *page, struct rmap_walk_control *rwc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(PageKsm(page)))
 		rmap_walk_ksm(page, rwc);
 	else if (PageAnon(page))
@@ -1827,6 +1922,7 @@ static void __hugepage_set_anon_rmap(struct page *page,
 {
 	struct anon_vma *anon_vma = vma->anon_vma;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!anon_vma);
 
 	if (PageAnon(page))
@@ -1845,6 +1941,7 @@ void hugepage_add_anon_rmap(struct page *page,
 	struct anon_vma *anon_vma = vma->anon_vma;
 	int first;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!PageLocked(page));
 	BUG_ON(!anon_vma);
 	/* address might be in next vma when migration races vma_adjust */
@@ -1856,6 +1953,7 @@ void hugepage_add_anon_rmap(struct page *page,
 void hugepage_add_new_anon_rmap(struct page *page,
 			struct vm_area_struct *vma, unsigned long address)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(address < vma->vm_start || address >= vma->vm_end);
 	atomic_set(compound_mapcount_ptr(page), 0);
 	__hugepage_set_anon_rmap(page, vma, address, 1);
diff --git a/mm/shmem.c b/mm/shmem.c
index 07a1d22..a4aeaa3 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Resizable virtual memory filesystem for Linux.
  *
@@ -134,6 +136,7 @@ int shmem_getpage(struct inode *inode, pgoff_t index,
 
 static inline struct shmem_sb_info *SHMEM_SB(struct super_block *sb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sb->s_fs_info;
 }
 
@@ -159,12 +162,18 @@ static inline int shmem_reacct_size(unsigned long flags,
 		loff_t oldsize, loff_t newsize)
 {
 	if (!(flags & VM_NORESERVE)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (VM_ACCT(newsize) > VM_ACCT(oldsize))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return security_vm_enough_memory_mm(current->mm,
 					VM_ACCT(newsize) - VM_ACCT(oldsize));
+}
 		else if (VM_ACCT(newsize) < VM_ACCT(oldsize))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vm_unacct_memory(VM_ACCT(oldsize) - VM_ACCT(newsize));
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -177,7 +186,9 @@ static inline int shmem_reacct_size(unsigned long flags,
 static inline int shmem_acct_block(unsigned long flags, long pages)
 {
 	if (!(flags & VM_NORESERVE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	return security_vm_enough_memory_mm(current->mm,
 			pages * VM_ACCT(PAGE_SIZE));
@@ -191,11 +202,14 @@ static inline void shmem_unacct_blocks(unsigned long flags, long pages)
 
 static inline bool shmem_inode_acct_block(struct inode *inode, long pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
 
 	if (shmem_acct_block(info->flags, pages))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (sbinfo->max_blocks) {
 		if (percpu_counter_compare(&sbinfo->used_blocks,
@@ -204,6 +218,7 @@ static inline bool shmem_inode_acct_block(struct inode *inode, long pages)
 		percpu_counter_add(&sbinfo->used_blocks, pages);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 
 unacct:
@@ -213,6 +228,7 @@ static inline bool shmem_inode_acct_block(struct inode *inode, long pages)
 
 static inline void shmem_inode_unacct_blocks(struct inode *inode, long pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
 
@@ -232,6 +248,7 @@ static struct file_system_type shmem_fs_type;
 
 bool vma_is_shmem(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return vma->vm_ops == &shmem_vm_ops;
 }
 
@@ -240,23 +257,29 @@ static DEFINE_MUTEX(shmem_swaplist_mutex);
 
 static int shmem_reserve_inode(struct super_block *sb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
 	if (sbinfo->max_inodes) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock(&sbinfo->stat_lock);
 		if (!sbinfo->free_inodes) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(&sbinfo->stat_lock);
 			return -ENOSPC;
 		}
 		sbinfo->free_inodes--;
 		spin_unlock(&sbinfo->stat_lock);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
 static void shmem_free_inode(struct super_block *sb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
 	if (sbinfo->max_inodes) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock(&sbinfo->stat_lock);
 		sbinfo->free_inodes++;
 		spin_unlock(&sbinfo->stat_lock);
@@ -277,6 +300,7 @@ static void shmem_free_inode(struct super_block *sb)
  */
 static void shmem_recalc_inode(struct inode *inode)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	long freed;
 
@@ -290,6 +314,7 @@ static void shmem_recalc_inode(struct inode *inode)
 
 bool shmem_charge(struct inode *inode, long pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	unsigned long flags;
 
@@ -308,6 +333,7 @@ bool shmem_charge(struct inode *inode, long pages)
 
 void shmem_uncharge(struct inode *inode, long pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	unsigned long flags;
 
@@ -334,7 +360,9 @@ static int shmem_radix_tree_replace(struct address_space *mapping,
 	VM_BUG_ON(!replacement);
 	item = __radix_tree_lookup(&mapping->page_tree, index, &node, &pslot);
 	if (!item)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOENT;
+}
 	if (item != expected)
 		return -ENOENT;
 	__radix_tree_replace(&mapping->page_tree, node, pslot,
@@ -558,6 +586,7 @@ static long shmem_unused_huge_count(struct super_block *sb,
 static unsigned long shmem_unused_huge_shrink(struct shmem_sb_info *sbinfo,
 		struct shrink_control *sc, unsigned long nr_to_split)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 #endif /* CONFIG_TRANSPARENT_HUGE_PAGECACHE */
@@ -591,20 +620,26 @@ static int shmem_add_to_page_cache(struct page *page,
 		if (radix_tree_gang_lookup_slot(&mapping->page_tree,
 					&results, &idx, index, 1) &&
 				idx < index + HPAGE_PMD_NR) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EEXIST;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			for (i = 0; i < HPAGE_PMD_NR; i++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = radix_tree_insert(&mapping->page_tree,
 						index + i, page + i);
 				VM_BUG_ON(error);
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			count_vm_event(THP_FILE_ALLOC);
 		}
 	} else if (!expected) {
 		error = radix_tree_insert(&mapping->page_tree, index, page);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = shmem_radix_tree_replace(mapping, index, expected,
 								 page);
 	}
@@ -612,11 +647,14 @@ static int shmem_add_to_page_cache(struct page *page,
 	if (!error) {
 		mapping->nrpages += nr;
 		if (PageTransHuge(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			__inc_node_page_state(page, NR_SHMEM_THPS);
+}
 		__mod_node_page_state(page_pgdat(page), NR_FILE_PAGES, nr);
 		__mod_node_page_state(page_pgdat(page), NR_SHMEM, nr);
 		spin_unlock_irq(&mapping->tree_lock);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page->mapping = NULL;
 		spin_unlock_irq(&mapping->tree_lock);
 		page_ref_sub(page, nr);
@@ -642,6 +680,7 @@ static void shmem_delete_from_page_cache(struct page *page, void *radswap)
 	__dec_node_page_state(page, NR_SHMEM);
 	spin_unlock_irq(&mapping->tree_lock);
 	put_page(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(error);
 }
 
@@ -657,7 +696,9 @@ static int shmem_free_swap(struct address_space *mapping,
 	old = radix_tree_delete_item(&mapping->page_tree, index, radswap);
 	spin_unlock_irq(&mapping->tree_lock);
 	if (old != radswap)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOENT;
+}
 	free_swap_and_cache(radix_to_swp_entry(radswap));
 	return 0;
 }
@@ -679,6 +720,7 @@ unsigned long shmem_partial_swap_usage(struct address_space *mapping,
 
 	rcu_read_lock();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
 		if (iter.index >= end)
 			break;
@@ -713,6 +755,7 @@ unsigned long shmem_partial_swap_usage(struct address_space *mapping,
  */
 unsigned long shmem_swap_usage(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = file_inode(vma->vm_file);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct address_space *mapping = inode->i_mapping;
@@ -788,8 +831,11 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 	int i;
 
 	if (lend == -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		end = -1;	/* unsigned, so actually very big */
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pagevec_init(&pvec, 0);
 	index = start;
 	while (index < end) {
@@ -806,13 +852,16 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 				break;
 
 			if (radix_tree_exceptional_entry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (unfalloc)
 					continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nr_swaps_freed += !shmem_free_swap(mapping,
 								index, page);
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			VM_BUG_ON_PAGE(page_to_pgoff(page) != index, page);
 
 			if (!trylock_page(page))
@@ -824,6 +873,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 				unlock_page(page);
 				continue;
 			} else if (PageTransHuge(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (index == round_down(end, HPAGE_PMD_NR)) {
 					/*
 					 * Range ends in the middle of THP:
@@ -833,13 +883,17 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 					unlock_page(page);
 					continue;
 				}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				index += HPAGE_PMD_NR - 1;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				i += HPAGE_PMD_NR - 1;
 			}
 
 			if (!unfalloc || !PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				VM_BUG_ON_PAGE(PageTail(page), page);
 				if (page_mapping(page) == mapping) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					VM_BUG_ON_PAGE(PageWriteback(page), page);
 					truncate_inode_page(mapping, page);
 				}
@@ -858,6 +912,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 		if (page) {
 			unsigned int top = PAGE_SIZE;
 			if (start > end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				top = partial_end;
 				partial_end = 0;
 			}
@@ -871,6 +926,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 		struct page *page = NULL;
 		shmem_getpage(inode, end, &page, SGP_READ);
 		if (page) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zero_user_segment(page, 0, partial_end);
 			set_page_dirty(page);
 			unlock_page(page);
@@ -878,8 +934,11 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 		}
 	}
 	if (start >= end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	index = start;
 	while (index < end) {
 		cond_resched();
@@ -902,18 +961,23 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 			if (index >= end)
 				break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (radix_tree_exceptional_entry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (unfalloc)
 					continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (shmem_free_swap(mapping, index, page)) {
 					/* Swap was replaced by page: retry */
 					index--;
 					break;
 				}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				nr_swaps_freed++;
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			lock_page(page);
 
 			if (PageTransTail(page)) {
@@ -926,9 +990,12 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 				 * again on !pvec.nr restart.
 				 */
 				if (index != round_down(end, HPAGE_PMD_NR))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					start++;
+}
 				continue;
 			} else if (PageTransHuge(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (index == round_down(end, HPAGE_PMD_NR)) {
 					/*
 					 * Range ends in the middle of THP:
@@ -938,13 +1005,18 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 					unlock_page(page);
 					continue;
 				}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				index += HPAGE_PMD_NR - 1;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				i += HPAGE_PMD_NR - 1;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!unfalloc || !PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				VM_BUG_ON_PAGE(PageTail(page), page);
 				if (page_mapping(page) == mapping) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					VM_BUG_ON_PAGE(PageWriteback(page), page);
 					truncate_inode_page(mapping, page);
 				} else {
@@ -954,6 +1026,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 					break;
 				}
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			unlock_page(page);
 		}
 		pagevec_remove_exceptionals(&pvec);
@@ -961,6 +1034,7 @@ static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,
 		index++;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irq(&info->lock);
 	info->swapped -= nr_swaps_freed;
 	shmem_recalc_inode(inode);
@@ -981,6 +1055,7 @@ static int shmem_getattr(const struct path *path, struct kstat *stat,
 	struct shmem_inode_info *info = SHMEM_I(inode);
 
 	if (info->alloced - info->swapped != inode->i_mapping->nrpages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irq(&info->lock);
 		shmem_recalc_inode(inode);
 		spin_unlock_irq(&info->lock);
@@ -991,6 +1066,7 @@ static int shmem_getattr(const struct path *path, struct kstat *stat,
 
 static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = d_inode(dentry);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
@@ -998,7 +1074,9 @@ static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 
 	error = setattr_prepare(dentry, attr);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
 	if (S_ISREG(inode->i_mode) && (attr->ia_valid & ATTR_SIZE)) {
 		loff_t oldsize = inode->i_size;
@@ -1013,7 +1091,10 @@ static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 			error = shmem_reacct_size(SHMEM_I(inode)->flags,
 					oldsize, newsize);
 			if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return error;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			i_size_write(inode, newsize);
 			inode->i_ctime = inode->i_mtime = current_time(inode);
 		}
@@ -1035,6 +1116,7 @@ static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 			 * to shrink under memory pressure.
 			 */
 			if (IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_lock(&sbinfo->shrinklist_lock);
 				/*
 				 * _careful to defend against unlocked access to
@@ -1045,6 +1127,7 @@ static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 							&sbinfo->shrinklist);
 					sbinfo->shrinklist_len++;
 				}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_unlock(&sbinfo->shrinklist_lock);
 			}
 		}
@@ -1053,11 +1136,13 @@ static int shmem_setattr(struct dentry *dentry, struct iattr *attr)
 	setattr_copy(inode, attr);
 	if (attr->ia_valid & ATTR_MODE)
 		error = posix_acl_chmod(inode, inode->i_mode);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return error;
 }
 
 static void shmem_evict_inode(struct inode *inode)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
 
@@ -1066,14 +1151,17 @@ static void shmem_evict_inode(struct inode *inode)
 		inode->i_size = 0;
 		shmem_truncate_range(inode, 0, (loff_t)-1);
 		if (!list_empty(&info->shrinklist)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_lock(&sbinfo->shrinklist_lock);
 			if (!list_empty(&info->shrinklist)) {
 				list_del_init(&info->shrinklist);
 				sbinfo->shrinklist_len--;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(&sbinfo->shrinklist_lock);
 		}
 		if (!list_empty(&info->swaplist)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mutex_lock(&shmem_swaplist_mutex);
 			list_del_init(&info->swaplist);
 			mutex_unlock(&shmem_swaplist_mutex);
@@ -1094,6 +1182,7 @@ static unsigned long find_swap_entry(struct radix_tree_root *root, void *item)
 	unsigned int checked = 0;
 
 	rcu_read_lock();
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	radix_tree_for_each_slot(slot, root, &iter, 0) {
 		if (*slot == item) {
 			found = iter.index;
@@ -1125,7 +1214,9 @@ static int shmem_unuse_inode(struct shmem_inode_info *info,
 	radswap = swp_to_radix_entry(swap);
 	index = find_swap_entry(&mapping->page_tree, radswap);
 	if (index == -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EAGAIN;	/* tell shmem_unuse we found nothing */
+}
 
 	/*
 	 * Move _head_ to start search for next from here.
@@ -1255,6 +1346,7 @@ static int shmem_writepage(struct page *page, struct writeback_control *wbc)
 	pgoff_t index;
 
 	VM_BUG_ON_PAGE(PageCompound(page), page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!PageLocked(page));
 	mapping = page->mapping;
 	index = page->index;
@@ -1359,6 +1451,7 @@ static void shmem_show_mpol(struct seq_file *seq, struct mempolicy *mpol)
 {
 	char buffer[64];
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!mpol || mpol->mode == MPOL_DEFAULT)
 		return;		/* show nothing */
 
@@ -1371,6 +1464,7 @@ static struct mempolicy *shmem_get_sbmpol(struct shmem_sb_info *sbinfo)
 {
 	struct mempolicy *mpol = NULL;
 	if (sbinfo->mpol) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock(&sbinfo->stat_lock);	/* prevent replace/use races */
 		mpol = sbinfo->mpol;
 		mpol_get(mpol);
@@ -1432,7 +1526,9 @@ static struct page *shmem_alloc_hugepage(gfp_t gfp,
 	struct page *page;
 
 	if (!IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	hindex = round_down(index, HPAGE_PMD_NR);
 	rcu_read_lock();
@@ -1469,28 +1565,35 @@ static struct page *shmem_alloc_and_acct_page(gfp_t gfp,
 		struct inode *inode,
 		pgoff_t index, bool huge)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct page *page;
 	int nr;
 	int err = -ENOSPC;
 
 	if (!IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		huge = false;
+}
 	nr = huge ? HPAGE_PMD_NR : 1;
 
 	if (!shmem_inode_acct_block(inode, nr))
 		goto failed;
 
 	if (huge)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = shmem_alloc_hugepage(gfp, info, index);
+}
 	else
 		page = shmem_alloc_page(gfp, info, index);
 	if (page) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__SetPageLocked(page);
 		__SetPageSwapBacked(page);
 		return page;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	err = -ENOMEM;
 	shmem_inode_unacct_blocks(inode, nr);
 failed:
@@ -1511,6 +1614,7 @@ static struct page *shmem_alloc_and_acct_page(gfp_t gfp,
  */
 static bool shmem_should_replace_page(struct page *page, gfp_t gfp)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return page_zonenum(page) > gfp_zone(gfp);
 }
 
@@ -1533,7 +1637,9 @@ static int shmem_replace_page(struct page **pagep, gfp_t gfp,
 	gfp &= ~GFP_CONSTRAINT_MASK;
 	newpage = shmem_alloc_page(gfp, info, index);
 	if (!newpage)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	get_page(newpage);
 	copy_highpage(newpage, oldpage);
@@ -1608,19 +1714,25 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 	int alloced = 0;
 
 	if (index > (MAX_LFS_FILESIZE >> PAGE_SHIFT))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EFBIG;
+}
 	if (sgp == SGP_NOHUGE || sgp == SGP_HUGE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sgp = SGP_CACHE;
+}
 repeat:
 	swap.val = 0;
 	page = find_lock_entry(mapping, index);
 	if (radix_tree_exceptional_entry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		swap = radix_to_swp_entry(page);
 		page = NULL;
 	}
 
 	if (sgp <= SGP_CACHE &&
 	    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = -EINVAL;
 		goto unlock;
 	}
@@ -1632,6 +1744,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 	if (page && !PageUptodate(page)) {
 		if (sgp != SGP_READ)
 			goto clear;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(page);
 		put_page(page);
 		page = NULL;
@@ -1654,6 +1767,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 		if (!page) {
 			/* Or update major stats only when swapin succeeds?? */
 			if (fault_type) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				*fault_type |= VM_FAULT_MAJOR;
 				count_vm_event(PGMAJFAULT);
 				count_memcg_event_mm(charge_mm, PGMAJFAULT);
@@ -1661,6 +1775,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 			/* Here we actually start the io */
 			page = shmem_swapin(swap, gfp, info, index);
 			if (!page) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = -ENOMEM;
 				goto failed;
 			}
@@ -1668,23 +1783,30 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 
 		/* We have to do this with page locked to prevent races */
 		lock_page(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!PageSwapCache(page) || page_private(page) != swap.val ||
 		    !shmem_confirm_swap(mapping, index, swap)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EEXIST;	/* try again */
 			goto unlock;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!PageUptodate(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EIO;
 			goto failed;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		wait_on_page_writeback(page);
 
 		if (shmem_should_replace_page(page, gfp)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = shmem_replace_page(&page, gfp, info, index);
 			if (error)
 				goto failed;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = mem_cgroup_try_charge(page, charge_mm, gfp, &memcg,
 				false);
 		if (!error) {
@@ -1703,13 +1825,16 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 			 * "repeat": reading a hole and writing should succeed.
 			 */
 			if (error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				mem_cgroup_cancel_charge(page, memcg, false);
 				delete_from_swap_cache(page);
 			}
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (error)
 			goto failed;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mem_cgroup_commit_charge(page, memcg, true, false);
 
 		spin_lock_irq(&info->lock);
@@ -1718,14 +1843,19 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 		spin_unlock_irq(&info->lock);
 
 		if (sgp == SGP_WRITE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mark_page_accessed(page);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		delete_from_swap_cache(page);
 		set_page_dirty(page);
 		swap_free(swap);
 
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (vma && userfaultfd_missing(vma)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
 			return 0;
 		}
@@ -1745,6 +1875,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 		case SHMEM_HUGE_WITHIN_SIZE:
 			off = round_up(index, HPAGE_PMD_NR);
 			i_size = round_up(i_size_read(inode), PAGE_SIZE);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (i_size >= HPAGE_PMD_SIZE &&
 					i_size >> PAGE_SHIFT >= off)
 				goto alloc_huge;
@@ -1777,6 +1908,7 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 				ret = shmem_unused_huge_shrink(sbinfo, NULL, 1);
 				if (ret == SHRINK_STOP)
 					break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (ret)
 					goto alloc_nohuge;
 			}
@@ -1784,7 +1916,9 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 		}
 
 		if (PageTransHuge(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			hindex = round_down(index, HPAGE_PMD_NR);
+}
 		else
 			hindex = index;
 
@@ -1831,10 +1965,12 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 			 * ->shrink_list in shmem_unused_huge_shrink()
 			 */
 			if (list_empty_careful(&info->shrinklist)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				list_add_tail(&info->shrinklist,
 						&sbinfo->shrinklist);
 				sbinfo->shrinklist_len++;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(&sbinfo->shrinklist_lock);
 		}
 
@@ -1842,7 +1978,9 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 		 * Let SGP_FALLOC use the SGP_WRITE optimization on a new page.
 		 */
 		if (sgp == SGP_FALLOC)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			sgp = SGP_WRITE;
+}
 clear:
 		/*
 		 * Let SGP_WRITE caller clear ends if write does not fill page;
@@ -1855,8 +1993,10 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 
 			for (i = 0; i < (1 << compound_order(head)); i++) {
 				clear_highpage(head + i);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				flush_dcache_page(head + i);
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			SetPageUptodate(head);
 		}
 	}
@@ -1864,13 +2004,16 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 	/* Perhaps the file has been truncated since we checked */
 	if (sgp <= SGP_CACHE &&
 	    ((loff_t)index << PAGE_SHIFT) >= i_size_read(inode)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (alloced) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ClearPageDirty(page);
 			delete_from_page_cache(page);
 			spin_lock_irq(&info->lock);
 			shmem_recalc_inode(inode);
 			spin_unlock_irq(&info->lock);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = -EINVAL;
 		goto unlock;
 	}
@@ -1884,26 +2027,34 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
 	shmem_inode_unacct_blocks(inode, 1 << compound_order(page));
 
 	if (PageTransHuge(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(page);
 		put_page(page);
 		goto alloc_nohuge;
 	}
 failed:
 	if (swap.val && !shmem_confirm_swap(mapping, index, swap))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = -EEXIST;
+}
 unlock:
 	if (page) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unlock_page(page);
 		put_page(page);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (error == -ENOSPC && !once++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irq(&info->lock);
 		shmem_recalc_inode(inode);
 		spin_unlock_irq(&info->lock);
 		goto repeat;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (error == -EEXIST)	/* from above or from radix_tree_insert */
 		goto repeat;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return error;
 }
 
@@ -1914,6 +2065,7 @@ alloc_nohuge:		page = shmem_alloc_and_acct_page(gfp, inode,
  */
 static int synchronous_wake_function(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int ret = default_wake_function(wait, mode, sync, key);
 	list_del_init(&wait->entry);
 	return ret;
@@ -1965,6 +2117,7 @@ static int shmem_fault(struct vm_fault *vmf)
 				ret = VM_FAULT_RETRY;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			shmem_falloc_waitq = shmem_falloc->waitq;
 			prepare_to_wait(shmem_falloc_waitq, &shmem_fault_wait,
 					TASK_UNINTERRUPTIBLE);
@@ -1983,21 +2136,27 @@ static int shmem_fault(struct vm_fault *vmf)
 			spin_unlock(&inode->i_lock);
 			return ret;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(&inode->i_lock);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	sgp = SGP_CACHE;
 
 	if ((vma->vm_flags & VM_NOHUGEPAGE) ||
 	    test_bit(MMF_DISABLE_THP, &vma->vm_mm->flags))
 		sgp = SGP_NOHUGE;
 	else if (vma->vm_flags & VM_HUGEPAGE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sgp = SGP_HUGE;
+}
 
 	error = shmem_getpage_gfp(inode, vmf->pgoff, &vmf->page, sgp,
 				  gfp, vma, vmf, &ret);
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ((error == -ENOMEM) ? VM_FAULT_OOM : VM_FAULT_SIGBUS);
+}
 	return ret;
 }
 
@@ -2014,38 +2173,57 @@ unsigned long shmem_get_unmapped_area(struct file *file,
 	unsigned long inflated_offset;
 
 	if (len > TASK_SIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	get_area = current->mm->get_unmapped_area;
 	addr = get_area(file, uaddr, len, pgoff, flags);
 
 	if (!IS_ENABLED(CONFIG_TRANSPARENT_HUGE_PAGECACHE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 	if (IS_ERR_VALUE(addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 	if (addr & ~PAGE_MASK)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 	if (addr > TASK_SIZE - len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 
 	if (shmem_huge == SHMEM_HUGE_DENY)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 	if (len < HPAGE_PMD_SIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 	if (flags & MAP_FIXED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 	/*
 	 * Our priority is to support MAP_SHARED mapped hugely;
 	 * and support MAP_PRIVATE mapped hugely too, until it is COWed.
 	 * But if caller specified an address hint, respect that as before.
 	 */
 	if (uaddr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 
 	if (shmem_huge != SHMEM_HUGE_FORCE) {
 		struct super_block *sb;
 
 		if (file) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			VM_BUG_ON(file->f_op != &shmem_file_operations);
 			sb = file_inode(file)->i_sb;
 		} else {
@@ -2054,44 +2232,76 @@ unsigned long shmem_get_unmapped_area(struct file *file,
 			 * for "/dev/zero", to create a shared anonymous object.
 			 */
 			if (IS_ERR(shm_mnt))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return addr;
+}
 			sb = shm_mnt->mnt_sb;
 		}
 		if (SHMEM_SB(sb)->huge == SHMEM_HUGE_NEVER)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return addr;
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	offset = (pgoff << PAGE_SHIFT) & (HPAGE_PMD_SIZE-1);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (offset && offset + len < 2 * HPAGE_PMD_SIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((addr & (HPAGE_PMD_SIZE-1)) == offset)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inflated_len = len + HPAGE_PMD_SIZE - PAGE_SIZE;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (inflated_len > TASK_SIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (inflated_len < len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inflated_addr = get_area(NULL, 0, inflated_len, 0, flags);
 	if (IS_ERR_VALUE(inflated_addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (inflated_addr & ~PAGE_MASK)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inflated_offset = inflated_addr & (HPAGE_PMD_SIZE-1);
 	inflated_addr += offset - inflated_offset;
 	if (inflated_offset > offset)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		inflated_addr += HPAGE_PMD_SIZE;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (inflated_addr > TASK_SIZE - len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return addr;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return inflated_addr;
 }
 
 #ifdef CONFIG_NUMA
 static int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = file_inode(vma->vm_file);
 	return mpol_set_shared_policy(&SHMEM_I(inode)->policy, vma, mpol);
 }
@@ -2099,6 +2309,7 @@ static int shmem_set_policy(struct vm_area_struct *vma, struct mempolicy *mpol)
 static struct mempolicy *shmem_get_policy(struct vm_area_struct *vma,
 					  unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = file_inode(vma->vm_file);
 	pgoff_t index;
 
@@ -2109,6 +2320,7 @@ static struct mempolicy *shmem_get_policy(struct vm_area_struct *vma,
 
 int shmem_lock(struct file *file, int lock, struct user_struct *user)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = file_inode(file);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	int retval = -ENOMEM;
@@ -2152,7 +2364,9 @@ static struct inode *shmem_get_inode(struct super_block *sb, const struct inode
 	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
 
 	if (shmem_reserve_inode(sb))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	inode = new_inode(sb);
 	if (inode) {
@@ -2199,7 +2413,10 @@ static struct inode *shmem_get_inode(struct super_block *sb, const struct inode
 			break;
 		}
 	} else
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		shmem_free_inode(sb);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return inode;
 }
 
@@ -2216,6 +2433,7 @@ static int shmem_mfill_atomic_pte(struct mm_struct *dst_mm,
 				  bool zeropage,
 				  struct page **pagep)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = file_inode(dst_vma->vm_file);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	struct address_space *mapping = inode->i_mapping;
@@ -2325,6 +2543,7 @@ int shmem_mcopy_atomic_pte(struct mm_struct *dst_mm,
 			   unsigned long src_addr,
 			   struct page **pagep)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return shmem_mfill_atomic_pte(dst_mm, dst_pmd, dst_vma,
 				      dst_addr, src_addr, false, pagep);
 }
@@ -2361,10 +2580,16 @@ shmem_write_begin(struct file *file, struct address_space *mapping,
 
 	/* i_mutex is held by caller */
 	if (unlikely(info->seals & (F_SEAL_WRITE | F_SEAL_GROW))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (info->seals & F_SEAL_WRITE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EPERM;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if ((info->seals & F_SEAL_GROW) && pos + len > inode->i_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EPERM;
+}
 	}
 
 	return shmem_getpage(inode, index, pagep, SGP_WRITE);
@@ -2378,17 +2603,23 @@ shmem_write_end(struct file *file, struct address_space *mapping,
 	struct inode *inode = mapping->host;
 
 	if (pos + copied > inode->i_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		i_size_write(inode, pos + copied);
+}
 
 	if (!PageUptodate(page)) {
 		struct page *head = compound_head(page);
 		if (PageTransCompound(page)) {
 			int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			for (i = 0; i < HPAGE_PMD_NR; i++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (head + i == page)
 					continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				clear_highpage(head + i);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				flush_dcache_page(head + i);
 			}
 		}
@@ -2397,6 +2628,7 @@ shmem_write_end(struct file *file, struct address_space *mapping,
 			zero_user_segments(page, 0, from,
 					from + copied, PAGE_SIZE);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		SetPageUptodate(head);
 	}
 	set_page_dirty(page);
@@ -2424,7 +2656,9 @@ static ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 	 * and even mark them dirty, so it cannot exceed the max_blocks limit.
 	 */
 	if (!iter_is_iovec(to))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sgp = SGP_CACHE;
+}
 
 	index = *ppos >> PAGE_SHIFT;
 	offset = *ppos & ~PAGE_MASK;
@@ -2446,8 +2680,11 @@ static ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 
 		error = shmem_getpage(inode, index, &page, sgp);
 		if (error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (error == -EINVAL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				error = 0;
+}
 			break;
 		}
 		if (page) {
@@ -2466,8 +2703,11 @@ static ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 		if (index == end_index) {
 			nr = i_size & ~PAGE_MASK;
 			if (nr <= offset) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					put_page(page);
+}
 				break;
 			}
 		}
@@ -2480,13 +2720,16 @@ static ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 			 * before reading the page on the kernel side.
 			 */
 			if (mapping_writably_mapped(mapping))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				flush_dcache_page(page);
+}
 			/*
 			 * Mark the page accessed if we read the beginning.
 			 */
 			if (!offset)
 				mark_page_accessed(page);
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = ZERO_PAGE(0);
 			get_page(page);
 		}
@@ -2505,6 +2748,7 @@ static ssize_t shmem_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 		if (!iov_iter_count(to))
 			break;
 		if (ret < nr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EFAULT;
 			break;
 		}
@@ -2530,6 +2774,7 @@ static pgoff_t shmem_seek_hole_data(struct address_space *mapping,
 
 	pagevec_init(&pvec, 0);
 	pvec.nr = 1;		/* start small: we may be there already */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!done) {
 		pvec.nr = find_get_entries(mapping, index,
 					pvec.nr, pvec.pages, indices);
@@ -2576,30 +2821,47 @@ static loff_t shmem_file_llseek(struct file *file, loff_t offset, int whence)
 	if (whence != SEEK_DATA && whence != SEEK_HOLE)
 		return generic_file_llseek_size(file, offset, whence,
 					MAX_LFS_FILESIZE, i_size_read(inode));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inode_lock(inode);
 	/* We're holding i_mutex so we can access i_size directly */
 
 	if (offset < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		offset = -EINVAL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	else if (offset >= inode->i_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		offset = -ENXIO;
+}
 	else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start = offset >> PAGE_SHIFT;
 		end = (inode->i_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
 		new_offset = shmem_seek_hole_data(mapping, start, end, whence);
 		new_offset <<= PAGE_SHIFT;
 		if (new_offset > offset) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (new_offset < inode->i_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				offset = new_offset;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			else if (whence == SEEK_DATA)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				offset = -ENXIO;
+}
 			else
 				offset = inode->i_size;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (offset >= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		offset = vfs_setpos(file, offset, MAX_LFS_FILESIZE);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inode_unlock(inode);
 	return offset;
 }
@@ -2622,6 +2884,7 @@ static void shmem_tag_pins(struct address_space *mapping)
 	start = 0;
 	rcu_read_lock();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	radix_tree_for_each_slot(slot, &mapping->page_tree, &iter, start) {
 		page = radix_tree_deref_slot(slot);
 		if (!page || radix_tree_exception(page)) {
@@ -2664,6 +2927,7 @@ static int shmem_wait_for_pins(struct address_space *mapping)
 	shmem_tag_pins(mapping);
 
 	error = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (scan = 0; scan <= LAST_SCAN; scan++) {
 		if (!radix_tree_tagged(&mapping->page_tree, SHMEM_TAG_PINNED))
 			break;
@@ -2724,6 +2988,7 @@ static int shmem_wait_for_pins(struct address_space *mapping)
 
 int shmem_add_seals(struct file *file, unsigned int seals)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = file_inode(file);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	int error;
@@ -2795,6 +3060,7 @@ EXPORT_SYMBOL_GPL(shmem_add_seals);
 
 int shmem_get_seals(struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (file->f_op != &shmem_file_operations)
 		return -EINVAL;
 
@@ -2828,6 +3094,7 @@ long shmem_fcntl(struct file *file, unsigned int cmd, unsigned long arg)
 static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 							 loff_t len)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = file_inode(file);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(inode->i_sb);
 	struct shmem_inode_info *info = SHMEM_I(inode);
@@ -2836,8 +3103,11 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 	int error;
 
 	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EOPNOTSUPP;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	inode_lock(inode);
 
 	if (mode & FALLOC_FL_PUNCH_HOLE) {
@@ -2848,6 +3118,7 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 
 		/* protected by i_mutex */
 		if (info->seals & F_SEAL_WRITE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EPERM;
 			goto out;
 		}
@@ -2880,6 +3151,7 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 		goto out;
 
 	if ((info->seals & F_SEAL_GROW) && offset + len > inode->i_size) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = -EPERM;
 		goto out;
 	}
@@ -2888,6 +3160,7 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 	end = (offset + len + PAGE_SIZE - 1) >> PAGE_SHIFT;
 	/* Try to avoid a swapstorm if len is impossible to satisfy */
 	if (sbinfo->max_blocks && end - start > sbinfo->max_blocks) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = -ENOSPC;
 		goto out;
 	}
@@ -2909,14 +3182,19 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 		 * been interrupted because we are using up too much memory.
 		 */
 		if (signal_pending(current))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -EINTR;
+}
 		else if (shmem_falloc.nr_unswapped > shmem_falloc.nr_falloced)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			error = -ENOMEM;
+}
 		else
 			error = shmem_getpage(inode, index, &page, SGP_FALLOC);
 		if (error) {
 			/* Remove the !PageUptodate pages we added */
 			if (index > start) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				shmem_undo_range(inode,
 				    (loff_t)start << PAGE_SHIFT,
 				    ((loff_t)index << PAGE_SHIFT) - 1, true);
@@ -2946,7 +3224,9 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 	}
 
 	if (!(mode & FALLOC_FL_KEEP_SIZE) && offset + len > inode->i_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		i_size_write(inode, offset + len);
+}
 	inode->i_ctime = current_time(inode);
 undone:
 	spin_lock(&inode->i_lock);
@@ -2998,12 +3278,14 @@ shmem_mknod(struct inode *dir, struct dentry *dentry, umode_t mode, dev_t dev)
 		if (error && error != -EOPNOTSUPP)
 			goto out_iput;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = 0;
 		dir->i_size += BOGO_DIRENT_SIZE;
 		dir->i_ctime = dir->i_mtime = current_time(dir);
 		d_instantiate(dentry, inode);
 		dget(dentry); /* Extra count - pin the dentry in core */
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return error;
 out_iput:
 	iput(inode);
@@ -3028,6 +3310,7 @@ shmem_tmpfile(struct inode *dir, struct dentry *dentry, umode_t mode)
 			goto out_iput;
 		d_tmpfile(dentry, inode);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return error;
 out_iput:
 	iput(inode);
@@ -3039,7 +3322,9 @@ static int shmem_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)
 	int error;
 
 	if ((error = shmem_mknod(dir, dentry, mode | S_IFDIR, 0)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 	inc_nlink(dir);
 	return 0;
 }
@@ -3055,6 +3340,7 @@ static int shmem_create(struct inode *dir, struct dentry *dentry, umode_t mode,
  */
 static int shmem_link(struct dentry *old_dentry, struct inode *dir, struct dentry *dentry)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = d_inode(old_dentry);
 	int ret;
 
@@ -3079,6 +3365,7 @@ static int shmem_link(struct dentry *old_dentry, struct inode *dir, struct dentr
 
 static int shmem_unlink(struct inode *dir, struct dentry *dentry)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = d_inode(dentry);
 
 	if (inode->i_nlink > 1 && !S_ISDIR(inode->i_mode))
@@ -3094,7 +3381,9 @@ static int shmem_unlink(struct inode *dir, struct dentry *dentry)
 static int shmem_rmdir(struct inode *dir, struct dentry *dentry)
 {
 	if (!simple_empty(dentry))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOTEMPTY;
+}
 
 	drop_nlink(d_inode(dentry));
 	drop_nlink(dir);
@@ -3103,6 +3392,7 @@ static int shmem_rmdir(struct inode *dir, struct dentry *dentry)
 
 static int shmem_exchange(struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bool old_is_dir = d_is_dir(old_dentry);
 	bool new_is_dir = d_is_dir(new_dentry);
 
@@ -3130,7 +3420,9 @@ static int shmem_whiteout(struct inode *old_dir, struct dentry *old_dentry)
 
 	whiteout = d_alloc(old_dentry->d_parent, &old_dentry->d_name);
 	if (!whiteout)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	error = shmem_mknod(old_dir, whiteout,
 			    S_IFCHR | WHITEOUT_MODE, WHITEOUT_DEV);
@@ -3157,29 +3449,39 @@ static int shmem_whiteout(struct inode *old_dir, struct dentry *old_dentry)
  */
 static int shmem_rename2(struct inode *old_dir, struct dentry *old_dentry, struct inode *new_dir, struct dentry *new_dentry, unsigned int flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = d_inode(old_dentry);
 	int they_are_dirs = S_ISDIR(inode->i_mode);
 
 	if (flags & ~(RENAME_NOREPLACE | RENAME_EXCHANGE | RENAME_WHITEOUT))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (flags & RENAME_EXCHANGE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return shmem_exchange(old_dir, old_dentry, new_dir, new_dentry);
+}
 
 	if (!simple_empty(new_dentry))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOTEMPTY;
+}
 
 	if (flags & RENAME_WHITEOUT) {
 		int error;
 
 		error = shmem_whiteout(old_dir, old_dentry);
 		if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return error;
+}
 	}
 
 	if (d_really_is_positive(new_dentry)) {
 		(void) shmem_unlink(new_dir, new_dentry);
 		if (they_are_dirs) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			drop_nlink(d_inode(new_dentry));
 			drop_nlink(old_dir);
 		}
@@ -3206,40 +3508,53 @@ static int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *s
 
 	len = strlen(symname) + 1;
 	if (len > PAGE_SIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENAMETOOLONG;
+}
 
 	inode = shmem_get_inode(dir->i_sb, dir, S_IFLNK|S_IRWXUGO, 0, VM_NORESERVE);
 	if (!inode)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOSPC;
+}
 
 	error = security_inode_init_security(inode, dir, &dentry->d_name,
 					     shmem_initxattrs, NULL);
 	if (error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (error != -EOPNOTSUPP) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			iput(inode);
 			return error;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = 0;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info = SHMEM_I(inode);
 	inode->i_size = len-1;
 	if (len <= SHORT_SYMLINK_LEN) {
 		inode->i_link = kmemdup(symname, len, GFP_KERNEL);
 		if (!inode->i_link) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			iput(inode);
 			return -ENOMEM;
 		}
 		inode->i_op = &shmem_short_symlink_operations;
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		inode_nohighmem(inode);
 		error = shmem_getpage(inode, 0, &page, SGP_WRITE);
 		if (error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			iput(inode);
 			return error;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		inode->i_mapping->a_ops = &shmem_aops;
 		inode->i_op = &shmem_symlink_inode_operations;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		memcpy(page_address(page), symname, len);
 		SetPageUptodate(page);
 		set_page_dirty(page);
@@ -3255,6 +3570,7 @@ static int shmem_symlink(struct inode *dir, struct dentry *dentry, const char *s
 
 static void shmem_put_link(void *arg)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mark_page_accessed(arg);
 	put_page(arg);
 }
@@ -3266,6 +3582,7 @@ static const char *shmem_get_link(struct dentry *dentry,
 	struct page *page = NULL;
 	int error;
 	if (!dentry) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page = find_get_page(inode->i_mapping, 0);
 		if (!page)
 			return ERR_PTR(-ECHILD);
@@ -3298,6 +3615,7 @@ static int shmem_initxattrs(struct inode *inode,
 			    const struct xattr *xattr_array,
 			    void *fs_info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 	const struct xattr *xattr;
 	struct simple_xattr *new_xattr;
@@ -3331,6 +3649,7 @@ static int shmem_xattr_handler_get(const struct xattr_handler *handler,
 				   struct dentry *unused, struct inode *inode,
 				   const char *name, void *buffer, size_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 
 	name = xattr_full_name(handler, name);
@@ -3342,6 +3661,7 @@ static int shmem_xattr_handler_set(const struct xattr_handler *handler,
 				   const char *name, const void *value,
 				   size_t size, int flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(inode);
 
 	name = xattr_full_name(handler, name);
@@ -3372,6 +3692,7 @@ static const struct xattr_handler *shmem_xattr_handlers[] = {
 
 static ssize_t shmem_listxattr(struct dentry *dentry, char *buffer, size_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_inode_info *info = SHMEM_I(d_inode(dentry));
 	return simple_xattr_list(d_inode(dentry), &info->xattrs, buffer, size);
 }
@@ -3393,6 +3714,7 @@ static const struct inode_operations shmem_symlink_inode_operations = {
 
 static struct dentry *shmem_get_parent(struct dentry *child)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ERR_PTR(-ESTALE);
 }
 
@@ -3412,7 +3734,9 @@ static struct dentry *shmem_fh_to_dentry(struct super_block *sb,
 	u64 inum;
 
 	if (fh_len < 3)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	inum = fid->raw[2];
 	inum = (inum << 32) | fid->raw[1];
@@ -3424,6 +3748,7 @@ static struct dentry *shmem_fh_to_dentry(struct super_block *sb,
 		iput(inode);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return dentry;
 }
 
@@ -3431,6 +3756,7 @@ static int shmem_encode_fh(struct inode *inode, __u32 *fh, int *len,
 				struct inode *parent)
 {
 	if (*len < 3) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*len = 3;
 		return FILEID_INVALID;
 	}
@@ -3446,6 +3772,7 @@ static int shmem_encode_fh(struct inode *inode, __u32 *fh, int *len,
 		if (inode_unhashed(inode))
 			__insert_inode_hash(inode,
 					    inode->i_ino + inode->i_generation);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(&lock);
 	}
 
@@ -3472,6 +3799,7 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 	gid_t gid;
 
 	while (options != NULL) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		this_char = options;
 		for (;;) {
 			/*
@@ -3493,6 +3821,7 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 		if ((value = strchr(this_char,'=')) != NULL) {
 			*value++ = 0;
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_err("tmpfs: No value for mount option '%s'\n",
 			       this_char);
 			goto error;
@@ -3512,6 +3841,7 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 			sbinfo->max_blocks =
 				DIV_ROUND_UP(size, PAGE_SIZE);
 		} else if (!strcmp(this_char,"nr_blocks")) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			sbinfo->max_blocks = memparse(value, &rest);
 			if (*rest)
 				goto bad_val;
@@ -3526,20 +3856,26 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 			if (*rest)
 				goto bad_val;
 		} else if (!strcmp(this_char,"uid")) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (remount)
 				continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			uid = simple_strtoul(value, &rest, 0);
 			if (*rest)
 				goto bad_val;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			sbinfo->uid = make_kuid(current_user_ns(), uid);
 			if (!uid_valid(sbinfo->uid))
 				goto bad_val;
 		} else if (!strcmp(this_char,"gid")) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (remount)
 				continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			gid = simple_strtoul(value, &rest, 0);
 			if (*rest)
 				goto bad_val;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			sbinfo->gid = make_kgid(current_user_ns(), gid);
 			if (!gid_valid(sbinfo->gid))
 				goto bad_val;
@@ -3556,12 +3892,14 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 #endif
 #ifdef CONFIG_NUMA
 		} else if (!strcmp(this_char,"mpol")) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mpol_put(mpol);
 			mpol = NULL;
 			if (mpol_parse_str(value, &mpol))
 				goto bad_val;
 #endif
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_err("tmpfs: Bad mount option %s\n", this_char);
 			goto error;
 		}
@@ -3580,6 +3918,7 @@ static int shmem_parse_options(char *options, struct shmem_sb_info *sbinfo,
 
 static int shmem_remount_fs(struct super_block *sb, int *flags, char *data)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
 	struct shmem_sb_info config = *sbinfo;
 	unsigned long inodes;
@@ -3587,8 +3926,11 @@ static int shmem_remount_fs(struct super_block *sb, int *flags, char *data)
 
 	config.mpol = NULL;
 	if (shmem_parse_options(data, &config, true))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&sbinfo->stat_lock);
 	inodes = sbinfo->max_inodes - sbinfo->free_inodes;
 	if (percpu_counter_compare(&sbinfo->used_blocks, config.max_blocks) > 0)
@@ -3605,6 +3947,7 @@ static int shmem_remount_fs(struct super_block *sb, int *flags, char *data)
 	if (config.max_inodes && !sbinfo->max_inodes)
 		goto out;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	error = 0;
 	sbinfo->huge = config.huge;
 	sbinfo->max_blocks  = config.max_blocks;
@@ -3615,6 +3958,7 @@ static int shmem_remount_fs(struct super_block *sb, int *flags, char *data)
 	 * Preserve previous mempolicy unless mpol remount option was specified.
 	 */
 	if (config.mpol) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mpol_put(sbinfo->mpol);
 		sbinfo->mpol = config.mpol;	/* transfers initial ref */
 	}
@@ -3666,6 +4010,7 @@ SYSCALL_DEFINE2(memfd_create,
 	long len;
 
 	if (!(flags & MFD_HUGETLB)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (flags & ~(unsigned int)MFD_ALL_FLAGS)
 			return -EINVAL;
 	} else {
@@ -3747,6 +4092,7 @@ SYSCALL_DEFINE2(memfd_create,
 
 static void shmem_put_super(struct super_block *sb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct shmem_sb_info *sbinfo = SHMEM_SB(sb);
 
 	percpu_counter_destroy(&sbinfo->used_blocks);
@@ -3765,7 +4111,9 @@ int shmem_fill_super(struct super_block *sb, void *data, int silent)
 	sbinfo = kzalloc(max((int)sizeof(struct shmem_sb_info),
 				L1_CACHE_BYTES), GFP_KERNEL);
 	if (!sbinfo)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	sbinfo->mode = S_IRWXUGO | S_ISVTX;
 	sbinfo->uid = current_fsuid();
@@ -3782,6 +4130,7 @@ int shmem_fill_super(struct super_block *sb, void *data, int silent)
 		sbinfo->max_blocks = shmem_default_max_blocks();
 		sbinfo->max_inodes = shmem_default_max_inodes();
 		if (shmem_parse_options(data, sbinfo, false)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			err = -EINVAL;
 			goto failed;
 		}
@@ -3823,6 +4172,7 @@ int shmem_fill_super(struct super_block *sb, void *data, int silent)
 	sb->s_root = d_make_root(inode);
 	if (!sb->s_root)
 		goto failed;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 failed:
@@ -3837,12 +4187,15 @@ static struct inode *shmem_alloc_inode(struct super_block *sb)
 	struct shmem_inode_info *info;
 	info = kmem_cache_alloc(shmem_inode_cachep, GFP_KERNEL);
 	if (!info)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	return &info->vfs_inode;
 }
 
 static void shmem_destroy_callback(struct rcu_head *head)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct inode *inode = container_of(head, struct inode, i_rcu);
 	if (S_ISLNK(inode->i_mode))
 		kfree(inode->i_link);
@@ -3851,8 +4204,11 @@ static void shmem_destroy_callback(struct rcu_head *head)
 
 static void shmem_destroy_inode(struct inode *inode)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (S_ISREG(inode->i_mode))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mpol_free_shared_policy(&SHMEM_I(inode)->policy);
+}
 	call_rcu(&inode->i_rcu, shmem_destroy_callback);
 }
 
@@ -3872,6 +4228,7 @@ static int shmem_init_inodecache(void)
 
 static void shmem_destroy_inodecache(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kmem_cache_destroy(shmem_inode_cachep);
 }
 
@@ -3989,7 +4346,9 @@ int __init shmem_init(void)
 
 	/* If rootfs called this, don't re-init */
 	if (shmem_inode_cachep)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	error = shmem_init_inodecache();
 	if (error)
@@ -3997,12 +4356,14 @@ int __init shmem_init(void)
 
 	error = register_filesystem(&shmem_fs_type);
 	if (error) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("Could not register tmpfs\n");
 		goto out2;
 	}
 
 	shm_mnt = kern_mount(&shmem_fs_type);
 	if (IS_ERR(shm_mnt)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		error = PTR_ERR(shm_mnt);
 		pr_err("Could not kern_mount tmpfs\n");
 		goto out1;
@@ -4193,14 +4554,21 @@ static struct file *__shmem_file_setup(const char *name, loff_t size,
 	struct qstr this;
 
 	if (IS_ERR(shm_mnt))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_CAST(shm_mnt);
+}
 
 	if (size < 0 || size > MAX_LFS_FILESIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-EINVAL);
+}
 
 	if (shmem_acct_size(flags, size))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	res = ERR_PTR(-ENOMEM);
 	this.name = name;
 	this.len = strlen(name);
@@ -4230,6 +4598,7 @@ static struct file *__shmem_file_setup(const char *name, loff_t size,
 	if (IS_ERR(res))
 		goto put_path;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return res;
 
 put_memory:
@@ -4251,6 +4620,7 @@ static struct file *__shmem_file_setup(const char *name, loff_t size,
  */
 struct file *shmem_kernel_file_setup(const char *name, loff_t size, unsigned long flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __shmem_file_setup(name, size, flags, S_PRIVATE);
 }
 
@@ -4262,6 +4632,7 @@ struct file *shmem_kernel_file_setup(const char *name, loff_t size, unsigned lon
  */
 struct file *shmem_file_setup(const char *name, loff_t size, unsigned long flags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __shmem_file_setup(name, size, flags, 0);
 }
 EXPORT_SYMBOL_GPL(shmem_file_setup);
@@ -4283,10 +4654,14 @@ int shmem_zero_setup(struct vm_area_struct *vma)
 	 */
 	file = __shmem_file_setup("dev/zero", size, vma->vm_flags, S_PRIVATE);
 	if (IS_ERR(file))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PTR_ERR(file);
+}
 
 	if (vma->vm_file)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		fput(vma->vm_file);
+}
 	vma->vm_file = file;
 	vma->vm_ops = &shmem_vm_ops;
 
@@ -4296,6 +4671,7 @@ int shmem_zero_setup(struct vm_area_struct *vma)
 		khugepaged_enter(vma, vma->vm_flags);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -4322,6 +4698,7 @@ struct page *shmem_read_mapping_page_gfp(struct address_space *mapping,
 	struct page *page;
 	int error;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(mapping->a_ops != &shmem_aops);
 	error = shmem_getpage_gfp(inode, index, &page, SGP_CACHE,
 				  gfp, NULL, NULL, NULL);
diff --git a/mm/slab.c b/mm/slab.c
index 966839a..2ac1926 100644
--- a/mm/slab.c
+++ b/mm/slab.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * linux/mm/slab.c
diff --git a/mm/slab.h b/mm/slab.h
index 485d9fb..3688600 100644
--- a/mm/slab.h
+++ b/mm/slab.h
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef MM_SLAB_H
 #define MM_SLAB_H
diff --git a/mm/slab_common.c b/mm/slab_common.c
index 65212ca..aef3b11 100644
--- a/mm/slab_common.c
+++ b/mm/slab_common.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Slab allocator functions that are independent of the allocator strategy
@@ -53,6 +55,7 @@ static bool slab_nomerge = !IS_ENABLED(CONFIG_SLAB_MERGE_DEFAULT);
 
 static int __init setup_slab_nomerge(char *str)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	slab_nomerge = true;
 	return 1;
 }
@@ -68,6 +71,7 @@ __setup("slab_nomerge", setup_slab_nomerge);
  */
 unsigned int kmem_cache_size(struct kmem_cache *s)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return s->object_size;
 }
 EXPORT_SYMBOL(kmem_cache_size);
@@ -106,6 +110,7 @@ static int kmem_cache_sanity_check(const char *name, size_t size)
 #else
 static inline int kmem_cache_sanity_check(const char *name, size_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 #endif
@@ -114,6 +119,7 @@ void __kmem_cache_free_bulk(struct kmem_cache *s, size_t nr, void **p)
 {
 	size_t i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < nr; i++) {
 		if (s)
 			kmem_cache_free(s, p[i]);
@@ -127,6 +133,7 @@ int __kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t nr,
 {
 	size_t i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < nr; i++) {
 		void *x = p[i] = kmem_cache_alloc(s, flags);
 		if (!x) {
@@ -255,6 +262,7 @@ static void memcg_unlink_cache(struct kmem_cache *s)
 static inline int init_memcg_params(struct kmem_cache *s,
 		struct mem_cgroup *memcg, struct kmem_cache *root_cache)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -273,19 +281,27 @@ static inline void memcg_unlink_cache(struct kmem_cache *s)
 int slab_unmergeable(struct kmem_cache *s)
 {
 	if (slab_nomerge || (s->flags & SLAB_NEVER_MERGE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	if (!is_root_cache(s))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	if (s->ctor)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	/*
 	 * We may have set a slab to be unmergeable during bootstrap.
 	 */
 	if (s->refcount < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	return 0;
 }
@@ -296,10 +312,14 @@ struct kmem_cache *find_mergeable(size_t size, size_t align,
 	struct kmem_cache *s;
 
 	if (slab_nomerge)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (ctor)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	size = ALIGN(size, sizeof(void *));
 	align = calculate_alignment(flags, align, size);
@@ -307,7 +327,9 @@ struct kmem_cache *find_mergeable(size_t size, size_t align,
 	flags = kmem_cache_flags(size, flags, name, NULL);
 
 	if (flags & SLAB_NEVER_MERGE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	list_for_each_entry_reverse(s, &slab_root_caches, root_caches_node) {
 		if (slab_unmergeable(s))
@@ -332,8 +354,10 @@ struct kmem_cache *find_mergeable(size_t size, size_t align,
 			(align > s->align || s->align % align))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return s;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -359,7 +383,9 @@ unsigned long calculate_alignment(unsigned long flags,
 	}
 
 	if (align < ARCH_SLAB_MINALIGN)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		align = ARCH_SLAB_MINALIGN;
+}
 
 	return ALIGN(align, sizeof(void *));
 }
@@ -396,7 +422,10 @@ static struct kmem_cache *create_cache(const char *name,
 	memcg_link_cache(s);
 out:
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(err);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return s;
 
 out_free_cache:
@@ -450,6 +479,7 @@ kmem_cache_create(const char *name, size_t size, size_t align,
 
 	/* Refuse requests with allocator specific flags */
 	if (flags & ~SLAB_FLAGS_PERMITTED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = -EINVAL;
 		goto out_unlock;
 	}
@@ -468,6 +498,7 @@ kmem_cache_create(const char *name, size_t size, size_t align,
 
 	cache_name = kstrdup_const(name, GFP_KERNEL);
 	if (!cache_name) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = -ENOMEM;
 		goto out_unlock;
 	}
@@ -476,6 +507,7 @@ kmem_cache_create(const char *name, size_t size, size_t align,
 			 calculate_alignment(flags, align, size),
 			 flags, ctor, NULL, NULL);
 	if (IS_ERR(s)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = PTR_ERR(s);
 		kfree_const(cache_name);
 	}
@@ -488,16 +520,22 @@ kmem_cache_create(const char *name, size_t size, size_t align,
 	put_online_cpus();
 
 	if (err) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (flags & SLAB_PANIC)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			panic("kmem_cache_create: Failed to create slab '%s'. Error %d\n",
 				name, err);
+}
 		else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_warn("kmem_cache_create(%s) failed with error %d\n",
 				name, err);
 			dump_stack();
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return s;
 }
 EXPORT_SYMBOL(kmem_cache_create);
@@ -521,7 +559,9 @@ static void slab_caches_to_rcu_destroy_workfn(struct work_struct *work)
 	mutex_unlock(&slab_mutex);
 
 	if (list_empty(&to_destroy))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	rcu_barrier();
 
@@ -805,12 +845,14 @@ static int shutdown_memcg_caches(struct kmem_cache *s)
 #else
 static inline int shutdown_memcg_caches(struct kmem_cache *s)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 #endif /* CONFIG_MEMCG && !CONFIG_SLOB */
 
 void slab_kmem_cache_release(struct kmem_cache *s)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__kmem_cache_release(s);
 	destroy_memcg_params(s);
 	kfree_const(s->name);
@@ -822,7 +864,9 @@ void kmem_cache_destroy(struct kmem_cache *s)
 	int err;
 
 	if (unlikely(!s))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	get_online_cpus();
 	get_online_mems();
@@ -892,8 +936,10 @@ void __init create_boot_cache(struct kmem_cache *s, const char *name, size_t siz
 	err = __kmem_cache_create(s, flags);
 
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("Creation of kmalloc slab %s size=%zu failed. Reason %d\n",
 					name, size, err);
+}
 
 	s->refcount = -1;	/* Exempt from merging for now */
 }
@@ -904,7 +950,9 @@ struct kmem_cache *__init create_kmalloc_cache(const char *name, size_t size,
 	struct kmem_cache *s = kmem_cache_zalloc(kmem_cache, GFP_NOWAIT);
 
 	if (!s)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("Out of memory when creating slab %s\n", name);
+}
 
 	create_boot_cache(s, name, size, flags);
 	list_add(&s->list, &slab_caches);
@@ -968,13 +1016,16 @@ struct kmem_cache *kmalloc_slab(size_t size, gfp_t flags)
 	int index;
 
 	if (unlikely(size > KMALLOC_MAX_SIZE)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON_ONCE(!(flags & __GFP_NOWARN));
 		return NULL;
 	}
 
 	if (size <= 192) {
 		if (!size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ZERO_SIZE_PTR;
+}
 
 		index = size_index[size_index_elem(size)];
 	} else
@@ -1025,6 +1076,7 @@ void __init setup_kmalloc_cache_index_table(void)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG_ON(KMALLOC_MIN_SIZE > 256 ||
 		(KMALLOC_MIN_SIZE & (KMALLOC_MIN_SIZE - 1)));
 
@@ -1036,16 +1088,20 @@ void __init setup_kmalloc_cache_index_table(void)
 		size_index[elem] = KMALLOC_SHIFT_LOW;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (KMALLOC_MIN_SIZE >= 64) {
 		/*
 		 * The 96 byte size cache is not used if the alignment
 		 * is 64 byte.
 		 */
 		for (i = 64 + 8; i <= 96; i += 8)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			size_index[size_index_elem(i)] = 7;
+}
 
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (KMALLOC_MIN_SIZE >= 128) {
 		/*
 		 * The 192 byte sized cache is not used if the alignment
@@ -1053,7 +1109,9 @@ void __init setup_kmalloc_cache_index_table(void)
 		 * instead.
 		 */
 		for (i = 128 + 8; i <= 192; i += 8)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			size_index[size_index_elem(i)] = 8;
+}
 	}
 }
 
@@ -1095,6 +1153,7 @@ void __init create_kmalloc_caches(unsigned long flags)
 		struct kmem_cache *s = kmalloc_caches[i];
 
 		if (s) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			int size = kmalloc_size(i);
 			char *n = kasprintf(GFP_NOWAIT,
 				 "dma-kmalloc-%d", size);
@@ -1120,6 +1179,7 @@ void *kmalloc_order(size_t size, gfp_t flags, unsigned int order)
 
 	flags |= __GFP_COMP;
 	page = alloc_pages(flags, order);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = page ? page_address(page) : NULL;
 	kmemleak_alloc(ret, size, 1, flags);
 	kasan_kmalloc_large(ret, size, flags);
@@ -1130,6 +1190,7 @@ EXPORT_SYMBOL(kmalloc_order);
 #ifdef CONFIG_TRACING
 void *kmalloc_order_trace(size_t size, gfp_t flags, unsigned int order)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	void *ret = kmalloc_order(size, flags, order);
 	trace_kmalloc(_RET_IP_, ret, size, PAGE_SIZE << order, flags);
 	return ret;
@@ -1215,17 +1276,20 @@ static void print_slabinfo_header(struct seq_file *m)
 
 void *slab_start(struct seq_file *m, loff_t *pos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mutex_lock(&slab_mutex);
 	return seq_list_start(&slab_root_caches, *pos);
 }
 
 void *slab_next(struct seq_file *m, void *p, loff_t *pos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_list_next(p, &slab_root_caches, pos);
 }
 
 void slab_stop(struct seq_file *m, void *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mutex_unlock(&slab_mutex);
 }
 
@@ -1236,7 +1300,9 @@ memcg_accumulate_slabinfo(struct kmem_cache *s, struct slabinfo *info)
 	struct slabinfo sinfo;
 
 	if (!is_root_cache(s))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	for_each_memcg_cache(c, s) {
 		memset(&sinfo, 0, sizeof(sinfo));
@@ -1273,6 +1339,7 @@ static void cache_show(struct kmem_cache *s, struct seq_file *m)
 
 static int slab_show(struct seq_file *m, void *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct kmem_cache *s = list_entry(p, struct kmem_cache, root_caches_node);
 
 	if (p == slab_root_caches.next)
@@ -1337,6 +1404,7 @@ static const struct seq_operations slabinfo_op = {
 
 static int slabinfo_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_open(file, &slabinfo_op);
 }
 
@@ -1367,14 +1435,18 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
 		ks = ksize(p);
 
 	if (ks >= new_size) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kasan_krealloc((void *)p, new_size, flags);
 		return (void *)p;
 	}
 
 	ret = kmalloc_track_caller(new_size, flags);
 	if (ret && p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		memcpy(ret, p, ks);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -1391,8 +1463,11 @@ static __always_inline void *__do_krealloc(const void *p, size_t new_size,
 void *__krealloc(const void *p, size_t new_size, gfp_t flags)
 {
 	if (unlikely(!new_size))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ZERO_SIZE_PTR;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __do_krealloc(p, new_size, flags);
 
 }
@@ -1414,14 +1489,17 @@ void *krealloc(const void *p, size_t new_size, gfp_t flags)
 	void *ret;
 
 	if (unlikely(!new_size)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(p);
 		return ZERO_SIZE_PTR;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = __do_krealloc(p, new_size, flags);
 	if (ret && p != ret)
 		kfree(p);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 EXPORT_SYMBOL(krealloc);
@@ -1443,7 +1521,9 @@ void kzfree(const void *p)
 	void *mem = (void *)p;
 
 	if (unlikely(ZERO_OR_NULL_PTR(mem)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	ks = ksize(mem);
 	memset(mem, 0, ks);
 	kfree(mem);
diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c
index 478ce6d..23fa19c 100644
--- a/mm/sparse-vmemmap.c
+++ b/mm/sparse-vmemmap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Virtual Memory Map support
@@ -59,7 +61,10 @@ void * __meminit vmemmap_alloc_block(unsigned long size, int node)
 			GFP_KERNEL | __GFP_ZERO | __GFP_RETRY_MAYFAIL,
 			get_order(size));
 		if (page)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return page_address(page);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
 	} else
 		return __earlyonly_bootmem_alloc(node, size, size,
@@ -72,12 +77,16 @@ static void * __meminit alloc_block_buf(unsigned long size, int node)
 	void *ptr;
 
 	if (!vmemmap_buf)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return vmemmap_alloc_block(size, node);
+}
 
 	/* take the from buf */
 	ptr = (void *)ALIGN((unsigned long)vmemmap_buf, size);
 	if (ptr + size > vmemmap_buf_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return vmemmap_alloc_block(size, node);
+}
 
 	vmemmap_buf = ptr + size;
 
@@ -86,6 +95,7 @@ static void * __meminit alloc_block_buf(unsigned long size, int node)
 
 static unsigned long __meminit vmem_altmap_next_pfn(struct vmem_altmap *altmap)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return altmap->base_pfn + altmap->reserve + altmap->alloc
 		+ altmap->align;
 }
@@ -95,7 +105,9 @@ static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)
 	unsigned long allocated = altmap->alloc + altmap->align;
 
 	if (altmap->free > allocated)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return altmap->free - allocated;
+}
 	return 0;
 }
 
@@ -109,6 +121,7 @@ static unsigned long __meminit vmem_altmap_nr_free(struct vmem_altmap *altmap)
 static unsigned long __meminit vmem_altmap_alloc(struct vmem_altmap *altmap,
 		unsigned long nr_pfns)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long pfn = vmem_altmap_next_pfn(altmap);
 	unsigned long nr_align;
 
@@ -129,6 +142,7 @@ static void * __meminit altmap_alloc_block_buf(unsigned long size,
 	void *ptr;
 
 	if (size & ~PAGE_MASK) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn_once("%s: allocations must be multiple of PAGE_SIZE (%ld)\n",
 				__func__, size);
 		return NULL;
@@ -151,13 +165,16 @@ void * __meminit __vmemmap_alloc_block_buf(unsigned long size, int node,
 		struct vmem_altmap *altmap)
 {
 	if (altmap)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return altmap_alloc_block_buf(size, altmap);
+}
 	return alloc_block_buf(size, node);
 }
 
 void __meminit vmemmap_verify(pte_t *pte, int node,
 				unsigned long start, unsigned long end)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long pfn = pte_pfn(*pte);
 	int actual_node = early_pfn_to_nid(pfn);
 
@@ -168,6 +185,7 @@ void __meminit vmemmap_verify(pte_t *pte, int node,
 
 pte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_t *pte = pte_offset_kernel(pmd, addr);
 	if (pte_none(*pte)) {
 		pte_t entry;
@@ -182,6 +200,7 @@ pte_t * __meminit vmemmap_pte_populate(pmd_t *pmd, unsigned long addr, int node)
 
 pmd_t * __meminit vmemmap_pmd_populate(pud_t *pud, unsigned long addr, int node)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pmd_t *pmd = pmd_offset(pud, addr);
 	if (pmd_none(*pmd)) {
 		void *p = vmemmap_alloc_block(PAGE_SIZE, node);
@@ -198,21 +217,28 @@ pud_t * __meminit vmemmap_pud_populate(p4d_t *p4d, unsigned long addr, int node)
 	if (pud_none(*pud)) {
 		void *p = vmemmap_alloc_block(PAGE_SIZE, node);
 		if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 		pud_populate(&init_mm, pud, p);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return pud;
 }
 
 p4d_t * __meminit vmemmap_p4d_populate(pgd_t *pgd, unsigned long addr, int node)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	p4d_t *p4d = p4d_offset(pgd, addr);
 	if (p4d_none(*p4d)) {
 		void *p = vmemmap_alloc_block(PAGE_SIZE, node);
 		if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 		p4d_populate(&init_mm, p4d, p);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return p4d;
 }
 
@@ -220,11 +246,16 @@ pgd_t * __meminit vmemmap_pgd_populate(unsigned long addr, int node)
 {
 	pgd_t *pgd = pgd_offset_k(addr);
 	if (pgd_none(*pgd)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		void *p = vmemmap_alloc_block(PAGE_SIZE, node);
 		if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pgd_populate(&init_mm, pgd, p);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return pgd;
 }
 
@@ -238,6 +269,7 @@ int __meminit vmemmap_populate_basepages(unsigned long start,
 	pmd_t *pmd;
 	pte_t *pte;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (; addr < end; addr += PAGE_SIZE) {
 		pgd = vmemmap_pgd_populate(addr, node);
 		if (!pgd)
@@ -271,7 +303,9 @@ struct page * __meminit sparse_mem_map_populate(unsigned long pnum, int nid)
 	end = (unsigned long)(map + PAGES_PER_SECTION);
 
 	if (vmemmap_populate(start, end, nid))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	return map;
 }
@@ -303,6 +337,7 @@ void __init sparse_mem_maps_populate_node(struct page **map_map,
 		map_map[pnum] = sparse_mem_map_populate(pnum, nodeid);
 		if (map_map[pnum])
 			continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ms = __nr_to_section(pnum);
 		pr_err("%s: sparsemem memory map backing failed some memory will not be available\n",
 		       __func__);
diff --git a/mm/sparse.c b/mm/sparse.c
index 30e56a1..d505c1d 100644
--- a/mm/sparse.c
+++ b/mm/sparse.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * sparse memory mappings.
@@ -66,7 +68,9 @@ static noinline struct mem_section __ref *sparse_index_alloc(int nid)
 				   sizeof(struct mem_section);
 
 	if (slab_is_available())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		section = kzalloc_node(array_size, GFP_KERNEL, nid);
+}
 	else
 		section = memblock_virt_alloc_node(array_size, nid);
 
@@ -79,11 +83,15 @@ static int __meminit sparse_index_init(unsigned long section_nr, int nid)
 	struct mem_section *section;
 
 	if (mem_section[root])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EEXIST;
+}
 
 	section = sparse_index_alloc(nid);
 	if (!section)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	mem_section[root] = section;
 
@@ -111,6 +119,7 @@ int __section_nr(struct mem_section* ms)
 		     break;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON(!root);
 
 	return (root_nr * SECTIONS_PER_ROOT) + (ms - root);
@@ -149,16 +158,20 @@ void __meminit mminit_validate_memmodel_limits(unsigned long *start_pfn,
 	 * in larger pfns than the maximum scope of sparsemem:
 	 */
 	if (*start_pfn > max_sparsemem_pfn) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mminit_dprintk(MMINIT_WARNING, "pfnvalidation",
 			"Start of range %lu -> %lu exceeds SPARSEMEM max %lu\n",
 			*start_pfn, *end_pfn, max_sparsemem_pfn);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON_ONCE(1);
 		*start_pfn = max_sparsemem_pfn;
 		*end_pfn = max_sparsemem_pfn;
 	} else if (*end_pfn > max_sparsemem_pfn) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mminit_dprintk(MMINIT_WARNING, "pfnvalidation",
 			"End of range %lu -> %lu exceeds SPARSEMEM max %lu\n",
 			*start_pfn, *end_pfn, max_sparsemem_pfn);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON_ONCE(1);
 		*end_pfn = max_sparsemem_pfn;
 	}
@@ -189,10 +202,13 @@ static inline int next_present_section_nr(int section_nr)
 	do {
 		section_nr++;
 		if (present_section_nr(section_nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return section_nr;
+}
 	} while ((section_nr < NR_MEM_SECTIONS) &&
 		 (section_nr <= __highest_present_section_nr));
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return -1;
 }
 #define for_each_present_section_nr(start, section_nr)		\
@@ -220,6 +236,7 @@ void __init memory_present(int nid, unsigned long start, unsigned long end)
 	start &= PAGE_SECTION_MASK;
 	mminit_validate_memmodel_limits(&start, &end);
 	for (pfn = start; pfn < end; pfn += PAGES_PER_SECTION) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unsigned long section = pfn_to_section_nr(pfn);
 		struct mem_section *ms;
 
@@ -246,6 +263,7 @@ unsigned long __init node_memmap_size_bytes(int nid, unsigned long start_pfn,
 	unsigned long nr_pages = 0;
 
 	mminit_validate_memmodel_limits(&start_pfn, &end_pfn);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (pfn = start_pfn; pfn < end_pfn; pfn += PAGES_PER_SECTION) {
 		if (nid != early_pfn_to_nid(pfn))
 			continue;
@@ -282,7 +300,9 @@ static int __meminit sparse_init_one_section(struct mem_section *ms,
 		unsigned long *pageblock_bitmap)
 {
 	if (!present_section(ms))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	ms->section_mem_map &= ~SECTION_MAP_MASK;
 	ms->section_mem_map |= sparse_encode_mem_map(mem_map, pnum) |
@@ -294,6 +314,7 @@ static int __meminit sparse_init_one_section(struct mem_section *ms,
 
 unsigned long usemap_size(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return BITS_TO_LONGS(SECTION_BLOCKFLAGS_BITS) * sizeof(unsigned long);
 }
 
@@ -382,6 +403,7 @@ static unsigned long * __init
 sparse_early_usemaps_alloc_pgdat_section(struct pglist_data *pgdat,
 					 unsigned long size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return memblock_virt_alloc_node_nopanic(size, pgdat->node_id);
 }
 
@@ -403,6 +425,7 @@ static void __init sparse_early_usemaps_alloc_node(void *data,
 	usemap = sparse_early_usemaps_alloc_pgdat_section(NODE_DATA(nodeid),
 							  size * usemap_count);
 	if (!usemap) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("%s: allocation failed\n", __func__);
 		return;
 	}
@@ -536,6 +559,7 @@ static void __init alloc_usemap_and_memmap(void (*alloc_func)
 		pnum_begin = pnum;
 		break;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	map_count = 1;
 	for_each_present_section_nr(pnum_begin + 1, pnum) {
 		struct mem_section *ms;
@@ -596,7 +620,9 @@ void __init sparse_init(void)
 	size = sizeof(unsigned long *) * NR_MEM_SECTIONS;
 	usemap_map = memblock_virt_alloc(size, 0);
 	if (!usemap_map)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("can not allocate usemap_map\n");
+}
 	alloc_usemap_and_memmap(sparse_early_usemaps_alloc_node,
 							(void *)usemap_map);
 
@@ -604,7 +630,9 @@ void __init sparse_init(void)
 	size2 = sizeof(struct page *) * NR_MEM_SECTIONS;
 	map_map = memblock_virt_alloc(size2, 0);
 	if (!map_map)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("can not allocate map_map\n");
+}
 	alloc_usemap_and_memmap(sparse_early_mem_maps_alloc_node,
 							(void *)map_map);
 #endif
diff --git a/mm/swap.c b/mm/swap.c
index a77d68f..2811a7f 100644
--- a/mm/swap.c
+++ b/mm/swap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/swap.c
  *
@@ -58,6 +60,7 @@ static DEFINE_PER_CPU(struct pagevec, activate_page_pvecs);
 static void __page_cache_release(struct page *page)
 {
 	if (PageLRU(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		struct zone *zone = page_zone(page);
 		struct lruvec *lruvec;
 		unsigned long flags;
@@ -69,6 +72,7 @@ static void __page_cache_release(struct page *page)
 		del_page_from_lru_list(page, lruvec, page_off_lru(page));
 		spin_unlock_irqrestore(zone_lru_lock(zone), flags);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__ClearPageWaiters(page);
 	mem_cgroup_uncharge(page);
 }
@@ -91,6 +95,7 @@ static void __put_compound_page(struct page *page)
 	 */
 	if (!PageHuge(page))
 		__page_cache_release(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	dtor = get_compound_page_dtor(page);
 	(*dtor)(page);
 }
@@ -98,6 +103,7 @@ static void __put_compound_page(struct page *page)
 void __put_page(struct page *page)
 {
 	if (is_zone_device_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_dev_pagemap(page->pgmap);
 
 		/*
@@ -126,6 +132,7 @@ void put_pages_list(struct list_head *pages)
 	while (!list_empty(pages)) {
 		struct page *victim;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		victim = list_entry(pages->prev, struct page, lru);
 		list_del(&victim->lru);
 		put_page(victim);
@@ -151,6 +158,7 @@ int get_kernel_pages(const struct kvec *kiov, int nr_segs, int write,
 {
 	int seg;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (seg = 0; seg < nr_segs; seg++) {
 		if (WARN_ON(kiov[seg].iov_len != PAGE_SIZE))
 			return seg;
@@ -200,7 +208,10 @@ static void pagevec_lru_move_fn(struct pagevec *pvec,
 
 		if (pagepgdat != pgdat) {
 			if (pgdat)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_unlock_irqrestore(&pgdat->lru_lock, flags);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pgdat = pagepgdat;
 			spin_lock_irqsave(&pgdat->lru_lock, flags);
 		}
@@ -209,7 +220,9 @@ static void pagevec_lru_move_fn(struct pagevec *pvec,
 		(*move_fn)(page, lruvec, arg);
 	}
 	if (pgdat)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irqrestore(&pgdat->lru_lock, flags);
+}
 	release_pages(pvec->pages, pvec->nr, pvec->cold);
 	pagevec_reinit(pvec);
 }
@@ -219,6 +232,7 @@ static void pagevec_move_tail_fn(struct page *page, struct lruvec *lruvec,
 {
 	int *pgmoved = arg;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (PageLRU(page) && !PageUnevictable(page)) {
 		del_page_from_lru_list(page, lruvec, page_lru(page));
 		ClearPageActive(page);
@@ -246,6 +260,7 @@ static void pagevec_move_tail(struct pagevec *pvec)
  */
 void rotate_reclaimable_page(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!PageLocked(page) && !PageDirty(page) &&
 	    !PageUnevictable(page) && PageLRU(page)) {
 		struct pagevec *pvec;
@@ -355,6 +370,7 @@ static void __lru_cache_activate_page(struct page *page)
 		struct page *pagevec_page = pvec->pages[i];
 
 		if (pagevec_page == page) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			SetPageActive(page);
 			break;
 		}
@@ -389,15 +405,20 @@ void mark_page_accessed(struct page *page)
 			activate_page(page);
 		else
 			__lru_cache_activate_page(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ClearPageReferenced(page);
 		if (page_is_file_cache(page))
 			workingset_activation(page);
 	} else if (!PageReferenced(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		SetPageReferenced(page);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (page_is_idle(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		clear_page_idle(page);
 }
+}
 EXPORT_SYMBOL(mark_page_accessed);
 
 static void __lru_cache_add(struct page *page)
@@ -417,12 +438,15 @@ static void __lru_cache_add(struct page *page)
 void lru_cache_add_anon(struct page *page)
 {
 	if (PageActive(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ClearPageActive(page);
+}
 	__lru_cache_add(page);
 }
 
 void lru_cache_add_file(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (PageActive(page))
 		ClearPageActive(page);
 	__lru_cache_add(page);
@@ -440,6 +464,7 @@ EXPORT_SYMBOL(lru_cache_add_file);
  */
 void lru_cache_add(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(PageActive(page) && PageUnevictable(page), page);
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 	__lru_cache_add(page);
@@ -457,6 +482,7 @@ void lru_cache_add(struct page *page)
  */
 void add_page_to_unevictable_list(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct pglist_data *pgdat = page_pgdat(page);
 	struct lruvec *lruvec;
 
@@ -482,9 +508,11 @@ void add_page_to_unevictable_list(struct page *page)
 void lru_cache_add_active_or_unevictable(struct page *page,
 					 struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 
 	if (likely((vma->vm_flags & (VM_LOCKED | VM_SPECIAL)) != VM_LOCKED)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		SetPageActive(page);
 		lru_cache_add(page);
 		return;
@@ -531,7 +559,9 @@ static void lru_deactivate_file_fn(struct page *page, struct lruvec *lruvec,
 	bool active;
 
 	if (!PageLRU(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (PageUnevictable(page))
 		return;
@@ -576,6 +606,7 @@ static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec,
 {
 	if (PageLRU(page) && PageAnon(page) && PageSwapBacked(page) &&
 	    !PageSwapCache(page) && !PageUnevictable(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bool active = PageActive(page);
 
 		del_page_from_lru_list(page, lruvec,
@@ -614,13 +645,17 @@ void lru_add_drain_cpu(int cpu)
 
 		/* No harm done if a racing interrupt already did this */
 		local_irq_save(flags);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pagevec_move_tail(pvec);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		local_irq_restore(flags);
 	}
 
 	pvec = &per_cpu(lru_deactivate_file_pvecs, cpu);
 	if (pagevec_count(pvec))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
+}
 
 	pvec = &per_cpu(lru_lazyfree_pvecs, cpu);
 	if (pagevec_count(pvec))
@@ -699,7 +734,9 @@ void lru_add_drain_all_cpuslocked(void)
 	 * initialized.
 	 */
 	if (WARN_ON(!mm_percpu_wq))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	mutex_lock(&lock);
 	cpumask_clear(&has_work);
@@ -726,6 +763,7 @@ void lru_add_drain_all_cpuslocked(void)
 
 void lru_add_drain_all(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	get_online_cpus();
 	lru_add_drain_all_cpuslocked();
 	put_online_cpus();
@@ -758,6 +796,7 @@ void release_pages(struct page **pages, int nr, bool cold)
 		 * same pgdat. The lock is held only if pgdat != NULL.
 		 */
 		if (locked_pgdat && ++lock_batch == SWAP_CLUSTER_MAX) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock_irqrestore(&locked_pgdat->lru_lock, flags);
 			locked_pgdat = NULL;
 		}
@@ -767,11 +806,14 @@ void release_pages(struct page **pages, int nr, bool cold)
 
 		/* Device public page can not be huge page */
 		if (is_device_public_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (locked_pgdat) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_unlock_irqrestore(&locked_pgdat->lru_lock,
 						       flags);
 				locked_pgdat = NULL;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_zone_device_private_or_public_page(page);
 			continue;
 		}
@@ -782,6 +824,7 @@ void release_pages(struct page **pages, int nr, bool cold)
 
 		if (PageCompound(page)) {
 			if (locked_pgdat) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_unlock_irqrestore(&locked_pgdat->lru_lock, flags);
 				locked_pgdat = NULL;
 			}
@@ -790,12 +833,15 @@ void release_pages(struct page **pages, int nr, bool cold)
 		}
 
 		if (PageLRU(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			struct pglist_data *pgdat = page_pgdat(page);
 
 			if (pgdat != locked_pgdat) {
 				if (locked_pgdat)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					spin_unlock_irqrestore(&locked_pgdat->lru_lock,
 									flags);
+}
 				lock_batch = 0;
 				locked_pgdat = pgdat;
 				spin_lock_irqsave(&locked_pgdat->lru_lock, flags);
@@ -814,7 +860,9 @@ void release_pages(struct page **pages, int nr, bool cold)
 		list_add(&page->lru, &pages_to_free);
 	}
 	if (locked_pgdat)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irqrestore(&locked_pgdat->lru_lock, flags);
+}
 
 	mem_cgroup_uncharge_list(&pages_to_free);
 	free_hot_cold_page_list(&pages_to_free, cold);
@@ -1004,7 +1052,9 @@ void __init swap_setup(void)
 
 	/* Use a smaller cluster for small-memory machines */
 	if (megs < 16)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		page_cluster = 2;
+}
 	else
 		page_cluster = 3;
 	/*
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 3264394..8ff0fb2 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  linux/mm/swap_state.c
@@ -800,14 +802,17 @@ static int __init swap_init_sysfs(void)
 
 	swap_kobj = kobject_create_and_add("swap", mm_kobj);
 	if (!swap_kobj) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("failed to create swap kobject\n");
 		return -ENOMEM;
 	}
 	err = sysfs_create_group(swap_kobj, &swap_attr_group);
 	if (err) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("failed to register swap group\n");
 		goto delete_obj;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 delete_obj:
diff --git a/mm/swapfile.c b/mm/swapfile.c
index e47a21e..707ed1c 100644
--- a/mm/swapfile.c
+++ b/mm/swapfile.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/swapfile.c
  *
@@ -100,6 +102,7 @@ atomic_t nr_rotate_swap = ATOMIC_INIT(0);
 
 static inline unsigned char swap_count(unsigned char ent)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ent & ~SWAP_HAS_CACHE;	/* may include SWAP_HAS_CONT flag */
 }
 
@@ -107,6 +110,7 @@ static inline unsigned char swap_count(unsigned char ent)
 static int
 __try_to_reclaim_swap(struct swap_info_struct *si, unsigned long offset)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	swp_entry_t entry = swp_entry(si->type, offset);
 	struct page *page;
 	int ret = 0;
@@ -145,6 +149,7 @@ static int discard_swap(struct swap_info_struct *si)
 	start_block = (se->start_block + 1) << (PAGE_SHIFT - 9);
 	nr_blocks = ((sector_t)se->nr_pages - 1) << (PAGE_SHIFT - 9);
 	if (nr_blocks) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = blkdev_issue_discard(si->bdev, start_block,
 				nr_blocks, GFP_KERNEL, 0);
 		if (err)
@@ -176,6 +181,7 @@ static void discard_swap_cluster(struct swap_info_struct *si,
 	struct swap_extent *se = si->curr_swap_extent;
 	int found_extent = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (nr_pages) {
 		if (se->start_page <= start_page &&
 		    start_page < se->start_page + se->nr_pages) {
@@ -212,68 +218,80 @@ static void discard_swap_cluster(struct swap_info_struct *si,
 static inline void cluster_set_flag(struct swap_cluster_info *info,
 	unsigned int flag)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info->flags = flag;
 }
 
 static inline unsigned int cluster_count(struct swap_cluster_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return info->data;
 }
 
 static inline void cluster_set_count(struct swap_cluster_info *info,
 				     unsigned int c)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info->data = c;
 }
 
 static inline void cluster_set_count_flag(struct swap_cluster_info *info,
 					 unsigned int c, unsigned int f)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info->flags = f;
 	info->data = c;
 }
 
 static inline unsigned int cluster_next(struct swap_cluster_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return info->data;
 }
 
 static inline void cluster_set_next(struct swap_cluster_info *info,
 				    unsigned int n)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info->data = n;
 }
 
 static inline void cluster_set_next_flag(struct swap_cluster_info *info,
 					 unsigned int n, unsigned int f)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info->flags = f;
 	info->data = n;
 }
 
 static inline bool cluster_is_free(struct swap_cluster_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return info->flags & CLUSTER_FLAG_FREE;
 }
 
 static inline bool cluster_is_null(struct swap_cluster_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return info->flags & CLUSTER_FLAG_NEXT_NULL;
 }
 
 static inline void cluster_set_null(struct swap_cluster_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info->flags = CLUSTER_FLAG_NEXT_NULL;
 	info->data = 0;
 }
 
 static inline bool cluster_is_huge(struct swap_cluster_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return info->flags & CLUSTER_FLAG_HUGE;
 }
 
 static inline void cluster_clear_huge(struct swap_cluster_info *info)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	info->flags &= ~CLUSTER_FLAG_HUGE;
 }
 
@@ -284,6 +302,7 @@ static inline struct swap_cluster_info *lock_cluster(struct swap_info_struct *si
 
 	ci = si->cluster_info;
 	if (ci) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ci += offset / SWAPFILE_CLUSTER;
 		spin_lock(&ci->lock);
 	}
@@ -293,8 +312,10 @@ static inline struct swap_cluster_info *lock_cluster(struct swap_info_struct *si
 static inline void unlock_cluster(struct swap_cluster_info *ci)
 {
 	if (ci)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(&ci->lock);
 }
+}
 
 static inline struct swap_cluster_info *lock_cluster_or_swap_info(
 	struct swap_info_struct *si,
@@ -304,7 +325,9 @@ static inline struct swap_cluster_info *lock_cluster_or_swap_info(
 
 	ci = lock_cluster(si, offset);
 	if (!ci)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock(&si->lock);
+}
 
 	return ci;
 }
@@ -312,6 +335,7 @@ static inline struct swap_cluster_info *lock_cluster_or_swap_info(
 static inline void unlock_cluster_or_swap_info(struct swap_info_struct *si,
 					       struct swap_cluster_info *ci)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ci)
 		unlock_cluster(ci);
 	else
@@ -320,16 +344,19 @@ static inline void unlock_cluster_or_swap_info(struct swap_info_struct *si,
 
 static inline bool cluster_list_empty(struct swap_cluster_list *list)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return cluster_is_null(&list->head);
 }
 
 static inline unsigned int cluster_list_first(struct swap_cluster_list *list)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return cluster_next(&list->head);
 }
 
 static void cluster_list_init(struct swap_cluster_list *list)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	cluster_set_null(&list->head);
 	cluster_set_null(&list->tail);
 }
@@ -338,6 +365,7 @@ static void cluster_list_add_tail(struct swap_cluster_list *list,
 				  struct swap_cluster_info *ci,
 				  unsigned int idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (cluster_list_empty(list)) {
 		cluster_set_next_flag(&list->head, idx, 0);
 		cluster_set_next_flag(&list->tail, idx, 0);
@@ -364,6 +392,7 @@ static unsigned int cluster_list_del_first(struct swap_cluster_list *list,
 
 	idx = cluster_next(&list->head);
 	if (cluster_next(&list->tail) == idx) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		cluster_set_null(&list->head);
 		cluster_set_null(&list->tail);
 	} else
@@ -410,6 +439,7 @@ static void swap_do_scheduled_discard(struct swap_info_struct *si)
 
 	info = si->cluster_info;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!cluster_list_empty(&si->discard_clusters)) {
 		idx = cluster_list_del_first(&si->discard_clusters, info);
 		spin_unlock(&si->lock);
@@ -430,6 +460,7 @@ static void swap_discard_work(struct work_struct *work)
 {
 	struct swap_info_struct *si;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	si = container_of(work, struct swap_info_struct, discard_work);
 
 	spin_lock(&si->lock);
@@ -458,6 +489,7 @@ static void free_cluster(struct swap_info_struct *si, unsigned long idx)
 	 */
 	if ((si->flags & (SWP_WRITEOK | SWP_PAGE_DISCARD)) ==
 	    (SWP_WRITEOK | SWP_PAGE_DISCARD)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		swap_cluster_schedule_discard(si, idx);
 		return;
 	}
@@ -475,7 +507,9 @@ static void inc_cluster_info_page(struct swap_info_struct *p,
 	unsigned long idx = page_nr / SWAPFILE_CLUSTER;
 
 	if (!cluster_info)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (cluster_is_free(&cluster_info[idx]))
 		alloc_cluster(p, idx);
 
@@ -495,7 +529,9 @@ static void dec_cluster_info_page(struct swap_info_struct *p,
 	unsigned long idx = page_nr / SWAPFILE_CLUSTER;
 
 	if (!cluster_info)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	VM_BUG_ON(cluster_count(&cluster_info[idx]) == 0);
 	cluster_set_count(&cluster_info[idx],
@@ -517,6 +553,7 @@ scan_swap_map_ssd_cluster_conflict(struct swap_info_struct *si,
 	bool conflict;
 
 	offset /= SWAPFILE_CLUSTER;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	conflict = !cluster_list_empty(&si->free_clusters) &&
 		offset != cluster_list_first(&si->free_clusters) &&
 		cluster_is_free(&si->cluster_info[offset]);
@@ -596,12 +633,14 @@ static void __del_from_avail_list(struct swap_info_struct *p)
 {
 	int nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node(nid)
 		plist_del(&p->avail_lists[nid], &swap_avail_heads[nid]);
 }
 
 static void del_from_avail_list(struct swap_info_struct *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&swap_avail_lock);
 	__del_from_avail_list(p);
 	spin_unlock(&swap_avail_lock);
@@ -613,7 +652,9 @@ static void swap_range_alloc(struct swap_info_struct *si, unsigned long offset,
 	unsigned int end = offset + nr_entries - 1;
 
 	if (offset == si->lowest_bit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		si->lowest_bit += nr_entries;
+}
 	if (end == si->highest_bit)
 		si->highest_bit -= nr_entries;
 	si->inuse_pages += nr_entries;
@@ -629,6 +670,7 @@ static void add_to_avail_list(struct swap_info_struct *p)
 	int nid;
 
 	spin_lock(&swap_avail_lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node(nid) {
 		WARN_ON(!plist_node_empty(&p->avail_lists[nid]));
 		plist_add(&p->avail_lists[nid], &swap_avail_heads[nid]);
@@ -643,7 +685,9 @@ static void swap_range_free(struct swap_info_struct *si, unsigned long offset,
 	void (*swap_slot_free_notify)(struct block_device *, unsigned long);
 
 	if (offset < si->lowest_bit)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		si->lowest_bit = offset;
+}
 	if (end > si->highest_bit) {
 		bool was_full = !si->highest_bit;
 
@@ -678,7 +722,9 @@ static int scan_swap_map_slots(struct swap_info_struct *si,
 	int n_ret = 0;
 
 	if (nr > SWAP_BATCH)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr = SWAP_BATCH;
+}
 
 	/*
 	 * We try to cluster swap pages by allocating them sequentially
@@ -904,6 +950,7 @@ static void swap_free_cluster(struct swap_info_struct *si, unsigned long idx)
 #else
 static int swap_alloc_cluster(struct swap_info_struct *si, swp_entry_t *slot)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_WARN_ON_ONCE(1);
 	return 0;
 }
@@ -918,7 +965,9 @@ static unsigned long scan_swap_map(struct swap_info_struct *si,
 	n_ret = scan_swap_map_slots(si, usage, 1, &entry);
 
 	if (n_ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return swp_offset(entry);
+}
 	else
 		return 0;
 
@@ -926,6 +975,7 @@ static unsigned long scan_swap_map(struct swap_info_struct *si,
 
 int get_swap_pages(int n_goal, bool cluster, swp_entry_t swp_entries[])
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long nr_pages = cluster ? SWAPFILE_CLUSTER : 1;
 	struct swap_info_struct *si, *next;
 	long avail_pgs;
@@ -1019,6 +1069,7 @@ swp_entry_t get_swap_page_of_type(int type)
 
 	si = swap_info[type];
 	spin_lock(&si->lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (si && (si->flags & SWP_WRITEOK)) {
 		atomic_long_dec(&nr_swap_pages);
 		/* This is called for allocating swap entry, not cache */
@@ -1040,6 +1091,7 @@ static struct swap_info_struct *__swap_info_get(swp_entry_t entry)
 
 	if (!entry.val)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	type = swp_type(entry);
 	if (type >= nr_swapfiles)
 		goto bad_nofile;
@@ -1070,6 +1122,7 @@ static struct swap_info_struct *_swap_info_get(swp_entry_t entry)
 	p = __swap_info_get(entry);
 	if (!p)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!p->swap_map[swp_offset(entry)])
 		goto bad_free;
 	return p;
@@ -1087,7 +1140,9 @@ static struct swap_info_struct *swap_info_get(swp_entry_t entry)
 
 	p = _swap_info_get(entry);
 	if (p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock(&p->lock);
+}
 	return p;
 }
 
@@ -1099,6 +1154,7 @@ static struct swap_info_struct *swap_info_get_cont(swp_entry_t entry,
 	p = _swap_info_get(entry);
 
 	if (p != q) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (q != NULL)
 			spin_unlock(&q->lock);
 		if (p != NULL)
@@ -1123,6 +1179,7 @@ static unsigned char __swap_entry_free(struct swap_info_struct *p,
 	count &= ~SWAP_HAS_CACHE;
 
 	if (usage == SWAP_HAS_CACHE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		VM_BUG_ON(!has_cache);
 		has_cache = 0;
 	} else if (count == SWAP_MAP_SHMEM) {
@@ -1176,6 +1233,7 @@ void swap_free(swp_entry_t entry)
 
 	p = _swap_info_get(entry);
 	if (p) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!__swap_entry_free(p, entry, 1))
 			free_swap_slot(entry);
 	}
@@ -1190,6 +1248,7 @@ static void swapcache_free(swp_entry_t entry)
 
 	p = _swap_info_get(entry);
 	if (p) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!__swap_entry_free(p, entry, SWAP_HAS_CACHE))
 			free_swap_slot(entry);
 	}
@@ -1263,6 +1322,7 @@ static inline void swapcache_free_cluster(swp_entry_t entry)
 
 void put_swap_page(struct page *page, swp_entry_t entry)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!PageTransHuge(page))
 		swapcache_free(entry);
 	else
@@ -1282,7 +1342,9 @@ void swapcache_free_entries(swp_entry_t *entries, int n)
 	int i;
 
 	if (n <= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	prev = NULL;
 	p = NULL;
@@ -1320,6 +1382,7 @@ int page_swapcount(struct page *page)
 	entry.val = page_private(page);
 	p = _swap_info_get(entry);
 	if (p) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		offset = swp_offset(entry);
 		ci = lock_cluster_or_swap_info(p, offset);
 		count = swap_count(p->swap_map[offset]);
@@ -1352,7 +1415,9 @@ int __swp_swapcount(swp_entry_t entry)
 
 	si = __swap_info_get(entry);
 	if (si)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		count = swap_swapcount(si, entry);
+}
 	return count;
 }
 
@@ -1371,7 +1436,9 @@ int swp_swapcount(swp_entry_t entry)
 
 	p = _swap_info_get(entry);
 	if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	offset = swp_offset(entry);
 
@@ -1523,7 +1590,9 @@ static int page_trans_huge_map_swapcount(struct page *page, int *total_mapcount,
 
 	mapcount = page_trans_huge_mapcount(page, total_mapcount);
 	if (PageSwapCache(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		swapcount = page_swapcount(page);
+}
 	if (total_swapcount)
 		*total_swapcount = swapcount;
 	return mapcount + swapcount;
@@ -1546,7 +1615,9 @@ bool reuse_swap_page(struct page *page, int *total_map_swapcount)
 
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	if (unlikely(PageKsm(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 	count = page_trans_huge_map_swapcount(page, &total_mapcount,
 					      &total_swapcount);
 	if (total_map_swapcount)
@@ -1555,7 +1626,9 @@ bool reuse_swap_page(struct page *page, int *total_map_swapcount)
 	    (likely(!PageTransCompound(page)) ||
 	     /* The remaining swap count will be freed soon */
 	     total_swapcount == page_swapcount(page))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!PageWriteback(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			page = compound_head(page);
 			delete_from_swap_cache(page);
 			SetPageDirty(page);
@@ -1566,9 +1639,11 @@ bool reuse_swap_page(struct page *page, int *total_map_swapcount)
 			entry.val = page_private(page);
 			p = swap_info_get(entry);
 			if (p->flags & SWP_STABLE_WRITES) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_unlock(&p->lock);
 				return false;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(&p->lock);
 		}
 	}
@@ -1582,6 +1657,7 @@ bool reuse_swap_page(struct page *page, int *total_map_swapcount)
  */
 int try_to_free_swap(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 
 	if (!PageSwapCache(page))
@@ -1626,7 +1702,9 @@ int free_swap_and_cache(swp_entry_t entry)
 	unsigned char count;
 
 	if (non_swap_entry(entry))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	p = _swap_info_get(entry);
 	if (p) {
@@ -1675,7 +1753,9 @@ int swap_type_of(dev_t device, sector_t offset, struct block_device **bdev_p)
 	int type;
 
 	if (device)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bdev = bdget(device);
+}
 
 	spin_lock(&swap_lock);
 	for (type = 0; type < nr_swapfiles; type++) {
@@ -1720,7 +1800,9 @@ sector_t swapdev_block(int type, pgoff_t offset)
 	struct block_device *bdev;
 
 	if ((unsigned int)type >= nr_swapfiles)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	if (!(swap_info[type]->flags & SWP_WRITEOK))
 		return 0;
 	return map_swap_entry(swp_entry(type, offset), &bdev);
@@ -1742,6 +1824,7 @@ unsigned int count_swap_pages(int type, int free)
 
 		spin_lock(&sis->lock);
 		if (sis->flags & SWP_WRITEOK) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			n = sis->pages;
 			if (free)
 				n -= sis->inuse_pages;
@@ -1755,6 +1838,7 @@ unsigned int count_swap_pages(int type, int free)
 
 static inline int pte_same_as_swp(pte_t pte, pte_t swp_pte)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return pte_same(pte_swp_clear_soft_dirty(pte), swp_pte);
 }
 
@@ -1775,7 +1859,9 @@ static int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,
 	swapcache = page;
 	page = ksm_might_need_to_copy(page, vma, addr);
 	if (unlikely(!page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (mem_cgroup_try_charge(page, vma->vm_mm, GFP_KERNEL,
 				&memcg, false)) {
@@ -1823,6 +1909,7 @@ static int unuse_pte_range(struct vm_area_struct *vma, pmd_t *pmd,
 				unsigned long addr, unsigned long end,
 				swp_entry_t entry, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_t swp_pte = swp_entry_to_pte(entry);
 	pte_t *pte;
 	int ret = 0;
@@ -1865,6 +1952,7 @@ static inline int unuse_pmd_range(struct vm_area_struct *vma, pud_t *pud,
 
 	pmd = pmd_offset(pud, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		cond_resched();
 		next = pmd_addr_end(addr, end);
 		if (pmd_none_or_trans_huge_or_clear_bad(pmd))
@@ -1886,6 +1974,7 @@ static inline int unuse_pud_range(struct vm_area_struct *vma, p4d_t *p4d,
 
 	pud = pud_offset(p4d, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = pud_addr_end(addr, end);
 		if (pud_none_or_clear_bad(pud))
 			continue;
@@ -1906,6 +1995,7 @@ static inline int unuse_p4d_range(struct vm_area_struct *vma, pgd_t *pgd,
 
 	p4d = p4d_offset(pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(p4d))
 			continue;
@@ -1924,6 +2014,7 @@ static int unuse_vma(struct vm_area_struct *vma,
 	int ret;
 
 	if (page_anon_vma(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		addr = page_address_in_vma(page, vma);
 		if (addr == -EFAULT)
 			return 0;
@@ -1990,6 +2081,7 @@ static unsigned int find_next_to_unuse(struct swap_info_struct *si,
 	 * allocations from this area (while holding swap_lock).
 	 */
 	for (;;) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (++i >= max) {
 			if (!prev) {
 				i = 0;
@@ -2256,6 +2348,7 @@ static void drain_mmlist(void)
 	struct list_head *p, *next;
 	unsigned int type;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (type = 0; type < nr_swapfiles; type++)
 		if (swap_info[type]->inuse_pages)
 			return;
@@ -2286,6 +2379,7 @@ static sector_t map_swap_entry(swp_entry_t entry, struct block_device **bdev)
 	se = start_se;
 
 	for ( ; ; ) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (se->start_page <= offset &&
 				offset < (se->start_page + se->nr_pages)) {
 			return se->start_block + (offset - se->start_page);
@@ -2311,6 +2405,7 @@ sector_t map_swap_page(struct page *page, struct block_device **bdev)
  */
 static void destroy_swap_extents(struct swap_info_struct *sis)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!list_empty(&sis->first_swap_extent.list)) {
 		struct swap_extent *se;
 
@@ -2344,6 +2439,7 @@ add_swap_extent(struct swap_info_struct *sis, unsigned long start_page,
 	struct list_head *lh;
 
 	if (start_page == 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		se = &sis->first_swap_extent;
 		sis->curr_swap_extent = se;
 		se->start_page = 0;
@@ -2414,6 +2510,7 @@ static int setup_swap_extents(struct swap_info_struct *sis, sector_t *span)
 	int ret;
 
 	if (S_ISBLK(inode->i_mode)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = add_swap_extent(sis, 0, sis->max, 0);
 		*span = sis->pages;
 		return ret;
@@ -2437,7 +2534,9 @@ static int swap_node(struct swap_info_struct *p)
 	struct block_device *bdev;
 
 	if (p->bdev)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bdev = p->bdev;
+}
 	else
 		bdev = p->swap_file->f_inode->i_sb->s_bdev;
 
@@ -2451,7 +2550,9 @@ static void _enable_swap_info(struct swap_info_struct *p, int prio,
 	int i;
 
 	if (prio >= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		p->prio = prio;
+}
 	else
 		p->prio = --least_priority;
 	/*
@@ -2495,6 +2596,7 @@ static void enable_swap_info(struct swap_info_struct *p, int prio,
 				struct swap_cluster_info *cluster_info,
 				unsigned long *frontswap_map)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	frontswap_init(p->type, frontswap_map);
 	spin_lock(&swap_lock);
 	spin_lock(&p->lock);
@@ -2505,6 +2607,7 @@ static void enable_swap_info(struct swap_info_struct *p, int prio,
 
 static void reinsert_swap_info(struct swap_info_struct *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&swap_lock);
 	spin_lock(&p->lock);
 	_enable_swap_info(p, p->prio, p->swap_map, p->cluster_info);
@@ -2518,7 +2621,9 @@ bool has_usable_swap(void)
 
 	spin_lock(&swap_lock);
 	if (plist_head_empty(&swap_active_head))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = false;
+}
 	spin_unlock(&swap_lock);
 	return ret;
 }
@@ -2537,7 +2642,9 @@ SYSCALL_DEFINE1(swapoff, const char __user *, specialfile)
 	unsigned int old_block_size;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 
 	BUG_ON(!current->mm);
 
@@ -2697,6 +2804,7 @@ static unsigned swaps_poll(struct file *file, poll_table *wait)
 	poll_wait(file, &proc_poll_wait, wait);
 
 	if (seq->poll_event != atomic_read(&proc_poll_event)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		seq->poll_event = atomic_read(&proc_poll_event);
 		return POLLIN | POLLRDNORM | POLLERR | POLLPRI;
 	}
@@ -2714,7 +2822,9 @@ static void *swap_start(struct seq_file *swap, loff_t *pos)
 	mutex_lock(&swapon_mutex);
 
 	if (!l)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return SEQ_START_TOKEN;
+}
 
 	for (type = 0; type < nr_swapfiles; type++) {
 		smp_rmb();	/* read nr_swapfiles before swap_info[type] */
@@ -2734,7 +2844,9 @@ static void *swap_next(struct seq_file *swap, void *v, loff_t *pos)
 	int type;
 
 	if (v == SEQ_START_TOKEN)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		type = 0;
+}
 	else
 		type = si->type + 1;
 
@@ -2752,6 +2864,7 @@ static void *swap_next(struct seq_file *swap, void *v, loff_t *pos)
 
 static void swap_stop(struct seq_file *swap, void *v)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mutex_unlock(&swapon_mutex);
 }
 
@@ -2762,6 +2875,7 @@ static int swap_show(struct seq_file *swap, void *v)
 	int len;
 
 	if (si == SEQ_START_TOKEN) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		seq_puts(swap,"Filename\t\t\t\tType\t\tSize\tUsed\tPriority\n");
 		return 0;
 	}
@@ -2792,7 +2906,9 @@ static int swaps_open(struct inode *inode, struct file *file)
 
 	ret = seq_open(file, &swaps_op);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	seq = file->private_data;
 	seq->poll_event = atomic_read(&proc_poll_event);
@@ -2818,6 +2934,7 @@ __initcall(procswaps_init);
 #ifdef MAX_SWAPFILES_CHECK
 static int __init max_swapfiles_check(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	MAX_SWAPFILES_CHECK();
 	return 0;
 }
@@ -2832,7 +2949,9 @@ static struct swap_info_struct *alloc_swap_info(void)
 
 	p = kzalloc(sizeof(*p), GFP_KERNEL);
 	if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	spin_lock(&swap_lock);
 	for (type = 0; type < nr_swapfiles; type++) {
@@ -2879,6 +2998,7 @@ static int claim_swapfile(struct swap_info_struct *p, struct inode *inode)
 	int error;
 
 	if (S_ISBLK(inode->i_mode)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		p->bdev = bdgrab(I_BDEV(inode));
 		error = blkdev_get(p->bdev,
 				   FMODE_READ | FMODE_WRITE | FMODE_EXCL, p);
@@ -2912,6 +3032,7 @@ static unsigned long read_swap_header(struct swap_info_struct *p,
 	unsigned long last_page;
 
 	if (memcmp("SWAPSPACE2", swap_header->magic.magic, 10)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("Unable to find swap-space signature\n");
 		return 0;
 	}
@@ -3000,6 +3121,7 @@ static int setup_swap_map_and_extents(struct swap_info_struct *p,
 	unsigned int nr_good_pages;
 	int nr_extents;
 	unsigned long nr_clusters = DIV_ROUND_UP(maxpages, SWAPFILE_CLUSTER);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long col = p->cluster_next / SWAPFILE_CLUSTER % SWAP_CLUSTER_COLS;
 	unsigned long i, idx;
 
@@ -3076,6 +3198,7 @@ static int setup_swap_map_and_extents(struct swap_info_struct *p,
  */
 static bool swap_discardable(struct swap_info_struct *si)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request_queue *q = bdev_get_queue(si->bdev);
 
 	if (!q || !blk_queue_discard(q))
@@ -3103,7 +3226,9 @@ SYSCALL_DEFINE2(swapon, const char __user *, specialfile, int, swap_flags)
 	struct inode *inode = NULL;
 
 	if (swap_flags & ~SWAP_FLAGS_VALID)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (!capable(CAP_SYS_ADMIN))
 		return -EPERM;
@@ -3323,8 +3448,11 @@ void si_swapinfo(struct sysinfo *val)
 	for (type = 0; type < nr_swapfiles; type++) {
 		struct swap_info_struct *si = swap_info[type];
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if ((si->flags & SWP_USED) && !(si->flags & SWP_WRITEOK))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			nr_to_be_unused += si->inuse_pages;
+}
 	}
 	val->freeswap = atomic_long_read(&nr_swap_pages) + nr_to_be_unused;
 	val->totalswap = total_swap_pages + nr_to_be_unused;
@@ -3354,6 +3482,7 @@ static int __swap_duplicate(swp_entry_t entry, unsigned char usage)
 	if (non_swap_entry(entry))
 		goto out;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	type = swp_type(entry);
 	if (type >= nr_swapfiles)
 		goto bad_file;
@@ -3420,6 +3549,7 @@ static int __swap_duplicate(swp_entry_t entry, unsigned char usage)
  */
 void swap_shmem_alloc(swp_entry_t entry)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__swap_duplicate(entry, SWAP_MAP_SHMEM);
 }
 
@@ -3434,6 +3564,7 @@ int swap_duplicate(swp_entry_t entry)
 {
 	int err = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!err && __swap_duplicate(entry, 1) == -ENOMEM)
 		err = add_swap_count_continuation(entry, GFP_ATOMIC);
 	return err;
@@ -3449,6 +3580,7 @@ int swap_duplicate(swp_entry_t entry)
  */
 int swapcache_prepare(swp_entry_t entry)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __swap_duplicate(entry, SWAP_HAS_CACHE);
 }
 
@@ -3463,6 +3595,7 @@ struct swap_info_struct *page_swap_info(struct page *page)
  */
 struct address_space *__page_file_mapping(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	VM_BUG_ON_PAGE(!PageSwapCache(page), page);
 	return page_swap_info(page)->swap_file->f_mapping;
 }
@@ -3517,6 +3650,7 @@ int add_swap_count_continuation(swp_entry_t entry, gfp_t gfp_mask)
 		goto outer;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	offset = swp_offset(entry);
 
 	ci = lock_cluster(si, offset);
@@ -3612,6 +3746,7 @@ static bool swap_count_continued(struct swap_info_struct *si,
 
 	head = vmalloc_to_page(si->swap_map + offset);
 	if (page_private(head) != SWP_CONTINUED) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(count & COUNT_CONTINUED);
 		return false;		/* need to add count continuation */
 	}
@@ -3694,6 +3829,7 @@ static void free_swap_count_continuations(struct swap_info_struct *si)
 {
 	pgoff_t offset;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (offset = 0; offset < si->max; offset += PAGE_SIZE) {
 		struct page *head;
 		head = vmalloc_to_page(si->swap_map + offset);
@@ -3715,6 +3851,7 @@ static int __init swapfile_init(void)
 	swap_avail_heads = kmalloc_array(nr_node_ids, sizeof(struct plist_head),
 					 GFP_KERNEL);
 	if (!swap_avail_heads) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_emerg("Not enough memory for swap heads, swap is disabled\n");
 		return -ENOMEM;
 	}
@@ -3722,6 +3859,7 @@ static int __init swapfile_init(void)
 	for_each_node(nid)
 		plist_head_init(&swap_avail_heads[nid]);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 subsys_initcall(swapfile_init);
diff --git a/mm/truncate.c b/mm/truncate.c
index 2330223..af70dc8 100644
--- a/mm/truncate.c
+++ b/mm/truncate.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * mm/truncate.c - code for taking down pages from address_spaces
  *
@@ -39,6 +41,7 @@ static void clear_shadow_entry(struct address_space *mapping, pgoff_t index,
 	 */
 	if (!__radix_tree_lookup(&mapping->page_tree, index, &node, &slot))
 		goto unlock;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (*slot != entry)
 		goto unlock;
 	__radix_tree_replace(&mapping->page_tree, node, slot, NULL,
@@ -118,7 +121,9 @@ void do_invalidatepage(struct page *page, unsigned int offset,
 	invalidatepage = page->mapping->a_ops->invalidatepage;
 #ifdef CONFIG_BLOCK
 	if (!invalidatepage)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		invalidatepage = block_invalidatepage;
+}
 #endif
 	if (invalidatepage)
 		(*invalidatepage)(page, offset, length);
@@ -138,7 +143,9 @@ static int
 truncate_complete_page(struct address_space *mapping, struct page *page)
 {
 	if (page->mapping != mapping)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EIO;
+}
 
 	if (page_has_private(page))
 		do_invalidatepage(page, 0, PAGE_SIZE);
@@ -168,7 +175,9 @@ invalidate_complete_page(struct address_space *mapping, struct page *page)
 	int ret;
 
 	if (page->mapping != mapping)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (page_has_private(page) && !try_to_release_page(page, 0))
 		return 0;
@@ -185,6 +194,7 @@ int truncate_inode_page(struct address_space *mapping, struct page *page)
 
 	holelen = PageTransHuge(page) ? HPAGE_PMD_SIZE : PAGE_SIZE;
 	if (page_mapped(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unmap_mapping_range(mapping,
 				   (loff_t)page->index << PAGE_SHIFT,
 				   holelen, 0);
@@ -197,6 +207,7 @@ int truncate_inode_page(struct address_space *mapping, struct page *page)
  */
 int generic_error_remove_page(struct address_space *mapping, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!mapping)
 		return -EINVAL;
 	/*
@@ -217,6 +228,7 @@ EXPORT_SYMBOL(generic_error_remove_page);
  */
 int invalidate_inode_page(struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct address_space *mapping = page_mapping(page);
 	if (!mapping)
 		return 0;
@@ -287,6 +299,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 	else
 		end = (lend + 1) >> PAGE_SHIFT;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pagevec_init(&pvec, 0);
 	index = start;
 	while (index < end && pagevec_lookup_entries(&pvec, mapping, index,
@@ -301,6 +314,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 				break;
 
 			if (radix_tree_exceptional_entry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				truncate_exceptional_entry(mapping, index,
 							   page);
 				continue;
@@ -310,6 +324,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 				continue;
 			WARN_ON(page_to_index(page) != index);
 			if (PageWriteback(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				unlock_page(page);
 				continue;
 			}
@@ -323,6 +338,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 	}
 
 	if (partial_start) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		struct page *page = find_lock_page(mapping, start - 1);
 		if (page) {
 			unsigned int top = PAGE_SIZE;
@@ -331,25 +347,32 @@ void truncate_inode_pages_range(struct address_space *mapping,
 				top = partial_end;
 				partial_end = 0;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			wait_on_page_writeback(page);
 			zero_user_segment(page, partial_start, top);
 			cleancache_invalidate_page(mapping, page);
 			if (page_has_private(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				do_invalidatepage(page, partial_start,
 						  top - partial_start);
+}
 			unlock_page(page);
 			put_page(page);
 		}
 	}
 	if (partial_end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		struct page *page = find_lock_page(mapping, end);
 		if (page) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			wait_on_page_writeback(page);
 			zero_user_segment(page, 0, partial_end);
 			cleancache_invalidate_page(mapping, page);
 			if (page_has_private(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				do_invalidatepage(page, 0,
 						  partial_end);
+}
 			unlock_page(page);
 			put_page(page);
 		}
@@ -361,6 +384,7 @@ void truncate_inode_pages_range(struct address_space *mapping,
 	if (start >= end)
 		goto out;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	index = start;
 	for ( ; ; ) {
 		cond_resched();
@@ -373,12 +397,14 @@ void truncate_inode_pages_range(struct address_space *mapping,
 			index = start;
 			continue;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (index == start && indices[0] >= end) {
 			/* All gone out of hole to be punched, we're done */
 			pagevec_remove_exceptionals(&pvec);
 			pagevec_release(&pvec);
 			break;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for (i = 0; i < pagevec_count(&pvec); i++) {
 			struct page *page = pvec.pages[i];
 
@@ -390,18 +416,23 @@ void truncate_inode_pages_range(struct address_space *mapping,
 				break;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (radix_tree_exceptional_entry(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				truncate_exceptional_entry(mapping, index,
 							   page);
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			lock_page(page);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			WARN_ON(page_to_index(page) != index);
 			wait_on_page_writeback(page);
 			truncate_inode_page(mapping, page);
 			unlock_page(page);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pagevec_remove_exceptionals(&pvec);
 		pagevec_release(&pvec);
 		index++;
@@ -501,6 +532,7 @@ unsigned long invalidate_mapping_pages(struct address_space *mapping,
 	int i;
 
 	pagevec_init(&pvec, 0);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (index <= end && pagevec_lookup_entries(&pvec, mapping, index,
 			min(end - index, (pgoff_t)PAGEVEC_SIZE - 1) + 1,
 			indices)) {
@@ -573,7 +605,9 @@ invalidate_complete_page2(struct address_space *mapping, struct page *page)
 	unsigned long flags;
 
 	if (page->mapping != mapping)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (page_has_private(page) && !try_to_release_page(page, GFP_KERNEL))
 		return 0;
@@ -598,6 +632,7 @@ invalidate_complete_page2(struct address_space *mapping, struct page *page)
 
 static int do_launder_page(struct address_space *mapping, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!PageDirty(page))
 		return 0;
 	if (page->mapping != mapping || mapping->a_ops->launder_page == NULL)
@@ -627,6 +662,7 @@ int invalidate_inode_pages2_range(struct address_space *mapping,
 	int ret2 = 0;
 	int did_range_unmap = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (mapping->nrpages == 0 && mapping->nrexceptional == 0)
 		goto out;
 
@@ -720,6 +756,7 @@ EXPORT_SYMBOL_GPL(invalidate_inode_pages2_range);
  */
 int invalidate_inode_pages2(struct address_space *mapping)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return invalidate_inode_pages2_range(mapping, 0, -1);
 }
 EXPORT_SYMBOL_GPL(invalidate_inode_pages2);
@@ -778,7 +815,9 @@ void truncate_setsize(struct inode *inode, loff_t newsize)
 
 	i_size_write(inode, newsize);
 	if (newsize > oldsize)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pagecache_isize_extended(inode, oldsize, newsize);
+}
 	truncate_pagecache(inode, newsize);
 }
 EXPORT_SYMBOL(truncate_setsize);
@@ -804,6 +843,7 @@ EXPORT_SYMBOL(truncate_setsize);
  */
 void pagecache_isize_extended(struct inode *inode, loff_t from, loff_t to)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int bsize = i_blocksize(inode);
 	loff_t rounded_from;
 	struct page *page;
@@ -866,8 +906,10 @@ void truncate_pagecache_range(struct inode *inode, loff_t lstart, loff_t lend)
 	 * hole-punching should not remove private COWed pages from the hole.
 	 */
 	if ((u64)unmap_end > (u64)unmap_start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unmap_mapping_range(mapping, unmap_start,
 				    1 + unmap_end - unmap_start, 0);
+}
 	truncate_inode_pages_range(mapping, lstart, lend);
 }
 EXPORT_SYMBOL(truncate_pagecache_range);
diff --git a/mm/usercopy.c b/mm/usercopy.c
index a9852b2..f36f1f9 100644
--- a/mm/usercopy.c
+++ b/mm/usercopy.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * This implements the various checks for CONFIG_HARDENED_USERCOPY*,
  * which are designed to protect kernel memory from needless exposure
diff --git a/mm/util.c b/mm/util.c
index 34e57fae..7724504 100644
--- a/mm/util.c
+++ b/mm/util.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 #include <linux/mm.h>
 #include <linux/slab.h>
 #include <linux/string.h>
@@ -50,12 +52,17 @@ char *kstrdup(const char *s, gfp_t gfp)
 	char *buf;
 
 	if (!s)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	len = strlen(s) + 1;
 	buf = kmalloc_track_caller(len, gfp);
 	if (buf)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		memcpy(buf, s, len);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return buf;
 }
 EXPORT_SYMBOL(kstrdup);
@@ -72,7 +79,9 @@ EXPORT_SYMBOL(kstrdup);
 const char *kstrdup_const(const char *s, gfp_t gfp)
 {
 	if (is_kernel_rodata((unsigned long)s))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return s;
+}
 
 	return kstrdup(s, gfp);
 }
@@ -92,14 +101,19 @@ char *kstrndup(const char *s, size_t max, gfp_t gfp)
 	char *buf;
 
 	if (!s)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	len = strnlen(s, max);
 	buf = kmalloc_track_caller(len+1, gfp);
 	if (buf) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		memcpy(buf, s, len);
 		buf[len] = '\0';
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return buf;
 }
 EXPORT_SYMBOL(kstrndup);
@@ -117,7 +131,9 @@ void *kmemdup(const void *src, size_t len, gfp_t gfp)
 
 	p = kmalloc_track_caller(len, gfp);
 	if (p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		memcpy(p, src, len);
+}
 	return p;
 }
 EXPORT_SYMBOL(kmemdup);
@@ -133,7 +149,9 @@ char *kmemdup_nul(const char *s, size_t len, gfp_t gfp)
 	char *buf;
 
 	if (!s)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	buf = kmalloc_track_caller(len + 1, gfp);
 	if (buf) {
@@ -163,13 +181,17 @@ void *memdup_user(const void __user *src, size_t len)
 	 */
 	p = kmalloc_track_caller(len, GFP_KERNEL);
 	if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	if (copy_from_user(p, src, len)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(p);
 		return ERR_PTR(-EFAULT);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return p;
 }
 EXPORT_SYMBOL(memdup_user);
@@ -187,15 +209,21 @@ char *strndup_user(const char __user *s, long n)
 	length = strnlen_user(s, n);
 
 	if (!length)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-EFAULT);
+}
 
 	if (length > n)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-EINVAL);
+}
 
 	p = memdup_user(s, length);
 
 	if (IS_ERR(p))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return p;
+}
 
 	p[length - 1] = '\0';
 
@@ -222,9 +250,12 @@ void *memdup_user_nul(const void __user *src, size_t len)
 	 */
 	p = kmalloc_track_caller(len + 1, GFP_KERNEL);
 	if (!p)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	if (copy_from_user(p, src, len)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(p);
 		return ERR_PTR(-EFAULT);
 	}
@@ -259,6 +290,7 @@ void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,
 /* Check if the vma is being used as a stack by this task */
 int vma_is_stack_for_current(struct vm_area_struct *vma)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct task_struct * __maybe_unused t = current;
 
 	return (vma->vm_start <= KSTK_ESP(t) && vma->vm_end >= KSTK_ESP(t));
@@ -281,6 +313,7 @@ void arch_pick_mmap_layout(struct mm_struct *mm)
 int __weak __get_user_pages_fast(unsigned long start,
 				 int nr_pages, int write, struct page **pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 EXPORT_SYMBOL_GPL(__get_user_pages_fast);
@@ -329,14 +362,19 @@ unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,
 	ret = security_mmap_file(file, prot, flag);
 	if (!ret) {
 		if (down_write_killable(&mm->mmap_sem))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EINTR;
+}
 		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,
 				    &populate, &uf);
 		up_write(&mm->mmap_sem);
 		userfaultfd_unmap_complete(mm, &uf);
 		if (populate)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			mm_populate(ret, populate);
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -345,9 +383,13 @@ unsigned long vm_mmap(struct file *file, unsigned long addr,
 	unsigned long flag, unsigned long offset)
 {
 	if (unlikely(offset + PAGE_ALIGN(len) < offset))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	if (unlikely(offset_in_page(offset)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	return vm_mmap_pgoff(file, addr, len, prot, flag, offset >> PAGE_SHIFT);
 }
@@ -394,6 +436,7 @@ void *kvmalloc_node(size_t size, gfp_t flags, int node)
 			kmalloc_flags |= __GFP_NORETRY;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = kmalloc_node(size, kmalloc_flags, node);
 
 	/*
@@ -401,7 +444,9 @@ void *kvmalloc_node(size_t size, gfp_t flags, int node)
 	 * requests
 	 */
 	if (ret || size <= PAGE_SIZE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	return __vmalloc_node_flags_caller(size, node, flags,
 			__builtin_return_address(0));
@@ -411,7 +456,9 @@ EXPORT_SYMBOL(kvmalloc_node);
 void kvfree(const void *addr)
 {
 	if (is_vmalloc_addr(addr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		vfree(addr);
+}
 	else
 		kfree(addr);
 }
@@ -444,15 +491,26 @@ bool page_mapped(struct page *page)
 
 	if (likely(!PageCompound(page)))
 		return atomic_read(&page->_mapcount) >= 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	page = compound_head(page);
 	if (atomic_read(compound_mapcount_ptr(page)) >= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (PageHuge(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < hpage_nr_pages(page); i++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (atomic_read(&page[i]._mapcount) >= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return true;
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 EXPORT_SYMBOL(page_mapped);
@@ -464,7 +522,9 @@ struct anon_vma *page_anon_vma(struct page *page)
 	page = compound_head(page);
 	mapping = (unsigned long)page->mapping;
 	if ((mapping & PAGE_MAPPING_FLAGS) != PAGE_MAPPING_ANON)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	return __page_rmapping(page);
 }
 
@@ -476,7 +536,9 @@ struct address_space *page_mapping(struct page *page)
 
 	/* This happens if someone calls flush_dcache_page on slab page */
 	if (unlikely(PageSlab(page)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (unlikely(PageSwapCache(page))) {
 		swp_entry_t entry;
@@ -487,7 +549,9 @@ struct address_space *page_mapping(struct page *page)
 
 	mapping = page->mapping;
 	if ((unsigned long)mapping & PAGE_MAPPING_ANON)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	return (void *)((unsigned long)mapping & ~PAGE_MAPPING_FLAGS);
 }
@@ -504,11 +568,16 @@ int __page_mapcount(struct page *page)
 	 * of the page: no need to look into compound_mapcount.
 	 */
 	if (!PageAnon(page) && !PageHuge(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 	page = compound_head(page);
 	ret += atomic_read(compound_mapcount_ptr(page)) + 1;
 	if (PageDoubleMap(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret--;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 EXPORT_SYMBOL_GPL(__page_mapcount);
@@ -527,6 +596,7 @@ int overcommit_ratio_handler(struct ctl_table *table, int write,
 	int ret;
 
 	ret = proc_dointvec(table, write, buffer, lenp, ppos);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret == 0 && write)
 		sysctl_overcommit_kbytes = 0;
 	return ret;
@@ -539,6 +609,7 @@ int overcommit_kbytes_handler(struct ctl_table *table, int write,
 	int ret;
 
 	ret = proc_doulongvec_minmax(table, write, buffer, lenp, ppos);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret == 0 && write)
 		sysctl_overcommit_ratio = 0;
 	return ret;
@@ -552,7 +623,9 @@ unsigned long vm_commit_limit(void)
 	unsigned long allowed;
 
 	if (sysctl_overcommit_kbytes)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		allowed = sysctl_overcommit_kbytes >> (PAGE_SHIFT - 10);
+}
 	else
 		allowed = ((totalram_pages - hugetlb_total_pages())
 			   * sysctl_overcommit_ratio / 100);
@@ -577,6 +650,7 @@ struct percpu_counter vm_committed_as ____cacheline_aligned_in_smp;
  */
 unsigned long vm_memory_committed(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return percpu_counter_read_positive(&vm_committed_as);
 }
 EXPORT_SYMBOL_GPL(vm_memory_committed);
@@ -611,9 +685,12 @@ int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin)
 	 * Sometimes we want to use more memory than we have
 	 */
 	if (sysctl_overcommit_memory == OVERCOMMIT_ALWAYS)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (sysctl_overcommit_memory == OVERCOMMIT_GUESS) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		free = global_zone_page_state(NR_FREE_PAGES);
 		free += global_node_page_state(NR_FILE_PAGES);
 
@@ -650,28 +727,38 @@ int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin)
 			free -= sysctl_admin_reserve_kbytes >> (PAGE_SHIFT - 10);
 
 		if (free > pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 
 		goto error;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	allowed = vm_commit_limit();
 	/*
 	 * Reserve some for root
 	 */
 	if (!cap_sys_admin)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		allowed -= sysctl_admin_reserve_kbytes >> (PAGE_SHIFT - 10);
+}
 
 	/*
 	 * Don't let a single process grow so big a user can't recover
 	 */
 	if (mm) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		reserve = sysctl_user_reserve_kbytes >> (PAGE_SHIFT - 10);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		allowed -= min_t(long, mm->total_vm / 32, reserve);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (percpu_counter_read_positive(&vm_committed_as) < allowed)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 error:
 	vm_unacct_memory(pages);
 
@@ -695,6 +782,7 @@ int get_cmdline(struct task_struct *task, char *buffer, int buflen)
 	unsigned long arg_start, arg_end, env_start, env_end;
 	if (!mm)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!mm->arg_end)
 		goto out_mm;	/* Shh! No looking before we're done */
 
diff --git a/mm/vmacache.c b/mm/vmacache.c
index db7596e..10a4c18 100644
--- a/mm/vmacache.c
+++ b/mm/vmacache.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2014 Davidlohr Bueso.
@@ -19,6 +21,7 @@ void vmacache_flush_all(struct mm_struct *mm)
 {
 	struct task_struct *g, *p;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	count_vm_vmacache_event(VMACACHE_FULL_FLUSHES);
 
 	/*
@@ -70,8 +73,11 @@ static bool vmacache_valid(struct mm_struct *mm)
 	struct task_struct *curr;
 
 	if (!vmacache_valid_mm(mm))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	curr = current;
 	if (mm->vmacache_seqnum != curr->vmacache.seqnum) {
 		/*
@@ -82,6 +88,7 @@ static bool vmacache_valid(struct mm_struct *mm)
 		vmacache_flush(curr);
 		return false;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 
@@ -89,10 +96,13 @@ struct vm_area_struct *vmacache_find(struct mm_struct *mm, unsigned long addr)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	count_vm_vmacache_event(VMACACHE_FIND_CALLS);
 
 	if (!vmacache_valid(mm))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	for (i = 0; i < VMACACHE_SIZE; i++) {
 		struct vm_area_struct *vma = current->vmacache.vmas[i];
@@ -102,11 +112,14 @@ struct vm_area_struct *vmacache_find(struct mm_struct *mm, unsigned long addr)
 		if (WARN_ON_ONCE(vma->vm_mm != mm))
 			break;
 		if (vma->vm_start <= addr && vma->vm_end > addr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			count_vm_vmacache_event(VMACACHE_FIND_HITS);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return vma;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
diff --git a/mm/vmalloc.c b/mm/vmalloc.c
index ebff729..090b1af 100644
--- a/mm/vmalloc.c
+++ b/mm/vmalloc.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/vmalloc.c
  *
@@ -48,6 +50,7 @@ static void __vunmap(const void *, int);
 
 static void free_work(struct work_struct *w)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct vfree_deferred *p = container_of(w, struct vfree_deferred, wq);
 	struct llist_node *t, *llnode;
 
@@ -63,6 +66,7 @@ static void vunmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end)
 
 	pte = pte_offset_kernel(pmd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pte_t ptent = ptep_get_and_clear(&init_mm, addr, pte);
 		WARN_ON(!pte_none(ptent) && !pte_present(ptent));
 	} while (pte++, addr += PAGE_SIZE, addr != end);
@@ -107,12 +111,14 @@ static void vunmap_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end)
 
 	p4d = p4d_offset(pgd, addr);
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (p4d_clear_huge(p4d))
 			continue;
 		if (p4d_none_or_clear_bad(p4d))
 			continue;
 		vunmap_pud_range(p4d, addr, next);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (p4d++, addr = next, addr != end);
 }
 
@@ -143,17 +149,24 @@ static int vmap_pte_range(pmd_t *pmd, unsigned long addr,
 
 	pte = pte_alloc_kernel(pmd, addr);
 	if (!pte)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
 		struct page *page = pages[*nr];
 
 		if (WARN_ON(!pte_none(*pte)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EBUSY;
+}
 		if (WARN_ON(!page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 		set_pte_at(&init_mm, addr, pte, mk_pte(page, prot));
 		(*nr)++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -165,12 +178,17 @@ static int vmap_pmd_range(pud_t *pud, unsigned long addr,
 
 	pmd = pmd_alloc(&init_mm, pud, addr);
 	if (!pmd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
 		next = pmd_addr_end(addr, end);
 		if (vmap_pte_range(pmd, addr, next, prot, pages, nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 	} while (pmd++, addr = next, addr != end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -182,12 +200,17 @@ static int vmap_pud_range(p4d_t *p4d, unsigned long addr,
 
 	pud = pud_alloc(&init_mm, p4d, addr);
 	if (!pud)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
 		next = pud_addr_end(addr, end);
 		if (vmap_pmd_range(pud, addr, next, prot, pages, nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 	} while (pud++, addr = next, addr != end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -199,11 +222,17 @@ static int vmap_p4d_range(pgd_t *pgd, unsigned long addr,
 
 	p4d = p4d_alloc(&init_mm, pgd, addr);
 	if (!p4d)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		next = p4d_addr_end(addr, end);
 		if (vmap_pud_range(p4d, addr, next, prot, pages, nr))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (p4d++, addr = next, addr != end);
 	return 0;
 }
@@ -229,7 +258,9 @@ static int vmap_page_range_noflush(unsigned long start, unsigned long end,
 		next = pgd_addr_end(addr, end);
 		err = vmap_p4d_range(pgd, addr, next, prot, pages, &nr);
 		if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return err;
+}
 	} while (pgd++, addr = next, addr != end);
 
 	return nr;
@@ -241,7 +272,9 @@ static int vmap_page_range(unsigned long start, unsigned long end,
 	int ret;
 
 	ret = vmap_page_range_noflush(start, end, prot, pages);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_cache_vmap(start, end);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -254,6 +287,7 @@ int is_vmalloc_or_module_addr(const void *x)
 	 */
 #if defined(CONFIG_MODULES) && defined(MODULES_VADDR)
 	unsigned long addr = (unsigned long)x;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (addr >= MODULES_VADDR && addr < MODULES_END)
 		return 1;
 #endif
@@ -279,11 +313,17 @@ struct page *vmalloc_to_page(const void *vmalloc_addr)
 	 */
 	VIRTUAL_BUG_ON(!is_vmalloc_or_module_addr(vmalloc_addr));
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (pgd_none(*pgd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	p4d = p4d_offset(pgd, addr);
 	if (p4d_none(*p4d))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	pud = pud_offset(p4d, addr);
 
 	/*
@@ -296,16 +336,21 @@ struct page *vmalloc_to_page(const void *vmalloc_addr)
 	 */
 	WARN_ON_ONCE(pud_bad(*pud));
 	if (pud_none(*pud) || pud_bad(*pud))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	pmd = pmd_offset(pud, addr);
 	WARN_ON_ONCE(pmd_bad(*pmd));
 	if (pmd_none(*pmd) || pmd_bad(*pmd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	ptep = pte_offset_map(pmd, addr);
 	pte = *ptep;
 	if (pte_present(pte))
 		page = pte_page(pte);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pte_unmap(ptep);
 	return page;
 }
@@ -316,6 +361,7 @@ EXPORT_SYMBOL(vmalloc_to_page);
  */
 unsigned long vmalloc_to_pfn(const void *vmalloc_addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return page_to_pfn(vmalloc_to_page(vmalloc_addr));
 }
 EXPORT_SYMBOL(vmalloc_to_pfn);
@@ -356,6 +402,7 @@ static struct vmap_area *__find_vmap_area(unsigned long addr)
 			return va;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -369,6 +416,7 @@ static void __insert_vmap_area(struct vmap_area *va)
 		struct vmap_area *tmp_va;
 
 		parent = *p;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		tmp_va = rb_entry(parent, struct vmap_area, rb_node);
 		if (va->va_start < tmp_va->va_end)
 			p = &(*p)->rb_left;
@@ -385,6 +433,7 @@ static void __insert_vmap_area(struct vmap_area *va)
 	tmp = rb_prev(&va->rb_node);
 	if (tmp) {
 		struct vmap_area *prev;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		prev = rb_entry(tmp, struct vmap_area, rb_node);
 		list_add_rcu(&va->list, &prev->list);
 	} else
@@ -419,7 +468,9 @@ static struct vmap_area *alloc_vmap_area(unsigned long size,
 	va = kmalloc_node(sizeof(struct vmap_area),
 			gfp_mask & GFP_RECLAIM_MASK, node);
 	if (unlikely(!va))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	/*
 	 * Only scan the relevant parts containing pointers to other objects
@@ -471,6 +522,7 @@ static struct vmap_area *alloc_vmap_area(unsigned long size,
 			struct vmap_area *tmp;
 			tmp = rb_entry(n, struct vmap_area, rb_node);
 			if (tmp->va_end >= addr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				first = tmp;
 				if (tmp->va_start <= addr)
 					break;
@@ -512,40 +564,49 @@ static struct vmap_area *alloc_vmap_area(unsigned long size,
 	BUG_ON(va->va_start < vstart);
 	BUG_ON(va->va_end > vend);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return va;
 
 overflow:
 	spin_unlock(&vmap_area_lock);
 	if (!purged) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		purge_vmap_area_lazy();
 		purged = 1;
 		goto retry;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (gfpflags_allow_blocking(gfp_mask)) {
 		unsigned long freed = 0;
 		blocking_notifier_call_chain(&vmap_notify_list, 0, &freed);
 		if (freed > 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			purged = 0;
 			goto retry;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!(gfp_mask & __GFP_NOWARN) && printk_ratelimit())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("vmap allocation for size %lu failed: use vmalloc=<size> to increase size\n",
 			size);
+}
 	kfree(va);
 	return ERR_PTR(-EBUSY);
 }
 
 int register_vmap_purge_notifier(struct notifier_block *nb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return blocking_notifier_chain_register(&vmap_notify_list, nb);
 }
 EXPORT_SYMBOL_GPL(register_vmap_purge_notifier);
 
 int unregister_vmap_purge_notifier(struct notifier_block *nb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return blocking_notifier_chain_unregister(&vmap_notify_list, nb);
 }
 EXPORT_SYMBOL_GPL(unregister_vmap_purge_notifier);
@@ -559,6 +620,7 @@ static void __free_vmap_area(struct vmap_area *va)
 			free_vmap_cache = NULL;
 		} else {
 			struct vmap_area *cache;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			cache = rb_entry(free_vmap_cache, struct vmap_area, rb_node);
 			if (va->va_start <= cache->va_start) {
 				free_vmap_cache = rb_prev(&va->rb_node);
@@ -590,6 +652,7 @@ static void __free_vmap_area(struct vmap_area *va)
  */
 static void free_vmap_area(struct vmap_area *va)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&vmap_area_lock);
 	__free_vmap_area(va);
 	spin_unlock(&vmap_area_lock);
@@ -666,6 +729,7 @@ static void purge_fragmented_blocks_allcpus(void);
  */
 void set_iounmap_nonlazy(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	atomic_set(&vmap_lazy_nr, lazy_max_pages()+1);
 }
 
@@ -679,19 +743,28 @@ static bool __purge_vmap_area_lazy(unsigned long start, unsigned long end)
 	struct vmap_area *n_va;
 	bool do_free = false;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&vmap_purge_lock);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	valist = llist_del_all(&vmap_purge_list);
 	llist_for_each_entry(va, valist, purge_list) {
 		if (va->va_start < start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			start = va->va_start;
+}
 		if (va->va_end > end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			end = va->va_end;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		do_free = true;
 	}
 
 	if (!do_free)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	flush_tlb_kernel_range(start, end);
 
@@ -703,6 +776,7 @@ static bool __purge_vmap_area_lazy(unsigned long start, unsigned long end)
 		atomic_sub(nr, &vmap_lazy_nr);
 		cond_resched_lock(&vmap_area_lock);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&vmap_area_lock);
 	return true;
 }
@@ -724,6 +798,7 @@ static void try_purge_vmap_area_lazy(void)
  */
 static void purge_vmap_area_lazy(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mutex_lock(&vmap_purge_lock);
 	purge_fragmented_blocks_allcpus();
 	__purge_vmap_area_lazy(ULONG_MAX, 0);
@@ -754,6 +829,7 @@ static void free_vmap_area_noflush(struct vmap_area *va)
  */
 static void free_unmap_vmap_area(struct vmap_area *va)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_cache_vunmap(va->va_start, va->va_end);
 	unmap_vmap_area(va);
 	free_vmap_area_noflush(va);
@@ -837,6 +913,7 @@ static RADIX_TREE(vmap_block_tree, GFP_ATOMIC);
 
 static unsigned long addr_to_vb_idx(unsigned long addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	addr -= VMALLOC_START & ~(VMAP_BLOCK_SIZE-1);
 	addr /= VMAP_BLOCK_SIZE;
 	return addr;
@@ -847,6 +924,7 @@ static void *vmap_block_vaddr(unsigned long va_start, unsigned long pages_off)
 	unsigned long addr;
 
 	addr = va_start + (pages_off << PAGE_SHIFT);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(addr_to_vb_idx(addr) != addr_to_vb_idx(va_start));
 	return (void *)addr;
 }
@@ -873,7 +951,9 @@ static void *new_vmap_block(unsigned int order, gfp_t gfp_mask)
 	vb = kmalloc_node(sizeof(struct vmap_block),
 			gfp_mask & GFP_RECLAIM_MASK, node);
 	if (unlikely(!vb))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	va = alloc_vmap_area(VMAP_BLOCK_SIZE, VMAP_BLOCK_SIZE,
 					VMALLOC_START, VMALLOC_END,
@@ -926,6 +1006,7 @@ static void free_vmap_block(struct vmap_block *vb)
 	spin_lock(&vmap_block_tree_lock);
 	tmp = radix_tree_delete(&vmap_block_tree, vb_idx);
 	spin_unlock(&vmap_block_tree_lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(tmp != vb);
 
 	free_vmap_area_noflush(vb->va);
@@ -942,14 +1023,20 @@ static void purge_fragmented_blocks(int cpu)
 	rcu_read_lock();
 	list_for_each_entry_rcu(vb, &vbq->free, free_list) {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!(vb->free + vb->dirty == VMAP_BBMAP_BITS && vb->dirty != VMAP_BBMAP_BITS))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock(&vb->lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (vb->free + vb->dirty == VMAP_BBMAP_BITS && vb->dirty != VMAP_BBMAP_BITS) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vb->free = 0; /* prevent further allocs after releasing lock */
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vb->dirty = VMAP_BBMAP_BITS; /* prevent purging it again */
 			vb->dirty_min = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			vb->dirty_max = VMAP_BBMAP_BITS;
 			spin_lock(&vbq->lock);
 			list_del_rcu(&vb->free_list);
@@ -957,11 +1044,15 @@ static void purge_fragmented_blocks(int cpu)
 			spin_unlock(&vb->lock);
 			list_add_tail(&vb->purge, &purge);
 		} else
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(&vb->lock);
+}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_unlock();
 
 	list_for_each_entry_safe(vb, n_vb, &purge, purge) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		list_del(&vb->purge);
 		free_vmap_block(vb);
 	}
@@ -982,6 +1073,7 @@ static void *vb_alloc(unsigned long size, gfp_t gfp_mask)
 	void *vaddr = NULL;
 	unsigned int order;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(offset_in_page(size));
 	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);
 	if (WARN_ON(size == 0)) {
@@ -1035,6 +1127,7 @@ static void vb_free(const void *addr, unsigned long size)
 	unsigned int order;
 	struct vmap_block *vb;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(offset_in_page(size));
 	BUG_ON(size > PAGE_SIZE*VMAP_MAX_ALLOC);
 
@@ -1088,7 +1181,9 @@ void vm_unmap_aliases(void)
 	int flush = 0;
 
 	if (unlikely(!vmap_initialized))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	might_sleep();
 
@@ -1098,6 +1193,7 @@ void vm_unmap_aliases(void)
 
 		rcu_read_lock();
 		list_for_each_entry_rcu(vb, &vbq->free, free_list) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_lock(&vb->lock);
 			if (vb->dirty) {
 				unsigned long va_start = vb->va->va_start;
@@ -1106,20 +1202,26 @@ void vm_unmap_aliases(void)
 				s = va_start + (vb->dirty_min << PAGE_SHIFT);
 				e = va_start + (vb->dirty_max << PAGE_SHIFT);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				start = min(s, start);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				end   = max(e, end);
 
 				flush = 1;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(&vb->lock);
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rcu_read_unlock();
 	}
 
 	mutex_lock(&vmap_purge_lock);
 	purge_fragmented_blocks_allcpus();
 	if (!__purge_vmap_area_lazy(start, end) && flush)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		flush_tlb_kernel_range(start, end);
+}
 	mutex_unlock(&vmap_purge_lock);
 }
 EXPORT_SYMBOL_GPL(vm_unmap_aliases);
@@ -1135,6 +1237,7 @@ void vm_unmap_ram(const void *mem, unsigned int count)
 	unsigned long addr = (unsigned long)mem;
 	struct vmap_area *va;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	might_sleep();
 	BUG_ON(!addr);
 	BUG_ON(addr < VMALLOC_START);
@@ -1177,6 +1280,7 @@ void *vm_map_ram(struct page **pages, unsigned int count, int node, pgprot_t pro
 	void *mem;
 
 	if (likely(count <= VMAP_MAX_ALLOC)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mem = vb_alloc(size, GFP_KERNEL);
 		if (IS_ERR(mem))
 			return NULL;
@@ -1214,6 +1318,7 @@ void __init vm_area_add_early(struct vm_struct *vm)
 {
 	struct vm_struct *tmp, **p;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(vmap_initialized);
 	for (p = &vmlist; (tmp = *p) != NULL; p = &tmp->next) {
 		if (tmp->addr >= vm->addr) {
@@ -1271,6 +1376,7 @@ void __init vmalloc_init(void)
 
 	/* Import existing vmlist entries. */
 	for (tmp = vmlist; tmp; tmp = tmp->next) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		va = kzalloc(sizeof(struct vmap_area), GFP_NOWAIT);
 		va->flags = VM_VM_AREA;
 		va->va_start = (unsigned long)tmp->addr;
@@ -1325,6 +1431,7 @@ int map_kernel_range_noflush(unsigned long addr, unsigned long size,
  */
 void unmap_kernel_range_noflush(unsigned long addr, unsigned long size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	vunmap_page_range(addr, addr + size);
 }
 EXPORT_SYMBOL_GPL(unmap_kernel_range_noflush);
@@ -1341,6 +1448,7 @@ void unmap_kernel_range(unsigned long addr, unsigned long size)
 {
 	unsigned long end = addr + size;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	flush_cache_vunmap(addr, end);
 	vunmap_page_range(addr, end);
 	flush_tlb_kernel_range(addr, end);
@@ -1362,6 +1470,7 @@ EXPORT_SYMBOL_GPL(map_vm_area);
 static void setup_vmalloc_vm(struct vm_struct *vm, struct vmap_area *va,
 			      unsigned long flags, const void *caller)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&vmap_area_lock);
 	vm->flags = flags;
 	vm->addr = (void *)va->va_start;
@@ -1393,7 +1502,9 @@ static struct vm_struct *__get_vm_area_node(unsigned long size,
 	BUG_ON(in_interrupt());
 	size = PAGE_ALIGN(size);
 	if (unlikely(!size))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (flags & VM_IOREMAP)
 		align = 1ul << clamp_t(int, get_count_order_long(size),
@@ -1401,13 +1512,16 @@ static struct vm_struct *__get_vm_area_node(unsigned long size,
 
 	area = kzalloc_node(sizeof(*area), gfp_mask & GFP_RECLAIM_MASK, node);
 	if (unlikely(!area))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (!(flags & VM_NO_GUARD))
 		size += PAGE_SIZE;
 
 	va = alloc_vmap_area(size, align, start, end, node, gfp_mask);
 	if (IS_ERR(va)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(area);
 		return NULL;
 	}
@@ -1429,6 +1543,7 @@ struct vm_struct *__get_vm_area_caller(unsigned long size, unsigned long flags,
 				       unsigned long start, unsigned long end,
 				       const void *caller)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __get_vm_area_node(size, 1, flags, start, end, NUMA_NO_NODE,
 				  GFP_KERNEL, caller);
 }
@@ -1472,6 +1587,7 @@ struct vm_struct *find_vm_area(const void *addr)
 	if (va && va->flags & VM_VM_AREA)
 		return va->vm;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1505,6 +1621,7 @@ struct vm_struct *remove_vm_area(const void *addr)
 
 		return vm;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1513,7 +1630,9 @@ static void __vunmap(const void *addr, int deallocate_pages)
 	struct vm_struct *area;
 
 	if (!addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (WARN(!PAGE_ALIGNED(addr), "Trying to vfree() bad address (%p)\n",
 			addr))
@@ -1521,11 +1640,13 @@ static void __vunmap(const void *addr, int deallocate_pages)
 
 	area = remove_vm_area(addr);
 	if (unlikely(!area)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN(1, KERN_ERR "Trying to vfree() nonexistent vm area (%p)\n",
 				addr);
 		return;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	debug_check_no_locks_freed(addr, get_vm_area_size(area));
 	debug_check_no_obj_freed(addr, get_vm_area_size(area));
 
@@ -1571,10 +1692,13 @@ void vfree_atomic(const void *addr)
 {
 	BUG_ON(in_nmi());
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kmemleak_free(addr);
 
 	if (!addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	__vfree_deferred(addr);
 }
 
@@ -1596,12 +1720,17 @@ void vfree(const void *addr)
 {
 	BUG_ON(in_nmi());
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kmemleak_free(addr);
 
 	if (!addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (unlikely(in_interrupt()))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__vfree_deferred(addr);
+}
 	else
 		__vunmap(addr, 1);
 }
@@ -1618,6 +1747,7 @@ EXPORT_SYMBOL(vfree);
  */
 void vunmap(const void *addr)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(in_interrupt());
 	might_sleep();
 	if (addr)
@@ -1641,6 +1771,7 @@ void *vmap(struct page **pages, unsigned int count,
 	struct vm_struct *area;
 	unsigned long size;		/* In bytes */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	might_sleep();
 
 	if (count > totalram_pages)
@@ -1680,6 +1811,7 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,
 	area->nr_pages = nr_pages;
 	/* Please note that the recursion is strictly bounded. */
 	if (array_size > PAGE_SIZE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pages = __vmalloc_node(array_size, 1, nested_gfp|highmem_mask,
 				PAGE_KERNEL, node, area->caller);
 	} else {
@@ -1687,6 +1819,7 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,
 	}
 	area->pages = pages;
 	if (!area->pages) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		remove_vm_area(area->addr);
 		kfree(area);
 		return NULL;
@@ -1758,7 +1891,9 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,
 
 	addr = __vmalloc_area_node(area, gfp_mask, prot, node);
 	if (!addr)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	/*
 	 * In this function, newly allocated vm_struct has VM_UNINITIALIZED
@@ -1823,6 +1958,7 @@ static inline void *__vmalloc_node_flags(unsigned long size,
 void *__vmalloc_node_flags_caller(unsigned long size, int node, gfp_t flags,
 				  void *caller)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __vmalloc_node(size, 1, flags, PAGE_KERNEL, node, caller);
 }
 
@@ -1876,6 +2012,7 @@ void *vmalloc_user(unsigned long size)
 			     PAGE_KERNEL, NUMA_NO_NODE,
 			     __builtin_return_address(0));
 	if (ret) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		area = find_vm_area(ret);
 		area->flags |= VM_USERMAP;
 	}
@@ -1915,6 +2052,7 @@ EXPORT_SYMBOL(vmalloc_node);
  */
 void *vzalloc_node(unsigned long size, int node)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __vmalloc_node_flags(size, node,
 			 GFP_KERNEL | __GFP_ZERO);
 }
@@ -1983,6 +2121,7 @@ void *vmalloc_32_user(unsigned long size)
 	ret = __vmalloc_node(size, 1, GFP_VMALLOC32 | __GFP_ZERO, PAGE_KERNEL,
 			     NUMA_NO_NODE, __builtin_return_address(0));
 	if (ret) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		area = find_vm_area(ret);
 		area->flags |= VM_USERMAP;
 	}
@@ -2000,6 +2139,7 @@ static int aligned_vread(char *buf, char *addr, unsigned long count)
 	struct page *p;
 	int copied = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (count) {
 		unsigned long offset, length;
 
@@ -2039,6 +2179,7 @@ static int aligned_vwrite(char *buf, char *addr, unsigned long count)
 	struct page *p;
 	int copied = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (count) {
 		unsigned long offset, length;
 
@@ -2107,7 +2248,9 @@ long vread(char *buf, char *addr, unsigned long count)
 
 	/* Don't allow overflow */
 	if ((unsigned long) addr + count < count)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		count = -(unsigned long) addr;
+}
 
 	spin_lock(&vmap_area_lock);
 	list_for_each_entry(va, &vmap_area_list, list) {
@@ -2188,7 +2331,9 @@ long vwrite(char *buf, char *addr, unsigned long count)
 
 	/* Don't allow overflow */
 	if ((unsigned long) addr + count < count)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		count = -(unsigned long) addr;
+}
 	buflen = count;
 
 	spin_lock(&vmap_area_lock);
@@ -2251,6 +2396,7 @@ int remap_vmalloc_range_partial(struct vm_area_struct *vma, unsigned long uaddr,
 
 	size = PAGE_ALIGN(size);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!PAGE_ALIGNED(uaddr) || !PAGE_ALIGNED(kaddr))
 		return -EINVAL;
 
@@ -2300,6 +2446,7 @@ EXPORT_SYMBOL(remap_vmalloc_range_partial);
 int remap_vmalloc_range(struct vm_area_struct *vma, void *addr,
 						unsigned long pgoff)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return remap_vmalloc_range_partial(vma, vma->vm_start,
 					   addr + (pgoff << PAGE_SHIFT),
 					   vma->vm_end - vma->vm_start);
@@ -2320,6 +2467,7 @@ static int f(pte_t *pte, pgtable_t table, unsigned long addr, void *data)
 	pte_t ***p = data;
 
 	if (p) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*(*p) = pte;
 		(*p)++;
 	}
@@ -2347,7 +2495,9 @@ struct vm_struct *alloc_vm_area(size_t size, pte_t **ptes)
 	area = get_vm_area_caller(size, VM_IOREMAP,
 				__builtin_return_address(0));
 	if (area == NULL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	/*
 	 * This ensures that page tables are constructed for this region
@@ -2367,6 +2517,7 @@ void free_vm_area(struct vm_struct *area)
 {
 	struct vm_struct *ret;
 	ret = remove_vm_area(area->addr);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(ret != area);
 	kfree(area);
 }
@@ -2400,7 +2551,9 @@ static bool pvm_find_next_prev(unsigned long end,
 	while (n) {
 		va = rb_entry(n, struct vmap_area, rb_node);
 		if (end < va->va_end)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			n = n->rb_left;
+}
 		else if (end > va->va_end)
 			n = n->rb_right;
 		else
@@ -2408,15 +2561,19 @@ static bool pvm_find_next_prev(unsigned long end,
 	}
 
 	if (!va)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	if (va->va_end > end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*pnext = va;
 		*pprev = node_to_va(rb_prev(&(*pnext)->rb_node));
 	} else {
 		*pprev = va;
 		*pnext = node_to_va(rb_next(&(*pprev)->rb_node));
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 
@@ -2444,11 +2601,14 @@ static unsigned long pvm_determine_end(struct vmap_area **pnext,
 	unsigned long addr;
 
 	if (*pnext)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		addr = min((*pnext)->va_start & ~(align - 1), vmalloc_end);
+}
 	else
 		addr = vmalloc_end;
 
 	while (*pprev && (*pprev)->va_end > addr) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		*pnext = *pprev;
 		*pprev = node_to_va(rb_prev(&(*pnext)->rb_node));
 	}
@@ -2504,18 +2664,22 @@ struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 
 		/* detect the area with the highest address */
 		if (start > offsets[last_area])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			last_area = area;
+}
 
 		for (area2 = area + 1; area2 < nr_vms; area2++) {
 			unsigned long start2 = offsets[area2];
 			unsigned long end2 = start2 + sizes[area2];
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			BUG_ON(start2 < end && start < end2);
 		}
 	}
 	last_end = offsets[last_area] + sizes[last_area];
 
 	if (vmalloc_end - vmalloc_start < last_end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON(true);
 		return NULL;
 	}
@@ -2540,11 +2704,13 @@ struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 	end = start + sizes[area];
 
 	if (!pvm_find_next_prev(vmap_area_pcpu_hole, &next, &prev)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		base = vmalloc_end - last_end;
 		goto found;
 	}
 	base = pvm_determine_end(&next, &prev, align) - end;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (true) {
 		BUG_ON(next && next->va_end <= base + end);
 		BUG_ON(prev && prev->va_end > base + end);
@@ -2554,8 +2720,10 @@ struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 		 * comparing.
 		 */
 		if (base + last_end < vmalloc_start + last_end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			spin_unlock(&vmap_area_lock);
 			if (!purged) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				purge_vmap_area_lazy();
 				purged = true;
 				goto retry;
@@ -2568,6 +2736,7 @@ struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 		 * right below next and then recheck.
 		 */
 		if (next && next->va_start < base + end) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			base = pvm_determine_end(&next, &prev, align) - end;
 			term_area = area;
 			continue;
@@ -2579,6 +2748,7 @@ struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 		 * recheck.
 		 */
 		if (prev && prev->va_end > base + start)  {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			next = prev;
 			prev = node_to_va(rb_prev(&next->rb_node));
 			base = pvm_determine_end(&next, &prev, align) - end;
@@ -2593,6 +2763,7 @@ struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 		area = (area + nr_vms - 1) % nr_vms;
 		if (area == term_area)
 			break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		start = offsets[area];
 		end = start + sizes[area];
 		pvm_find_next_prev(base + end, &next, &prev);
@@ -2621,6 +2792,7 @@ struct vm_struct **pcpu_get_vm_areas(const unsigned long *offsets,
 
 err_free:
 	for (area = 0; area < nr_vms; area++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(vas[area]);
 		kfree(vms[area]);
 	}
@@ -2641,6 +2813,7 @@ void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < nr_vms; i++)
 		free_vm_area(vms[i]);
 	kfree(vms);
@@ -2651,23 +2824,27 @@ void pcpu_free_vm_areas(struct vm_struct **vms, int nr_vms)
 static void *s_start(struct seq_file *m, loff_t *pos)
 	__acquires(&vmap_area_lock)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&vmap_area_lock);
 	return seq_list_start(&vmap_area_list, *pos);
 }
 
 static void *s_next(struct seq_file *m, void *p, loff_t *pos)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_list_next(p, &vmap_area_list, pos);
 }
 
 static void s_stop(struct seq_file *m, void *p)
 	__releases(&vmap_area_lock)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&vmap_area_lock);
 }
 
 static void show_numa_info(struct seq_file *m, struct vm_struct *v)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (IS_ENABLED(CONFIG_NUMA)) {
 		unsigned int nr, *counters = m->private;
 
@@ -2695,6 +2872,7 @@ static int s_show(struct seq_file *m, void *p)
 	struct vmap_area *va;
 	struct vm_struct *v;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	va = list_entry(p, struct vmap_area, list);
 
 	/*
@@ -2753,6 +2931,7 @@ static const struct seq_operations vmalloc_op = {
 
 static int vmalloc_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (IS_ENABLED(CONFIG_NUMA))
 		return seq_open_private(file, &vmalloc_op,
 					nr_node_ids * sizeof(unsigned int));
diff --git a/mm/vmpressure.c b/mm/vmpressure.c
index 85350ce..609a1fe 100644
--- a/mm/vmpressure.c
+++ b/mm/vmpressure.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Linux VM pressure
  *
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 441f346..d9e9728 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  linux/mm/vmscan.c
@@ -193,11 +195,13 @@ static bool sane_reclaim(struct scan_control *sc)
 #else
 static bool global_reclaim(struct scan_control *sc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 
 static bool sane_reclaim(struct scan_control *sc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 #endif
@@ -214,8 +218,10 @@ unsigned long zone_reclaimable_pages(struct zone *zone)
 	nr = zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_FILE) +
 		zone_page_state_snapshot(zone, NR_ZONE_ACTIVE_FILE);
 	if (get_nr_swap_pages() > 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr += zone_page_state_snapshot(zone, NR_ZONE_INACTIVE_ANON) +
 			zone_page_state_snapshot(zone, NR_ZONE_ACTIVE_ANON);
+}
 
 	return nr;
 }
@@ -229,9 +235,11 @@ unsigned long pgdat_reclaimable_pages(struct pglist_data *pgdat)
 	     node_page_state_snapshot(pgdat, NR_ISOLATED_FILE);
 
 	if (get_nr_swap_pages() > 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr += node_page_state_snapshot(pgdat, NR_ACTIVE_ANON) +
 		      node_page_state_snapshot(pgdat, NR_INACTIVE_ANON) +
 		      node_page_state_snapshot(pgdat, NR_ISOLATED_ANON);
+}
 
 	return nr;
 }
@@ -248,7 +256,9 @@ unsigned long lruvec_lru_size(struct lruvec *lruvec, enum lru_list lru, int zone
 	int zid;
 
 	if (!mem_cgroup_disabled())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		lru_size = mem_cgroup_get_lru_size(lruvec, lru);
+}
 	else
 		lru_size = node_page_state(lruvec_pgdat(lruvec), NR_LRU_BASE + lru);
 
@@ -279,11 +289,15 @@ int register_shrinker(struct shrinker *shrinker)
 	size_t size = sizeof(*shrinker->nr_deferred);
 
 	if (shrinker->flags & SHRINKER_NUMA_AWARE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		size *= nr_node_ids;
+}
 
 	shrinker->nr_deferred = kzalloc(size, GFP_KERNEL);
 	if (!shrinker->nr_deferred)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	down_write(&shrinker_rwsem);
 	list_add_tail(&shrinker->list, &shrinker_list);
@@ -298,7 +312,9 @@ EXPORT_SYMBOL(register_shrinker);
 void unregister_shrinker(struct shrinker *shrinker)
 {
 	if (!shrinker->nr_deferred)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	down_write(&shrinker_rwsem);
 	list_del(&shrinker->list);
 	up_write(&shrinker_rwsem);
@@ -321,6 +337,7 @@ static unsigned long do_shrink_slab(struct shrink_control *shrinkctl,
 	long nr;
 	long new_nr;
 	int nid = shrinkctl->nid;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	long batch_size = shrinker->batch ? shrinker->batch
 					  : SHRINK_BATCH;
 	long scanned = 0, next_deferred;
@@ -465,6 +482,7 @@ static unsigned long shrink_slab(gfp_t gfp_mask, int nid,
 	struct shrinker *shrinker;
 	unsigned long freed = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (memcg && (!memcg_kmem_enabled() || !mem_cgroup_online(memcg)))
 		return 0;
 
@@ -519,6 +537,7 @@ void drop_slab_node(int nid)
 
 		freed = 0;
 		do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			freed += shrink_slab(GFP_KERNEL, nid, memcg,
 					     1000, 1000);
 		} while ((memcg = mem_cgroup_iter(NULL, memcg, NULL)) != NULL);
@@ -529,6 +548,7 @@ void drop_slab(void)
 {
 	int nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_online_node(nid)
 		drop_slab_node(nid);
 }
@@ -547,6 +567,7 @@ static inline int is_page_cache_freeable(struct page *page)
 
 static int may_write_to_inode(struct inode *inode, struct scan_control *sc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (current->flags & PF_SWAPWRITE)
 		return 1;
 	if (!inode_write_congested(inode))
@@ -571,6 +592,7 @@ static int may_write_to_inode(struct inode *inode, struct scan_control *sc)
 static void handle_write_error(struct address_space *mapping,
 				struct page *page, int error)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lock_page(page);
 	if (page_mapping(page) == mapping)
 		mapping_set_error(mapping, error);
@@ -674,6 +696,7 @@ static int __remove_mapping(struct address_space *mapping, struct page *page,
 	unsigned long flags;
 	int refcount;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!PageLocked(page));
 	BUG_ON(mapping != page_mapping(page));
 
@@ -767,6 +790,7 @@ static int __remove_mapping(struct address_space *mapping, struct page *page,
  */
 int remove_mapping(struct address_space *mapping, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (__remove_mapping(mapping, page, false)) {
 		/*
 		 * Unfreezing the refcount with 1 rather than 2 effectively
@@ -833,7 +857,9 @@ void putback_lru_page(struct page *page)
 	 * check after we added it to the list, again.
 	 */
 	if (is_unevictable && page_evictable(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!isolate_lru_page(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_page(page);
 			goto redo;
 		}
@@ -844,9 +870,13 @@ void putback_lru_page(struct page *page)
 	}
 
 	if (was_unevictable && !is_unevictable)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		count_vm_event(UNEVICTABLE_PGRESCUED);
+}
 	else if (!was_unevictable && is_unevictable)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		count_vm_event(UNEVICTABLE_PGCULLED);
+}
 
 	put_page(page);		/* drop ref from isolate */
 }
@@ -873,7 +903,9 @@ static enum page_references page_check_references(struct page *page,
 	 * move the page to the unevictable list.
 	 */
 	if (vm_flags & VM_LOCKED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PAGEREF_RECLAIM;
+}
 
 	if (referenced_ptes) {
 		if (PageSwapBacked(page))
@@ -978,6 +1010,7 @@ static unsigned long shrink_page_list(struct list_head *page_list,
 
 	cond_resched();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!list_empty(page_list)) {
 		struct address_space *mapping;
 		struct page *page;
@@ -1382,6 +1415,7 @@ unsigned long reclaim_clean_pages_from_list(struct zone *zone,
 	struct page *page, *next;
 	LIST_HEAD(clean_pages);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry_safe(page, next, page_list, lru) {
 		if (page_is_file_cache(page) && !PageDirty(page) &&
 		    !__PageMovable(page)) {
@@ -1413,7 +1447,9 @@ int __isolate_lru_page(struct page *page, isolate_mode_t mode)
 
 	/* Only take pages on the LRU. */
 	if (!PageLRU(page))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	/* Compaction should not handle unevictable pages but CMA can do so */
 	if (PageUnevictable(page) && !(mode & ISOLATE_UNEVICTABLE))
@@ -1474,6 +1510,7 @@ static __always_inline void update_lru_sizes(struct lruvec *lruvec,
 {
 	int zid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
 		if (!nr_zone_taken[zid])
 			continue;
@@ -1620,18 +1657,21 @@ int isolate_lru_page(struct page *page)
 	WARN_RATELIMIT(PageTail(page), "trying to isolate tail page");
 
 	if (PageLRU(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		struct zone *zone = page_zone(page);
 		struct lruvec *lruvec;
 
 		spin_lock_irq(zone_lru_lock(zone));
 		lruvec = mem_cgroup_page_lruvec(page, zone->zone_pgdat);
 		if (PageLRU(page)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			int lru = page_lru(page);
 			get_page(page);
 			ClearPageLRU(page);
 			del_page_from_lru_list(page, lruvec, lru);
 			ret = 0;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irq(zone_lru_lock(zone));
 	}
 	return ret;
@@ -1650,7 +1690,9 @@ static int too_many_isolated(struct pglist_data *pgdat, int file,
 	unsigned long inactive, isolated;
 
 	if (current_is_kswapd())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (!sane_reclaim(sc))
 		return 0;
@@ -1737,6 +1779,7 @@ putback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)
  */
 static int current_may_throttle(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return !(current->flags & PF_LESS_THROTTLE) ||
 		current->backing_dev_info == NULL ||
 		bdi_write_congested(current->backing_dev_info);
@@ -1761,6 +1804,7 @@ shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,
 	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
 	bool stalled = false;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (unlikely(too_many_isolated(pgdat, file, sc))) {
 		if (stalled)
 			return 0;
@@ -1929,6 +1973,7 @@ static unsigned move_active_pages_to_lru(struct lruvec *lruvec,
 				     struct list_head *pages_to_free,
 				     enum lru_list lru)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct pglist_data *pgdat = lruvec_pgdat(lruvec);
 	struct page *page;
 	int nr_pages;
@@ -1993,7 +2038,9 @@ static void shrink_active_list(unsigned long nr_to_scan,
 	lru_add_drain();
 
 	if (!sc->may_unmap)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		isolate_mode |= ISOLATE_UNMAPPED;
+}
 
 	spin_lock_irq(&pgdat->lru_lock);
 
@@ -2154,6 +2201,7 @@ static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,
 				 struct lruvec *lruvec, struct mem_cgroup *memcg,
 				 struct scan_control *sc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (is_active_lru(lru)) {
 		if (inactive_list_is_low(lruvec, is_file_lru(lru),
 					 memcg, sc, true))
@@ -2184,6 +2232,7 @@ static void get_scan_count(struct lruvec *lruvec, struct mem_cgroup *memcg,
 			   struct scan_control *sc, unsigned long *nr,
 			   unsigned long *lru_pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int swappiness = mem_cgroup_swappiness(memcg);
 	struct zone_reclaim_stat *reclaim_stat = &lruvec->reclaim_stat;
 	u64 fraction[2];
@@ -2384,6 +2433,7 @@ static void get_scan_count(struct lruvec *lruvec, struct mem_cgroup *memcg,
 static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg,
 			      struct scan_control *sc, unsigned long *lru_pages)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg);
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
@@ -2500,6 +2550,7 @@ static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memc
 /* Use reclaim/compaction for costly allocs or under memory pressure */
 static bool in_reclaim_compaction(struct scan_control *sc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (IS_ENABLED(CONFIG_COMPACTION) && sc->order &&
 			(sc->order > PAGE_ALLOC_COSTLY_ORDER ||
 			 sc->priority < DEF_PRIORITY - 2))
@@ -2526,7 +2577,9 @@ static inline bool should_continue_reclaim(struct pglist_data *pgdat,
 
 	/* If not in reclaim/compaction mode, stop */
 	if (!in_reclaim_compaction(sc))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	/* Consider stopping depending on scan and reclaim activity */
 	if (sc->gfp_mask & __GFP_RETRY_MAYFAIL) {
@@ -2583,6 +2636,7 @@ static inline bool should_continue_reclaim(struct pglist_data *pgdat,
 
 static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct reclaim_state *reclaim_state = current->reclaim_state;
 	unsigned long nr_reclaimed, nr_scanned;
 	bool reclaimable = false;
@@ -2739,6 +2793,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)
 	 */
 	orig_mask = sc->gfp_mask;
 	if (buffer_heads_over_limit) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sc->gfp_mask |= __GFP_HIGHMEM;
 		sc->reclaim_idx = gfp_zone(sc->gfp_mask);
 	}
@@ -2818,7 +2873,9 @@ static void snapshot_refaults(struct mem_cgroup *root_memcg, pg_data_t *pgdat)
 		struct lruvec *lruvec;
 
 		if (memcg)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			refaults = memcg_page_state(memcg, WORKINGSET_ACTIVATE);
+}
 		else
 			refaults = node_page_state(pgdat, WORKINGSET_ACTIVATE);
 
@@ -2914,7 +2971,9 @@ static bool allow_direct_reclaim(pg_data_t *pgdat)
 	bool wmark_ok;
 
 	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
 	for (i = 0; i <= ZONE_NORMAL; i++) {
 		zone = &pgdat->node_zones[i];
@@ -3059,7 +3118,9 @@ unsigned long try_to_free_pages(struct zonelist *zonelist, int order,
 	 * point.
 	 */
 	if (throttle_direct_reclaim(sc.gfp_mask, zonelist, nodemask))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 1;
+}
 
 	trace_mm_vmscan_direct_reclaim_begin(order,
 				sc.may_writepage,
@@ -3164,7 +3225,9 @@ static void age_active_anon(struct pglist_data *pgdat,
 	struct mem_cgroup *memcg;
 
 	if (!total_swap_pages)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	memcg = mem_cgroup_iter(NULL, NULL, NULL);
 	do {
@@ -3196,7 +3259,9 @@ static bool pgdat_balanced(pg_data_t *pgdat, int order, int classzone_idx)
 
 		mark = high_wmark_pages(zone);
 		if (zone_watermark_ok_safe(zone, order, mark, classzone_idx))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return true;
+}
 	}
 
 	/*
@@ -3205,8 +3270,11 @@ static bool pgdat_balanced(pg_data_t *pgdat, int order, int classzone_idx)
 	 * allocation tries to wake a remote kswapd.
 	 */
 	if (mark == -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -3240,17 +3308,22 @@ static bool prepare_kswapd_sleep(pg_data_t *pgdat, int order, int classzone_idx)
 	 * that here we are under prepare_to_wait().
 	 */
 	if (waitqueue_active(&pgdat->pfmemalloc_wait))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		wake_up_all(&pgdat->pfmemalloc_wait);
+}
 
 	/* Hopeless node, leave it to direct reclaim */
 	if (pgdat->kswapd_failures >= MAX_RECLAIM_RETRIES)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
 	if (pgdat_balanced(pgdat, order, classzone_idx)) {
 		clear_pgdat_congested(pgdat);
 		return true;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -3270,6 +3343,7 @@ static bool kswapd_shrink_node(pg_data_t *pgdat,
 
 	/* Reclaim a number of pages proportional to the number of zones */
 	sc->nr_to_reclaim = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (z = 0; z <= sc->reclaim_idx; z++) {
 		zone = pgdat->node_zones + z;
 		if (!managed_zone(zone))
@@ -3343,6 +3417,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)
 		 * request are balanced to avoid excessive reclaim from kswapd.
 		 */
 		if (buffer_heads_over_limit) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			for (i = MAX_NR_ZONES - 1; i >= 0; i--) {
 				zone = pgdat->node_zones + i;
 				if (!managed_zone(zone))
@@ -3438,8 +3513,11 @@ static enum zone_type kswapd_classzone_idx(pg_data_t *pgdat,
 					   enum zone_type classzone_idx)
 {
 	if (pgdat->kswapd_classzone_idx == MAX_NR_ZONES)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return classzone_idx;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return max(pgdat->kswapd_classzone_idx, classzone_idx);
 }
 
@@ -3450,7 +3528,9 @@ static void kswapd_try_to_sleep(pg_data_t *pgdat, int alloc_order, int reclaim_o
 	DEFINE_WAIT(wait);
 
 	if (freezing(current) || kthread_should_stop())
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	prepare_to_wait(&pgdat->kswapd_wait, &wait, TASK_INTERRUPTIBLE);
 
@@ -3484,7 +3564,9 @@ static void kswapd_try_to_sleep(pg_data_t *pgdat, int alloc_order, int reclaim_o
 		 * the previous request that slept prematurely.
 		 */
 		if (remaining) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pgdat->kswapd_classzone_idx = kswapd_classzone_idx(pgdat, classzone_idx);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pgdat->kswapd_order = max(pgdat->kswapd_order, reclaim_order);
 		}
 
@@ -3513,13 +3595,18 @@ static void kswapd_try_to_sleep(pg_data_t *pgdat, int alloc_order, int reclaim_o
 		if (!kthread_should_stop())
 			schedule();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set_pgdat_percpu_threshold(pgdat, calculate_pressure_threshold);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (remaining)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			count_vm_event(KSWAPD_LOW_WMARK_HIT_QUICKLY);
+}
 		else
 			count_vm_event(KSWAPD_HIGH_WMARK_HIT_QUICKLY);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	finish_wait(&pgdat->kswapd_wait, &wait);
 }
 
@@ -3613,6 +3700,7 @@ static int kswapd(void *p)
 			goto kswapd_try_sleep;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	tsk->flags &= ~(PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD);
 	current->reclaim_state = NULL;
 
@@ -3627,7 +3715,9 @@ void wakeup_kswapd(struct zone *zone, int order, enum zone_type classzone_idx)
 	pg_data_t *pgdat;
 
 	if (!managed_zone(zone))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (!cpuset_zone_allowed(zone, GFP_KERNEL | __GFP_HARDWALL))
 		return;
@@ -3699,6 +3789,7 @@ static int kswapd_cpu_online(unsigned int cpu)
 {
 	int nid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_node_state(nid, N_MEMORY) {
 		pg_data_t *pgdat = NODE_DATA(nid);
 		const struct cpumask *mask;
@@ -3722,16 +3813,20 @@ int kswapd_run(int nid)
 	int ret = 0;
 
 	if (pgdat->kswapd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	pgdat->kswapd = kthread_run(kswapd, pgdat, "kswapd%d", nid);
 	if (IS_ERR(pgdat->kswapd)) {
 		/* failure at boot is fatal */
 		BUG_ON(system_state < SYSTEM_RUNNING);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("Failed to start kswapd on node %d\n", nid);
 		ret = PTR_ERR(pgdat->kswapd);
 		pgdat->kswapd = NULL;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -3744,6 +3839,7 @@ void kswapd_stop(int nid)
 	struct task_struct *kswapd = NODE_DATA(nid)->kswapd;
 
 	if (kswapd) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kthread_stop(kswapd);
 		NODE_DATA(nid)->kswapd = NULL;
 	}
@@ -3800,6 +3896,7 @@ int sysctl_min_slab_ratio = 5;
 
 static inline unsigned long node_unmapped_file_pages(struct pglist_data *pgdat)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned long file_mapped = node_page_state(pgdat, NR_FILE_MAPPED);
 	unsigned long file_lru = node_page_state(pgdat, NR_INACTIVE_FILE) +
 		node_page_state(pgdat, NR_ACTIVE_FILE);
@@ -3825,7 +3922,9 @@ static unsigned long node_pagecache_reclaimable(struct pglist_data *pgdat)
 	 * a better estimate
 	 */
 	if (node_reclaim_mode & RECLAIM_UNMAP)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr_pagecache_reclaimable = node_page_state(pgdat, NR_FILE_PAGES);
+}
 	else
 		nr_pagecache_reclaimable = node_unmapped_file_pages(pgdat);
 
@@ -3971,6 +4070,7 @@ void check_move_unevictable_pages(struct page **pages, int nr_pages)
 	int pgrescued = 0;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < nr_pages; i++) {
 		struct page *page = pages[i];
 		struct pglist_data *pagepgdat = page_pgdat(page);
diff --git a/mm/vmstat.c b/mm/vmstat.c
index 4bb13e7..9e588db 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  linux/mm/vmstat.c
  *
@@ -58,6 +60,7 @@ static void sum_vm_events(unsigned long *ret)
 */
 void all_vm_events(unsigned long *ret)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	get_online_cpus();
 	sum_vm_events(ret);
 	put_online_cpus();
@@ -72,6 +75,7 @@ EXPORT_SYMBOL_GPL(all_vm_events);
  */
 void vm_events_fold_cpu(int cpu)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct vm_event_state *fold_state = &per_cpu(vm_event_states, cpu);
 	int i;
 
@@ -111,6 +115,7 @@ int calculate_pressure_threshold(struct zone *zone)
 	 * the min watermark
 	 */
 	watermark_distance = low_wmark_pages(zone) - min_wmark_pages(zone);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	threshold = max(1, (int)(watermark_distance / num_online_cpus()));
 
 	/*
@@ -211,8 +216,10 @@ void refresh_zone_stat_thresholds(void)
 		tolerate_drift = low_wmark_pages(zone) - min_wmark_pages(zone);
 		max_drift = num_online_cpus() * threshold;
 		if (max_drift > tolerate_drift)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			zone->percpu_drift_mark = high_wmark_pages(zone) +
 					max_drift;
+}
 	}
 }
 
@@ -229,7 +236,9 @@ void set_pgdat_percpu_threshold(pg_data_t *pgdat,
 		if (!zone->percpu_drift_mark)
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		threshold = (*calculate_pressure)(zone);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for_each_online_cpu(cpu)
 			per_cpu_ptr(zone->pageset, cpu)->stat_threshold
 							= threshold;
@@ -310,6 +319,7 @@ void __inc_zone_state(struct zone *zone, enum zone_stat_item item)
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	v = __this_cpu_inc_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v > t)) {
@@ -338,6 +348,7 @@ void __inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
 
 void __inc_zone_page_state(struct page *page, enum zone_stat_item item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__inc_zone_state(page_zone(page), item);
 }
 EXPORT_SYMBOL(__inc_zone_page_state);
@@ -354,6 +365,7 @@ void __dec_zone_state(struct zone *zone, enum zone_stat_item item)
 	s8 __percpu *p = pcp->vm_stat_diff + item;
 	s8 v, t;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	v = __this_cpu_dec_return(*p);
 	t = __this_cpu_read(pcp->stat_threshold);
 	if (unlikely(v < - t)) {
@@ -382,6 +394,7 @@ void __dec_node_state(struct pglist_data *pgdat, enum node_stat_item item)
 
 void __dec_zone_page_state(struct page *page, enum zone_stat_item item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__dec_zone_state(page_zone(page), item);
 }
 EXPORT_SYMBOL(__dec_zone_page_state);
@@ -413,6 +426,7 @@ static inline void mod_zone_state(struct zone *zone,
 	long o, n, t, z;
 
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		z = 0;  /* overflow to zone counters */
 
 		/*
@@ -470,6 +484,7 @@ static inline void mod_node_state(struct pglist_data *pgdat,
 	long o, n, t, z;
 
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		z = 0;  /* overflow to node counters */
 
 		/*
@@ -509,6 +524,7 @@ EXPORT_SYMBOL(mod_node_page_state);
 
 void inc_node_state(struct pglist_data *pgdat, enum node_stat_item item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mod_node_state(pgdat, item, 1, 1);
 }
 
@@ -614,6 +630,7 @@ static int fold_diff(int *zone_diff, int *numa_diff, int *node_diff)
 	int i;
 	int changes = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
 		if (zone_diff[i]) {
 			atomic_long_add(zone_diff[i], &vm_zone_stat[i]);
@@ -703,16 +720,21 @@ static int refresh_cpu_vm_stats(bool do_pagesets)
 		for (i = 0; i < NR_VM_NUMA_STAT_ITEMS; i++) {
 			int v;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			v = this_cpu_xchg(p->vm_numa_stat_diff[i], 0);
 			if (v) {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				atomic_long_add(v, &zone->vm_numa_stat[i]);
 				global_numa_diff[i] += v;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				__this_cpu_write(p->expire, 3);
 			}
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (do_pagesets) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			cond_resched();
 			/*
 			 * Deal with draining the remote pageset of this
@@ -729,14 +751,18 @@ static int refresh_cpu_vm_stats(bool do_pagesets)
 			 * We never drain zones local to this processor.
 			 */
 			if (zone_to_nid(zone) == numa_node_id()) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				__this_cpu_write(p->expire, 0);
 				continue;
 			}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (__this_cpu_dec_return(p->expire))
 				continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (__this_cpu_read(p->pcp.count)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				drain_zone_pages(zone, this_cpu_ptr(&p->pcp));
 				changes++;
 			}
@@ -783,6 +809,7 @@ void cpu_vm_stats_fold(int cpu)
 #endif
 	int global_node_diff[NR_VM_NODE_STAT_ITEMS] = { 0, };
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_populated_zone(zone) {
 		struct per_cpu_pageset *p;
 
@@ -842,6 +869,7 @@ void drain_zonestat(struct zone *zone, struct per_cpu_pageset *pset)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
 		if (pset->vm_stat_diff[i]) {
 			int v = pset->vm_stat_diff[i];
@@ -871,6 +899,7 @@ void __inc_numa_state(struct zone *zone,
 	u16 __percpu *p = pcp->vm_numa_stat_diff + item;
 	u16 v;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	v = __this_cpu_inc_return(*p);
 
 	if (unlikely(v > NUMA_STATS_THRESHOLD)) {
@@ -891,6 +920,7 @@ unsigned long sum_zone_node_page_state(int node,
 	int i;
 	unsigned long count = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < MAX_NR_ZONES; i++)
 		count += zone_page_state(zones + i, item);
 
@@ -908,6 +938,7 @@ unsigned long sum_zone_numa_state(int node,
 	int i;
 	unsigned long count = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < MAX_NR_ZONES; i++)
 		count += zone_numa_state_snapshot(zones + i, item);
 
@@ -920,6 +951,7 @@ unsigned long sum_zone_numa_state(int node,
 unsigned long node_page_state(struct pglist_data *pgdat,
 				enum node_stat_item item)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	long x = atomic_long_read(&pgdat->vm_stat[item]);
 #ifdef CONFIG_SMP
 	if (x < 0)
@@ -955,6 +987,7 @@ static void fill_contig_page_info(struct zone *zone,
 	info->free_blocks_total = 0;
 	info->free_blocks_suitable = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (order = 0; order < MAX_ORDER; order++) {
 		unsigned long blocks;
 
@@ -983,6 +1016,7 @@ static int __fragmentation_index(unsigned int order, struct contig_page_info *in
 {
 	unsigned long requested = 1UL << order;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(order >= MAX_ORDER))
 		return 0;
 
@@ -1260,6 +1294,7 @@ static void walk_zones_in_node(struct seq_file *m, pg_data_t *pgdat,
 	struct zone *node_zones = pgdat->node_zones;
 	unsigned long flags;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zone = node_zones; zone - node_zones < MAX_NR_ZONES; ++zone) {
 		if (assert_populated && !populated_zone(zone))
 			continue;
@@ -1280,6 +1315,7 @@ static void frag_show_print(struct seq_file *m, pg_data_t *pgdat,
 	int order;
 
 	seq_printf(m, "Node %d, zone %8s ", pgdat->node_id, zone->name);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (order = 0; order < MAX_ORDER; ++order)
 		seq_printf(m, "%6lu ", zone->free_area[order].nr_free);
 	seq_putc(m, '\n');
@@ -1300,6 +1336,7 @@ static void pagetypeinfo_showfree_print(struct seq_file *m,
 {
 	int order, mtype;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (mtype = 0; mtype < MIGRATE_TYPES; mtype++) {
 		seq_printf(m, "Node %4d, zone %8s, type %12s ",
 					pgdat->node_id,
@@ -1328,6 +1365,7 @@ static int pagetypeinfo_showfree(struct seq_file *m, void *arg)
 
 	/* Print header */
 	seq_printf(m, "%-43s ", "Free pages count per migrate type at order");
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (order = 0; order < MAX_ORDER; ++order)
 		seq_printf(m, "%6d ", order);
 	seq_putc(m, '\n');
@@ -1346,6 +1384,7 @@ static void pagetypeinfo_showblockcount_print(struct seq_file *m,
 	unsigned long end_pfn = zone_end_pfn(zone);
 	unsigned long count[MIGRATE_TYPES] = { 0, };
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {
 		struct page *page;
 
@@ -1380,6 +1419,7 @@ static int pagetypeinfo_showblockcount(struct seq_file *m, void *arg)
 	pg_data_t *pgdat = (pg_data_t *)arg;
 
 	seq_printf(m, "\n%-23s", "Number of blocks type ");
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (mtype = 0; mtype < MIGRATE_TYPES; mtype++)
 		seq_printf(m, "%12s ", migratetype_names[mtype]);
 	seq_putc(m, '\n');
@@ -1425,7 +1465,9 @@ static int pagetypeinfo_show(struct seq_file *m, void *arg)
 
 	/* check memoryless node */
 	if (!node_state(pgdat->node_id, N_MEMORY))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	seq_printf(m, "Page block order: %d\n", pageblock_order);
 	seq_printf(m, "Pages per block:  %lu\n", pageblock_nr_pages);
@@ -1446,6 +1488,7 @@ static const struct seq_operations fragmentation_op = {
 
 static int fragmentation_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_open(file, &fragmentation_op);
 }
 
@@ -1465,6 +1508,7 @@ static const struct seq_operations pagetypeinfo_op = {
 
 static int pagetypeinfo_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_open(file, &pagetypeinfo_op);
 }
 
@@ -1479,6 +1523,7 @@ static bool is_zone_first_populated(pg_data_t *pgdat, struct zone *zone)
 {
 	int zid;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (zid = 0; zid < MAX_NR_ZONES; zid++) {
 		struct zone *compare = &pgdat->node_zones[zid];
 
@@ -1495,6 +1540,7 @@ static void zoneinfo_show_print(struct seq_file *m, pg_data_t *pgdat,
 	int i;
 	seq_printf(m, "Node %d, zone %8s", pgdat->node_id, zone->name);
 	if (is_zone_first_populated(pgdat, zone)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		seq_printf(m, "\n  per-node stats");
 		for (i = 0; i < NR_VM_NODE_STAT_ITEMS; i++) {
 			seq_printf(m, "\n      %-12s %lu",
@@ -1595,6 +1641,7 @@ static const struct seq_operations zoneinfo_op = {
 
 static int zoneinfo_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_open(file, &zoneinfo_op);
 }
 
@@ -1617,7 +1664,10 @@ static void *vmstat_start(struct seq_file *m, loff_t *pos)
 	int i, stat_items_size;
 
 	if (*pos >= ARRAY_SIZE(vmstat_text))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	stat_items_size = NR_VM_ZONE_STAT_ITEMS * sizeof(unsigned long) +
 			  NR_VM_NUMA_STAT_ITEMS * sizeof(unsigned long) +
 			  NR_VM_NODE_STAT_ITEMS * sizeof(unsigned long) +
@@ -1630,14 +1680,20 @@ static void *vmstat_start(struct seq_file *m, loff_t *pos)
 	v = kmalloc(stat_items_size, GFP_KERNEL);
 	m->private = v;
 	if (!v)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++)
 		v[i] = global_zone_page_state(i);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	v += NR_VM_ZONE_STAT_ITEMS;
 
 #ifdef CONFIG_NUMA
 	for (i = 0; i < NR_VM_NUMA_STAT_ITEMS; i++)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		v[i] = global_numa_state(i);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	v += NR_VM_NUMA_STAT_ITEMS;
 #endif
 
@@ -1661,7 +1717,9 @@ static void *vmstat_next(struct seq_file *m, void *arg, loff_t *pos)
 {
 	(*pos)++;
 	if (*pos >= ARRAY_SIZE(vmstat_text))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	return (unsigned long *)m->private + *pos;
 }
 
@@ -1709,6 +1767,7 @@ int sysctl_stat_interval __read_mostly = HZ;
 #ifdef CONFIG_PROC_FS
 static void refresh_vm_stats(struct work_struct *work)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	refresh_cpu_vm_stats(true);
 }
 
@@ -1733,7 +1792,9 @@ int vmstat_refresh(struct ctl_table *table, int write,
 	 */
 	err = schedule_on_each_cpu(refresh_vm_stats);
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return err;
+}
 	for (i = 0; i < NR_VM_ZONE_STAT_ITEMS; i++) {
 		val = atomic_long_read(&vm_zone_stat[i]);
 		if (val < 0) {
@@ -1792,6 +1853,7 @@ static bool need_update(int cpu)
 	for_each_populated_zone(zone) {
 		struct per_cpu_pageset *p = per_cpu_ptr(zone->pageset, cpu);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUILD_BUG_ON(sizeof(p->vm_stat_diff[0]) != 1);
 #ifdef CONFIG_NUMA
 		BUILD_BUG_ON(sizeof(p->vm_numa_stat_diff[0]) != 2);
@@ -1802,12 +1864,17 @@ static bool need_update(int cpu)
 		 * This works because the diffs are byte sized items.
 		 */
 		if (memchr_inv(p->vm_stat_diff, 0, NR_VM_ZONE_STAT_ITEMS))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return true;
+}
 #ifdef CONFIG_NUMA
 		if (memchr_inv(p->vm_numa_stat_diff, 0, NR_VM_NUMA_STAT_ITEMS))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return true;
+}
 #endif
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -1819,13 +1886,19 @@ static bool need_update(int cpu)
 void quiet_vmstat(void)
 {
 	if (system_state != SYSTEM_RUNNING)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (!delayed_work_pending(this_cpu_ptr(&vmstat_work)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (!need_update(smp_processor_id()))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * Just refresh counters and do not care about the pending delayed
@@ -1858,6 +1931,7 @@ static void vmstat_shepherd(struct work_struct *w)
 		if (!delayed_work_pending(dw) && need_update(cpu))
 			queue_delayed_work_on(cpu, mm_percpu_wq, dw, 0);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	put_online_cpus();
 
 	schedule_delayed_work(&shepherd,
@@ -1882,12 +1956,15 @@ static void __init init_cpu_node_state(void)
 
 	for_each_online_node(node) {
 		if (cpumask_weight(cpumask_of_node(node)) > 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			node_set_state(node, N_CPU);
+}
 	}
 }
 
 static int vmstat_cpu_online(unsigned int cpu)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	refresh_zone_stat_thresholds();
 	node_set_state(cpu_to_node(cpu), N_CPU);
 	return 0;
@@ -1895,6 +1972,7 @@ static int vmstat_cpu_online(unsigned int cpu)
 
 static int vmstat_cpu_down_prep(unsigned int cpu)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	cancel_delayed_work_sync(&per_cpu(vmstat_work, cpu));
 	return 0;
 }
@@ -1909,7 +1987,9 @@ static int vmstat_cpu_dead(unsigned int cpu)
 	refresh_zone_stat_thresholds();
 	node_cpus = cpumask_of_node(node);
 	if (cpumask_weight(node_cpus) > 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	node_clear_state(node, N_CPU);
 	return 0;
@@ -1929,14 +2009,19 @@ void __init init_mm_internals(void)
 	ret = cpuhp_setup_state_nocalls(CPUHP_MM_VMSTAT_DEAD, "mm/vmstat:dead",
 					NULL, vmstat_cpu_dead);
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("vmstat: failed to register 'dead' hotplug state\n");
+}
 
 	ret = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN, "mm/vmstat:online",
 					vmstat_cpu_online,
 					vmstat_cpu_down_prep);
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("vmstat: failed to register 'online' hotplug state\n");
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	get_online_cpus();
 	init_cpu_node_state();
 	put_online_cpus();
@@ -1985,6 +2070,7 @@ static void unusable_show_print(struct seq_file *m,
 	seq_printf(m, "Node %d, zone %8s ",
 				pgdat->node_id,
 				zone->name);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (order = 0; order < MAX_ORDER; ++order) {
 		fill_contig_page_info(zone, order, &info);
 		index = unusable_free_index(order, &info);
@@ -2009,7 +2095,9 @@ static int unusable_show(struct seq_file *m, void *arg)
 
 	/* check memoryless node */
 	if (!node_state(pgdat->node_id, N_MEMORY))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	walk_zones_in_node(m, pgdat, true, false, unusable_show_print);
 
@@ -2025,6 +2113,7 @@ static const struct seq_operations unusable_op = {
 
 static int unusable_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_open(file, &unusable_op);
 }
 
@@ -2047,6 +2136,7 @@ static void extfrag_show_print(struct seq_file *m,
 	seq_printf(m, "Node %d, zone %8s ",
 				pgdat->node_id,
 				zone->name);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (order = 0; order < MAX_ORDER; ++order) {
 		fill_contig_page_info(zone, order, &info);
 		index = __fragmentation_index(order, &info);
@@ -2077,6 +2167,7 @@ static const struct seq_operations extfrag_op = {
 
 static int extfrag_open(struct inode *inode, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return seq_open(file, &extfrag_op);
 }
 
@@ -2093,7 +2184,9 @@ static int __init extfrag_debug_init(void)
 
 	extfrag_debug_root = debugfs_create_dir("extfrag", NULL);
 	if (!extfrag_debug_root)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (!debugfs_create_file("unusable_index", 0444,
 			extfrag_debug_root, NULL, &unusable_file_ops))
@@ -2103,6 +2196,7 @@ static int __init extfrag_debug_init(void)
 			extfrag_debug_root, NULL, &extfrag_file_ops))
 		goto fail;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 fail:
 	debugfs_remove_recursive(extfrag_debug_root);
diff --git a/mm/workingset.c b/mm/workingset.c
index b997c9d..76b7d5c 100644
--- a/mm/workingset.c
+++ b/mm/workingset.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Workingset detection
@@ -172,6 +174,7 @@ static unsigned int bucket_order __read_mostly;
 
 static void *pack_shadow(int memcgid, pg_data_t *pgdat, unsigned long eviction)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	eviction >>= bucket_order;
 	eviction = (eviction << MEM_CGROUP_ID_SHIFT) | memcgid;
 	eviction = (eviction << NODES_SHIFT) | pgdat->node_id;
@@ -207,6 +210,7 @@ static void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,
  */
 void *workingset_eviction(struct address_space *mapping, struct page *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct mem_cgroup *memcg = page_memcg(page);
 	struct pglist_data *pgdat = page_pgdat(page);
 	int memcgid = mem_cgroup_id(memcg);
@@ -263,6 +267,7 @@ bool workingset_refault(void *shadow)
 	 * configurations instead.
 	 */
 	memcg = mem_cgroup_from_id(memcgid);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!mem_cgroup_disabled() && !memcg) {
 		rcu_read_unlock();
 		return false;
@@ -346,7 +351,9 @@ void workingset_update_node(struct radix_tree_node *node, void *private)
 
 	/* Only regular page cache has shadow entries */
 	if (dax_mapping(mapping) || shmem_mapping(mapping))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * Track non-empty nodes that contain only shadow entries;
@@ -357,11 +364,16 @@ void workingset_update_node(struct radix_tree_node *node, void *private)
 	 * as node->private_list is protected by &mapping->tree_lock.
 	 */
 	if (node->count && node->count == node->exceptional) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (list_empty(&node->private_list))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			list_lru_add(&shadow_nodes, &node->private_list);
+}
 	} else {
 		if (!list_empty(&node->private_list))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			list_lru_del(&shadow_nodes, &node->private_list);
+}
 	}
 }
 
@@ -435,6 +447,7 @@ static enum lru_status shadow_lru_isolate(struct list_head *item,
 	 * to reclaim, take the node off-LRU, and drop the lru_lock.
 	 */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	node = container_of(item, struct radix_tree_node, private_list);
 	mapping = container_of(node->root, struct address_space, page_tree);
 
@@ -519,6 +532,7 @@ static int __init workingset_init(void)
 	unsigned int max_order;
 	int ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG_ON(BITS_PER_LONG < EVICTION_SHIFT);
 	/*
 	 * Calculate the eviction bucket size to cover the longest
@@ -530,7 +544,9 @@ static int __init workingset_init(void)
 	timestamp_bits = BITS_PER_LONG - EVICTION_SHIFT;
 	max_order = fls_long(totalram_pages - 1);
 	if (max_order > timestamp_bits)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bucket_order = max_order - timestamp_bits;
+}
 	pr_info("workingset: timestamp_bits=%d max_order=%d bucket_order=%u\n",
 	       timestamp_bits, max_order, bucket_order);
 
@@ -540,6 +556,7 @@ static int __init workingset_init(void)
 	ret = register_shrinker(&workingset_shadow_shrinker);
 	if (ret)
 		goto err_list_lru;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 err_list_lru:
 	list_lru_destroy(&shadow_nodes);
-- 
2.7.4

