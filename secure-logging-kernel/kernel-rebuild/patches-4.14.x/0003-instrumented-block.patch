From 0e8336a802d7d52309856c727626edd8e3155780 Mon Sep 17 00:00:00 2001
From: Yiwen Li <detectivelyw@gmail.com>
Date: Tue, 4 Jun 2019 13:59:24 -0400
Subject: [PATCH 03/11] instrumented block.

---
 block/bio-integrity.c      |   2 +
 block/bio.c                | 120 ++++++++++++++++++++++++++++++++
 block/blk-cgroup.c         |   2 +
 block/blk-core.c           | 132 +++++++++++++++++++++++++++++++++++
 block/blk-exec.c           |   4 ++
 block/blk-flush.c          |  12 ++++
 block/blk-integrity.c      |   2 +
 block/blk-ioc.c            |  29 ++++++++
 block/blk-map.c            |  22 ++++++
 block/blk-merge.c          |  37 ++++++++++
 block/blk-mq-cpumap.c      |  11 +++
 block/blk-mq-debugfs.c     |  59 ++++++++++++++++
 block/blk-mq-sched.c       |  38 ++++++++++
 block/blk-mq-sysfs.c       |  25 +++++++
 block/blk-mq-tag.c         |  30 ++++++++
 block/blk-mq.c             | 170 +++++++++++++++++++++++++++++++++++++++++++++
 block/blk-mq.h             |   2 +
 block/blk-settings.c       |  32 +++++++++
 block/blk-softirq.c        |  13 ++++
 block/blk-stat.c           |  25 +++++++
 block/blk-stat.h           |   2 +
 block/blk-sysfs.c          |  63 +++++++++++++++++
 block/blk-tag.c            |  33 +++++++++
 block/blk-throttle.c       |   2 +
 block/blk-timeout.c        |  17 +++++
 block/blk.h                |   2 +
 block/bounce.c             |  19 +++++
 block/bsg.c                |  48 +++++++++++++
 block/cfq-iosched.c        |   5 ++
 block/deadline-iosched.c   |  25 +++++++
 block/elevator.c           |  68 ++++++++++++++++++
 block/genhd.c              |  95 +++++++++++++++++++++++++
 block/ioctl.c              |  42 +++++++++++
 block/kyber-iosched.c      |  26 +++++++
 block/mq-deadline.c        |  25 +++++++
 block/noop-iosched.c       |  12 ++++
 block/partition-generic.c  |  55 +++++++++++++++
 block/partitions/check.c   |  16 +++++
 block/partitions/check.h   |   2 +
 block/partitions/cmdline.c |   2 +
 block/partitions/efi.c     |  58 ++++++++++++++++
 block/partitions/ldm.c     |   2 +
 block/partitions/msdos.c   |  52 ++++++++++++++
 block/scsi_ioctl.c         |  29 ++++++++
 44 files changed, 1467 insertions(+)

diff --git a/block/bio-integrity.c b/block/bio-integrity.c
index 5df3290..c8af05b 100644
--- a/block/bio-integrity.c
+++ b/block/bio-integrity.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * bio-integrity.c - bio data integrity extensions
  *
diff --git a/block/bio.c b/block/bio.c
index 7f978ea..518ecf4 100644
--- a/block/bio.c
+++ b/block/bio.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Copyright (C) 2001 Jens Axboe <axboe@kernel.dk>
  *
@@ -84,8 +86,11 @@ static struct kmem_cache *bio_find_or_create_slab(unsigned int extra_size)
 		bslab = &bio_slabs[i];
 
 		if (!bslab->slab && entry == -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			entry = i;
+}
 		else if (bslab->slab_size == sz) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			slab = bslab->slab;
 			bslab->slab_ref++;
 			break;
@@ -132,6 +137,7 @@ static void bio_put_slab(struct bio_set *bs)
 
 	mutex_lock(&bio_slab_lock);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < bio_slab_nr; i++) {
 		if (bs->bio_slab == bio_slabs[i].slab) {
 			bslab = &bio_slabs[i];
@@ -156,18 +162,25 @@ static void bio_put_slab(struct bio_set *bs)
 
 unsigned int bvec_nr_vecs(unsigned short idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return bvec_slabs[idx].nr_vecs;
 }
 
 void bvec_free(mempool_t *pool, struct bio_vec *bv, unsigned int idx)
 {
 	if (!idx)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	idx--;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BIO_BUG_ON(idx >= BVEC_POOL_NR);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (idx == BVEC_POOL_MAX) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mempool_free(bv, pool);
 	} else {
 		struct biovec_slab *bvs = bvec_slabs + idx;
@@ -277,6 +290,7 @@ static void bio_free(struct bio *bio)
 void bio_init(struct bio *bio, struct bio_vec *table,
 	      unsigned short max_vecs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	memset(bio, 0, sizeof(*bio));
 	atomic_set(&bio->__bi_remaining, 1);
 	atomic_set(&bio->__bi_cnt, 1);
@@ -313,13 +327,16 @@ static struct bio *__bio_chain_endio(struct bio *bio)
 	struct bio *parent = bio->bi_private;
 
 	if (!parent->bi_status)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		parent->bi_status = bio->bi_status;
+}
 	bio_put(bio);
 	return parent;
 }
 
 static void bio_chain_endio(struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_endio(__bio_chain_endio(bio));
 }
 
@@ -336,6 +353,7 @@ static void bio_chain_endio(struct bio *bio)
  */
 void bio_chain(struct bio *bio, struct bio *parent)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(bio->bi_private || bio->bi_end_io);
 
 	bio->bi_private = parent;
@@ -346,6 +364,7 @@ EXPORT_SYMBOL(bio_chain);
 
 static void bio_alloc_rescue(struct work_struct *work)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct bio_set *bs = container_of(work, struct bio_set, rescue_work);
 	struct bio *bio;
 
@@ -366,6 +385,7 @@ static void punt_bios_to_rescuer(struct bio_set *bs)
 	struct bio_list punt, nopunt;
 	struct bio *bio;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(!bs->rescue_workqueue))
 		return;
 	/*
@@ -445,7 +465,9 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 
 	if (!bs) {
 		if (nr_iovecs > UIO_MAXIOV)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 
 		p = kmalloc(sizeof(struct bio) +
 			    nr_iovecs * sizeof(struct bio_vec),
@@ -455,7 +477,9 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 	} else {
 		/* should not use nobvec bioset for nr_iovecs > 0 */
 		if (WARN_ON_ONCE(!bs->bvec_pool && nr_iovecs > 0))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return NULL;
+}
 		/*
 		 * generic_make_request() converts recursion to iteration; this
 		 * means if we're running beneath it, any bios we allocate and
@@ -485,6 +509,7 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 
 		p = mempool_alloc(bs->bio_pool, gfp_mask);
 		if (!p && gfp_mask != saved_gfp) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			punt_bios_to_rescuer(bs);
 			gfp_mask = saved_gfp;
 			p = mempool_alloc(bs->bio_pool, gfp_mask);
@@ -495,7 +520,9 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 	}
 
 	if (unlikely(!p))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	bio = p + front_pad;
 	bio_init(bio, NULL, 0);
@@ -504,15 +531,19 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, unsigned int nr_iovecs,
 		unsigned long idx = 0;
 
 		bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, bs->bvec_pool);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!bvl && gfp_mask != saved_gfp) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			punt_bios_to_rescuer(bs);
 			gfp_mask = saved_gfp;
 			bvl = bvec_alloc(gfp_mask, nr_iovecs, &idx, bs->bvec_pool);
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (unlikely(!bvl))
 			goto err_free;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio->bi_flags |= idx << BVEC_POOL_OFFSET;
 	} else if (nr_iovecs) {
 		bvl = bio->bi_inline_vecs;
@@ -535,6 +566,7 @@ void zero_fill_bio(struct bio *bio)
 	struct bio_vec bv;
 	struct bvec_iter iter;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment(bv, bio, iter) {
 		char *data = bvec_kmap_irq(&bv, &flags);
 		memset(data, 0, bv.bv_len);
@@ -557,13 +589,16 @@ void bio_put(struct bio *bio)
 	if (!bio_flagged(bio, BIO_REFFED))
 		bio_free(bio);
 	else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BIO_BUG_ON(!atomic_read(&bio->__bi_cnt));
 
 		/*
 		 * last put frees it
 		 */
 		if (atomic_dec_and_test(&bio->__bi_cnt))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bio_free(bio);
+}
 	}
 }
 EXPORT_SYMBOL(bio_put);
@@ -590,6 +625,7 @@ EXPORT_SYMBOL(bio_phys_segments);
  */
 void __bio_clone_fast(struct bio *bio, struct bio *bio_src)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(bio->bi_pool && BVEC_POOL_IDX(bio));
 
 	/*
@@ -624,7 +660,9 @@ struct bio *bio_clone_fast(struct bio *bio, gfp_t gfp_mask, struct bio_set *bs)
 
 	b = bio_alloc_bioset(gfp_mask, 0, bs);
 	if (!b)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	__bio_clone_fast(b, bio);
 
@@ -683,7 +721,9 @@ struct bio *bio_clone_bioset(struct bio *bio_src, gfp_t gfp_mask,
 
 	bio = bio_alloc_bioset(gfp_mask, bio_segments(bio_src), bs);
 	if (!bio)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	bio->bi_disk		= bio_src->bi_disk;
 	bio->bi_opf		= bio_src->bi_opf;
 	bio->bi_write_hint	= bio_src->bi_write_hint;
@@ -745,10 +785,14 @@ int bio_add_pc_page(struct request_queue *q, struct bio *bio, struct page
 	 * cloned bio must not modify vec list
 	 */
 	if (unlikely(bio_flagged(bio, BIO_CLONED)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	if (((bio->bi_iter.bi_size + len) >> 9) > queue_max_hw_sectors(q))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * For filesystems with a blocksize smaller than the pagesize
@@ -760,6 +804,7 @@ int bio_add_pc_page(struct request_queue *q, struct bio *bio, struct page
 
 		if (page == prev->bv_page &&
 		    offset == prev->bv_offset + prev->bv_len) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			prev->bv_len += len;
 			bio->bi_iter.bi_size += len;
 			goto done;
@@ -770,11 +815,15 @@ int bio_add_pc_page(struct request_queue *q, struct bio *bio, struct page
 		 * offset would create a gap, disallow it.
 		 */
 		if (bvec_gap_to_prev(q, prev, offset))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 	}
 
 	if (bio->bi_vcnt >= bio->bi_max_vecs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * setup the new entry, we might clear it again later if we
@@ -795,16 +844,20 @@ int bio_add_pc_page(struct request_queue *q, struct bio *bio, struct page
 
 	while (bio->bi_phys_segments > queue_max_segments(q)) {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (retried_segments)
 			goto failed;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		retried_segments = 1;
 		blk_recount_segments(q, bio);
 	}
 
 	/* If we may be able to merge these biovecs, force a recount */
 	if (bio->bi_vcnt > 1 && (BIOVEC_PHYS_MERGEABLE(bvec-1, bvec)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio_clear_flag(bio, BIO_SEG_VALID);
+}
 
  done:
 	return len;
@@ -839,7 +892,9 @@ int bio_add_page(struct bio *bio, struct page *page,
 	 * cloned bio must not modify vec list
 	 */
 	if (WARN_ON_ONCE(bio_flagged(bio, BIO_CLONED)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * For filesystems with a blocksize smaller than the pagesize
@@ -847,17 +902,21 @@ int bio_add_page(struct bio *bio, struct page *page,
 	 * a consecutive offset.  Optimize this special case.
 	 */
 	if (bio->bi_vcnt > 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bv = &bio->bi_io_vec[bio->bi_vcnt - 1];
 
 		if (page == bv->bv_page &&
 		    offset == bv->bv_offset + bv->bv_len) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bv->bv_len += len;
 			goto done;
 		}
 	}
 
 	if (bio->bi_vcnt >= bio->bi_max_vecs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	bv		= &bio->bi_io_vec[bio->bi_vcnt];
 	bv->bv_page	= page;
@@ -889,7 +948,9 @@ int bio_iov_iter_get_pages(struct bio *bio, struct iov_iter *iter)
 
 	size = iov_iter_get_pages(iter, pages, LONG_MAX, nr_pages, &offset);
 	if (unlikely(size <= 0))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return size ? size : -EFAULT;
+}
 	nr_pages = (size + offset + PAGE_SIZE - 1) / PAGE_SIZE;
 
 	/*
@@ -973,7 +1034,9 @@ EXPORT_SYMBOL(submit_bio_wait);
 void bio_advance(struct bio *bio, unsigned bytes)
 {
 	if (bio_integrity(bio))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio_integrity_advance(bio, bytes);
+}
 
 	bio_advance_iter(bio, &bio->bi_iter, bytes);
 }
@@ -994,6 +1057,7 @@ int bio_alloc_pages(struct bio *bio, gfp_t gfp_mask)
 	int i;
 	struct bio_vec *bv;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment_all(bv, bio, i) {
 		bv->bv_page = alloc_page(gfp_mask);
 		if (!bv->bv_page) {
@@ -1029,6 +1093,7 @@ void bio_copy_data(struct bio *dst, struct bio *src)
 	src_iter = src->bi_iter;
 	dst_iter = dst->bi_iter;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (1) {
 		if (!src_iter.bi_size) {
 			src = src->bi_next;
@@ -1076,6 +1141,7 @@ struct bio_map_data {
 static struct bio_map_data *bio_alloc_map_data(unsigned int iov_count,
 					       gfp_t gfp_mask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (iov_count > UIO_MAXIOV)
 		return NULL;
 
@@ -1096,6 +1162,7 @@ static int bio_copy_from_iter(struct bio *bio, struct iov_iter iter)
 	int i;
 	struct bio_vec *bvec;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment_all(bvec, bio, i) {
 		ssize_t ret;
 
@@ -1127,6 +1194,7 @@ static int bio_copy_to_iter(struct bio *bio, struct iov_iter iter)
 	int i;
 	struct bio_vec *bvec;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment_all(bvec, bio, i) {
 		ssize_t ret;
 
@@ -1207,6 +1275,7 @@ struct bio *bio_copy_user_iov(struct request_queue *q,
 	int i, ret;
 	int nr_pages = 0;
 	unsigned int len = iter->count;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned int offset = map_data ? offset_in_page(map_data->offset) : 0;
 
 	for (i = 0; i < iter->nr_segs; i++) {
@@ -1336,6 +1405,7 @@ struct bio *bio_map_user_iov(struct request_queue *q,
 	struct iovec iov;
 	struct bio_vec *bvec;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	iov_for_each(iov, i, *iter) {
 		unsigned long uaddr = (unsigned long) iov.iov_base;
 		unsigned long len = iov.iov_len;
@@ -1478,6 +1548,7 @@ static void __bio_unmap_user(struct bio *bio)
  */
 void bio_unmap_user(struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__bio_unmap_user(bio);
 	bio_put(bio);
 }
@@ -1509,7 +1580,9 @@ struct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,
 
 	bio = bio_kmalloc(gfp_mask, nr_pages);
 	if (!bio)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	offset = offset_in_page(kaddr);
 	for (i = 0; i < nr_pages; i++) {
@@ -1519,7 +1592,9 @@ struct bio *bio_map_kern(struct request_queue *q, void *data, unsigned int len,
 			break;
 
 		if (bytes > len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bytes = len;
+}
 
 		if (bio_add_pc_page(q, bio, virt_to_page(data), bytes,
 				    offset) < bytes) {
@@ -1583,26 +1658,34 @@ struct bio *bio_copy_kern(struct request_queue *q, void *data, unsigned int len,
 	 * Overflow, abort
 	 */
 	if (end < start)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-EINVAL);
+}
 
 	nr_pages = end - start;
 	bio = bio_kmalloc(gfp_mask, nr_pages);
 	if (!bio)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	while (len) {
 		struct page *page;
 		unsigned int bytes = PAGE_SIZE;
 
 		if (bytes > len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bytes = len;
+}
 
 		page = alloc_page(q->bounce_gfp | gfp_mask);
 		if (!page)
 			goto cleanup;
 
 		if (!reading)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			memcpy(page_address(page), p, bytes);
+}
 
 		if (bio_add_pc_page(q, bio, page, bytes, 0) < bytes)
 			break;
@@ -1615,9 +1698,11 @@ struct bio *bio_copy_kern(struct request_queue *q, void *data, unsigned int len,
 		bio->bi_end_io = bio_copy_kern_endio_read;
 		bio->bi_private = data;
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio->bi_end_io = bio_copy_kern_endio;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return bio;
 
 cleanup:
@@ -1660,6 +1745,7 @@ void bio_set_pages_dirty(struct bio *bio)
 	struct bio_vec *bvec;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
@@ -1673,6 +1759,7 @@ static void bio_release_pages(struct bio *bio)
 	struct bio_vec *bvec;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
@@ -1706,6 +1793,7 @@ static void bio_dirty_fn(struct work_struct *work)
 	unsigned long flags;
 	struct bio *bio;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irqsave(&bio_dirty_lock, flags);
 	bio = bio_dirty_list;
 	bio_dirty_list = NULL;
@@ -1727,6 +1815,7 @@ void bio_check_pages_dirty(struct bio *bio)
 	int nr_clean_pages = 0;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment_all(bvec, bio, i) {
 		struct page *page = bvec->bv_page;
 
@@ -1754,6 +1843,7 @@ void bio_check_pages_dirty(struct bio *bio)
 void generic_start_io_acct(struct request_queue *q, int rw,
 			   unsigned long sectors, struct hd_struct *part)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int cpu = part_stat_lock();
 
 	part_round_stats(q, cpu, part);
@@ -1769,6 +1859,7 @@ void generic_end_io_acct(struct request_queue *q, int rw,
 			 struct hd_struct *part, unsigned long start_time)
 {
 	unsigned long duration = jiffies - start_time;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int cpu = part_stat_lock();
 
 	part_stat_add(cpu, part, ticks[rw], duration);
@@ -1798,15 +1889,21 @@ static inline bool bio_remaining_done(struct bio *bio)
 	 * we always end io on the first invocation.
 	 */
 	if (!bio_flagged(bio, BIO_CHAIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(atomic_read(&bio->__bi_remaining) <= 0);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (atomic_dec_and_test(&bio->__bi_remaining)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio_clear_flag(bio, BIO_CHAIN);
 		return true;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 
@@ -1828,9 +1925,13 @@ void bio_endio(struct bio *bio)
 {
 again:
 	if (!bio_remaining_done(bio))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (!bio_integrity_endio(bio))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * Need to have a real endio function for chained bios, otherwise
@@ -1841,6 +1942,7 @@ void bio_endio(struct bio *bio)
 	 * gcc's sibling call optimization.
 	 */
 	if (bio->bi_end_io == bio_chain_endio) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio = __bio_chain_endio(bio);
 		goto again;
 	}
@@ -1851,6 +1953,7 @@ void bio_endio(struct bio *bio)
 		bio_clear_flag(bio, BIO_TRACE_COMPLETION);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_throtl_bio_endio(bio);
 	/* release cgroup info */
 	bio_uninit(bio);
@@ -1878,6 +1981,7 @@ struct bio *bio_split(struct bio *bio, int sectors,
 {
 	struct bio *split = NULL;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(sectors <= 0);
 	BUG_ON(sectors >= bio_sectors(bio));
 
@@ -1911,6 +2015,7 @@ void bio_trim(struct bio *bio, int offset, int size)
 	 * the given offset and size.
 	 */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	size <<= 9;
 	if (offset == 0 && size == bio->bi_iter.bi_size)
 		return;
@@ -1940,6 +2045,7 @@ mempool_t *biovec_create_pool(int pool_entries)
 
 void bioset_free(struct bio_set *bs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (bs->rescue_workqueue)
 		destroy_workqueue(bs->rescue_workqueue);
 
@@ -1985,16 +2091,20 @@ struct bio_set *bioset_create(unsigned int pool_size,
 
 	bs = kzalloc(sizeof(*bs), GFP_KERNEL);
 	if (!bs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	bs->front_pad = front_pad;
 
 	spin_lock_init(&bs->rescue_lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_list_init(&bs->rescue_list);
 	INIT_WORK(&bs->rescue_work, bio_alloc_rescue);
 
 	bs->bio_slab = bio_find_or_create_slab(front_pad + back_pad);
 	if (!bs->bio_slab) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(bs);
 		return NULL;
 	}
@@ -2010,12 +2120,16 @@ struct bio_set *bioset_create(unsigned int pool_size,
 	}
 
 	if (!(flags & BIOSET_NEED_RESCUER))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return bs;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bs->rescue_workqueue = alloc_workqueue("bioset", WQ_MEM_RECLAIM, 0);
 	if (!bs->rescue_workqueue)
 		goto bad;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return bs;
 bad:
 	bioset_free(bs);
@@ -2132,17 +2246,23 @@ static int __init init_bio(void)
 	bio_slab_nr = 0;
 	bio_slabs = kzalloc(bio_slab_max * sizeof(struct bio_slab), GFP_KERNEL);
 	if (!bio_slabs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("bio: can't allocate bios\n");
+}
 
 	bio_integrity_init();
 	biovec_init_slabs();
 
 	fs_bio_set = bioset_create(BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);
 	if (!fs_bio_set)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("bio: can't allocate bios\n");
+}
 
 	if (bioset_integrity_create(fs_bio_set, BIO_POOL_SIZE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		panic("bio: can't create integrity pool\n");
+}
 
 	return 0;
 }
diff --git a/block/blk-cgroup.c b/block/blk-cgroup.c
index d3f56ba..ec03272 100644
--- a/block/blk-cgroup.c
+++ b/block/blk-cgroup.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Common Block IO controller cgroup interface
  *
diff --git a/block/blk-core.c b/block/blk-core.c
index 95b7ea9..e3ac0a9 100644
--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Copyright (C) 1991, 1992 Linus Torvalds
  * Copyright (C) 1994,      Karl Keyte: Added support for disk statistics
@@ -80,7 +82,9 @@ static void blk_clear_congested(struct request_list *rl, int sync)
 	 * flip its congestion state for events on other blkcgs.
 	 */
 	if (rl == &rl->q->root_rl)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		clear_wb_congested(rl->q->backing_dev_info->wb.congested, sync);
+}
 #endif
 }
 
@@ -101,17 +105,22 @@ void blk_queue_congestion_threshold(struct request_queue *q)
 
 	nr = q->nr_requests - (q->nr_requests / 8) + 1;
 	if (nr > q->nr_requests)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr = q->nr_requests;
+}
 	q->nr_congestion_on = nr;
 
 	nr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;
 	if (nr < 1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nr = 1;
+}
 	q->nr_congestion_off = nr;
 }
 
 void blk_rq_init(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	memset(rq, 0, sizeof(*rq));
 
 	INIT_LIST_HEAD(&rq->queuelist);
@@ -156,6 +165,7 @@ blk_status_t errno_to_blk_status(int errno)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < ARRAY_SIZE(blk_errors); i++) {
 		if (blk_errors[i].errno == errno)
 			return (__force blk_status_t)i;
@@ -170,7 +180,9 @@ int blk_status_to_errno(blk_status_t status)
 	int idx = (__force int)status;
 
 	if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EIO;
+}
 	return blk_errors[idx].errno;
 }
 EXPORT_SYMBOL_GPL(blk_status_to_errno);
@@ -179,6 +191,7 @@ static void print_req_error(struct request *req, blk_status_t status)
 {
 	int idx = (__force int)status;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(idx >= ARRAY_SIZE(blk_errors)))
 		return;
 
@@ -192,10 +205,14 @@ static void req_bio_endio(struct request *rq, struct bio *bio,
 			  unsigned int nbytes, blk_status_t error)
 {
 	if (error)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio->bi_status = error;
+}
 
 	if (unlikely(rq->rq_flags & RQF_QUIET))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio_set_flag(bio, BIO_QUIET);
+}
 
 	bio_advance(bio, nbytes);
 
@@ -222,6 +239,7 @@ static void blk_delay_work(struct work_struct *work)
 {
 	struct request_queue *q;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q = container_of(work, struct request_queue, delay_work.work);
 	spin_lock_irq(q->queue_lock);
 	__blk_run_queue(q);
@@ -240,6 +258,7 @@ static void blk_delay_work(struct work_struct *work)
  */
 void blk_delay_queue(struct request_queue *q, unsigned long msecs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
@@ -260,6 +279,7 @@ EXPORT_SYMBOL(blk_delay_queue);
  **/
 void blk_start_queue_async(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
@@ -279,6 +299,7 @@ EXPORT_SYMBOL(blk_start_queue_async);
  **/
 void blk_start_queue(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON(!in_interrupt() && !irqs_disabled());
 	WARN_ON_ONCE(q->mq_ops);
@@ -304,6 +325,7 @@ EXPORT_SYMBOL(blk_start_queue);
  **/
 void blk_stop_queue(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
@@ -332,6 +354,7 @@ EXPORT_SYMBOL(blk_stop_queue);
  */
 void blk_sync_queue(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	del_timer_sync(&q->timeout);
 	cancel_work_sync(&q->timeout_work);
 
@@ -365,7 +388,9 @@ inline void __blk_run_queue_uncond(struct request_queue *q)
 	WARN_ON_ONCE(q->mq_ops);
 
 	if (unlikely(blk_queue_dead(q)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * Some request_fn implementations, e.g. scsi_request_fn(), unlock
@@ -389,11 +414,14 @@ EXPORT_SYMBOL_GPL(__blk_run_queue_uncond);
  */
 void __blk_run_queue(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
 	if (unlikely(blk_queue_stopped(q)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	__blk_run_queue_uncond(q);
 }
@@ -414,6 +442,7 @@ EXPORT_SYMBOL(__blk_run_queue);
  */
 void blk_run_queue_async(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
@@ -444,6 +473,7 @@ EXPORT_SYMBOL(blk_run_queue);
 
 void blk_put_queue(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kobject_put(&q->kobj);
 }
 EXPORT_SYMBOL(blk_put_queue);
@@ -463,6 +493,7 @@ static void __blk_drain_queue(struct request_queue *q, bool drain_all)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
@@ -533,6 +564,7 @@ static void __blk_drain_queue(struct request_queue *q, bool drain_all)
 
 void blk_drain_queue(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irq(q->queue_lock);
 	__blk_drain_queue(q, true);
 	spin_unlock_irq(q->queue_lock);
@@ -563,6 +595,7 @@ void blk_queue_bypass_start(struct request_queue *q)
 	 * can happen many times during boot.
 	 */
 	if (blk_queue_init_done(q)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irq(q->queue_lock);
 		__blk_drain_queue(q, false);
 		spin_unlock_irq(q->queue_lock);
@@ -586,7 +619,9 @@ void blk_queue_bypass_end(struct request_queue *q)
 {
 	spin_lock_irq(q->queue_lock);
 	if (!--q->bypass_depth)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		queue_flag_clear(QUEUE_FLAG_BYPASS, q);
+}
 	WARN_ON_ONCE(q->bypass_depth < 0);
 	spin_unlock_irq(q->queue_lock);
 }
@@ -594,6 +629,7 @@ EXPORT_SYMBOL_GPL(blk_queue_bypass_end);
 
 void blk_set_queue_dying(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irq(q->queue_lock);
 	queue_flag_set(QUEUE_FLAG_DYING, q);
 	spin_unlock_irq(q->queue_lock);
@@ -672,7 +708,9 @@ void blk_cleanup_queue(struct request_queue *q)
 	 * from more than one contexts
 	 */
 	if (q->mq_ops)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_mq_quiesce_queue(q);
+}
 
 	/* for synchronous bio-based driver finish in-flight integrity i/o */
 	blk_flush_integrity();
@@ -705,6 +743,7 @@ static void *alloc_request_simple(gfp_t gfp_mask, void *data)
 
 static void free_request_simple(void *element, void *data)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kmem_cache_free(request_cachep, element);
 }
 
@@ -716,6 +755,7 @@ static void *alloc_request_size(gfp_t gfp_mask, void *data)
 	rq = kmalloc_node(sizeof(struct request) + q->cmd_size, gfp_mask,
 			q->node);
 	if (rq && q->init_rq_fn && q->init_rq_fn(q, rq, gfp_mask) < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(rq);
 		rq = NULL;
 	}
@@ -735,7 +775,9 @@ int blk_init_rl(struct request_list *rl, struct request_queue *q,
 		gfp_t gfp_mask)
 {
 	if (unlikely(rl->rq_pool))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	rl->q = q;
 	rl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;
@@ -748,21 +790,28 @@ int blk_init_rl(struct request_list *rl, struct request_queue *q,
 				alloc_request_size, free_request_size,
 				q, gfp_mask, q->node);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rl->rq_pool = mempool_create_node(BLKDEV_MIN_RQ,
 				alloc_request_simple, free_request_simple,
 				q, gfp_mask, q->node);
 	}
 	if (!rl->rq_pool)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (rl != &q->root_rl)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		WARN_ON_ONCE(!blk_get_queue(q));
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
 void blk_exit_rl(struct request_queue *q, struct request_list *rl)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rl->rq_pool) {
 		mempool_destroy(rl->rq_pool);
 		if (rl != &q->root_rl)
@@ -778,14 +827,20 @@ EXPORT_SYMBOL(blk_alloc_queue);
 
 int blk_queue_enter(struct request_queue *q, bool nowait)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (true) {
 		int ret;
 
 		if (percpu_ref_tryget_live(&q->q_usage_counter))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (nowait)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EBUSY;
+}
 
 		/*
 		 * read pair of barrier in blk_freeze_queue_start(),
@@ -796,13 +851,20 @@ int blk_queue_enter(struct request_queue *q, bool nowait)
 		 */
 		smp_rmb();
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = wait_event_interruptible(q->mq_freeze_wq,
 				!atomic_read(&q->mq_freeze_depth) ||
 				blk_queue_dying(q));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (blk_queue_dying(q))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENODEV;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ret;
+}
 	}
 }
 
@@ -833,7 +895,9 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 	q = kmem_cache_alloc_node(blk_requestq_cachep,
 				gfp_mask | __GFP_ZERO, node_id);
 	if (!q)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	q->id = ida_simple_get(&blk_queue_ida, 0, 0, gfp_mask);
 	if (q->id < 0)
@@ -906,6 +970,7 @@ struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
 	if (blkcg_init_queue(q))
 		goto fail_ref;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return q;
 
 fail_ref:
@@ -959,6 +1024,7 @@ EXPORT_SYMBOL(blk_alloc_queue_node);
 
 struct request_queue *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return blk_init_queue_node(rfn, lock, NUMA_NO_NODE);
 }
 EXPORT_SYMBOL(blk_init_queue);
@@ -970,7 +1036,9 @@ blk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)
 
 	q = blk_alloc_queue_node(GFP_KERNEL, node_id);
 	if (!q)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	q->request_fn = rfn;
 	if (lock)
@@ -993,7 +1061,9 @@ int blk_init_allocated_queue(struct request_queue *q)
 
 	q->fq = blk_alloc_flush_queue(q, NUMA_NO_NODE, q->cmd_size);
 	if (!q->fq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (q->init_rq_fn && q->init_rq_fn(q, q->fq->flush_rq, GFP_KERNEL))
 		goto out_free_flush_queue;
@@ -1016,6 +1086,7 @@ int blk_init_allocated_queue(struct request_queue *q)
 
 	/* init elevator */
 	if (elevator_init(q, NULL)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mutex_unlock(&q->sysfs_lock);
 		goto out_exit_flush_rq;
 	}
@@ -1025,7 +1096,9 @@ int blk_init_allocated_queue(struct request_queue *q)
 
 out_exit_flush_rq:
 	if (q->exit_rq_fn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		q->exit_rq_fn(q, q->fq->flush_rq);
+}
 out_free_flush_queue:
 	blk_free_flush_queue(q->fq);
 	return -ENOMEM;
@@ -1035,10 +1108,12 @@ EXPORT_SYMBOL(blk_init_allocated_queue);
 bool blk_get_queue(struct request_queue *q)
 {
 	if (likely(!blk_queue_dying(q))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__blk_get_queue(q);
 		return true;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return false;
 }
 EXPORT_SYMBOL(blk_get_queue);
@@ -1048,7 +1123,9 @@ static inline void blk_free_request(struct request_list *rl, struct request *rq)
 	if (rq->rq_flags & RQF_ELVPRIV) {
 		elv_put_request(rl->q, rq);
 		if (rq->elv.icq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_io_context(rq->elv.icq->ioc);
+}
 	}
 
 	mempool_free(rq, rl->rq_pool);
@@ -1061,7 +1138,9 @@ static inline void blk_free_request(struct request_list *rl, struct request *rq)
 static inline int ioc_batching(struct request_queue *q, struct io_context *ioc)
 {
 	if (!ioc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * Make sure the process is able to allocate at least 1 request
@@ -1081,6 +1160,7 @@ static inline int ioc_batching(struct request_queue *q, struct io_context *ioc)
  */
 static void ioc_set_batching(struct request_queue *q, struct io_context *ioc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!ioc || ioc_batching(q, ioc))
 		return;
 
@@ -1097,7 +1177,9 @@ static void __freed_request(struct request_list *rl, int sync)
 
 	if (rl->count[sync] + 1 <= q->nr_requests) {
 		if (waitqueue_active(&rl->wait[sync]))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			wake_up(&rl->wait[sync]);
+}
 
 		blk_clear_rl_full(rl, sync);
 	}
@@ -1120,14 +1202,17 @@ static void freed_request(struct request_list *rl, bool sync,
 	__freed_request(rl, sync);
 
 	if (unlikely(rl->starved[sync ^ 1]))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__freed_request(rl, sync ^ 1);
 }
+}
 
 int blk_update_nr_requests(struct request_queue *q, unsigned int nr)
 {
 	struct request_list *rl;
 	int on_thresh, off_thresh;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON_ONCE(q->mq_ops);
 
 	spin_lock_irq(q->queue_lock);
@@ -1192,16 +1277,20 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	int may_queue;
 	req_flags_t rq_flags = RQF_ALLOCED;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 
 	if (unlikely(blk_queue_dying(q)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENODEV);
+}
 
 	may_queue = elv_may_queue(q, op);
 	if (may_queue == ELV_MQUEUE_NO)
 		goto rq_starved;
 
 	if (rl->count[is_sync]+1 >= queue_congestion_on_threshold(q)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (rl->count[is_sync]+1 >= q->nr_requests) {
 			/*
 			 * The queue will fill after this allocation, so set
@@ -1210,9 +1299,11 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 			 * requests, others will be blocked.
 			 */
 			if (!blk_rl_full(rl, is_sync)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ioc_set_batching(q, ioc);
 				blk_set_rl_full(rl, is_sync);
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (may_queue != ELV_MQUEUE_MUST
 						&& !ioc_batching(q, ioc)) {
 					/*
@@ -1224,6 +1315,7 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 				}
 			}
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_set_congested(rl, is_sync);
 	}
 
@@ -1233,7 +1325,9 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	 * allocated with any setting of ->nr_requests
 	 */
 	if (rl->count[is_sync] >= (3 * q->nr_requests / 2))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	q->nr_rqs[is_sync]++;
 	rl->count[is_sync]++;
@@ -1253,10 +1347,13 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	 * it will be created after releasing queue_lock.
 	 */
 	if (!op_is_flush(op) && !blk_queue_bypass(q)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rq_flags |= RQF_ELVPRIV;
 		q->nr_rqs_elvpriv++;
 		if (et->icq_cache && ioc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			icq = ioc_lookup_icq(ioc, q);
+}
 	}
 
 	if (blk_queue_io_stat(q))
@@ -1276,8 +1373,12 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	/* init elvpriv */
 	if (rq_flags & RQF_ELVPRIV) {
 		if (unlikely(et->icq_cache && !icq)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (ioc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				icq = ioc_create_icq(ioc, q, gfp_mask);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!icq)
 				goto fail_elvpriv;
 		}
@@ -1288,7 +1389,9 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 
 		/* @rq->elv.icq holds io_context until @rq is freed */
 		if (icq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			get_io_context(icq->ioc);
+}
 	}
 out:
 	/*
@@ -1298,7 +1401,9 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	 * be some limit enforced by BLK_BATCH_TIME.
 	 */
 	if (ioc_batching(q, ioc))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ioc->nr_batch_requests--;
+}
 
 	trace_block_getrq(q, bio, op);
 	return rq;
@@ -1341,7 +1446,10 @@ static struct request *__get_request(struct request_list *rl, unsigned int op,
 	 */
 rq_starved:
 	if (unlikely(rl->count[is_sync] == 0))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rl->starved[is_sync] = 1;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ERR_PTR(-ENOMEM);
 }
 
@@ -1367,6 +1475,7 @@ static struct request *get_request(struct request_queue *q, unsigned int op,
 	struct request_list *rl;
 	struct request *rq;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
@@ -1374,14 +1483,20 @@ static struct request *get_request(struct request_queue *q, unsigned int op,
 retry:
 	rq = __get_request(rl, op, bio, gfp_mask);
 	if (!IS_ERR(rq))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return rq;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (op & REQ_NOWAIT) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_put_rl(rl);
 		return ERR_PTR(-EAGAIN);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_put_rl(rl);
 		return rq;
 	}
@@ -1421,6 +1536,7 @@ static struct request *blk_old_get_request(struct request_queue *q,
 	spin_lock_irq(q->queue_lock);
 	rq = get_request(q, op, NULL, gfp_mask);
 	if (IS_ERR(rq)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irq(q->queue_lock);
 		return rq;
 	}
@@ -1441,8 +1557,11 @@ struct request *blk_get_request(struct request_queue *q, unsigned int op,
 		req = blk_mq_alloc_request(q, op,
 			(gfp_mask & __GFP_DIRECT_RECLAIM) ?
 				0 : BLK_MQ_REQ_NOWAIT);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!IS_ERR(req) && q->mq_ops->initialize_rq_fn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			q->mq_ops->initialize_rq_fn(req);
+}
 	} else {
 		req = blk_old_get_request(q, op, gfp_mask);
 		if (!IS_ERR(req) && q->initialize_rq_fn)
@@ -1465,6 +1584,7 @@ EXPORT_SYMBOL(blk_get_request);
  */
 void blk_requeue_request(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 	WARN_ON_ONCE(q->mq_ops);
 
@@ -1485,6 +1605,7 @@ EXPORT_SYMBOL(blk_requeue_request);
 static void add_acct_request(struct request_queue *q, struct request *rq,
 			     int where)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_account_io_start(rq, true);
 	__elv_add_request(q, rq, where);
 }
@@ -1494,8 +1615,10 @@ static void part_round_stats_single(struct request_queue *q, int cpu,
 				    unsigned int inflight)
 {
 	if (inflight) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__part_stat_add(cpu, part, time_in_queue,
 				inflight * (now - part->stamp));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__part_stat_add(cpu, part, io_ticks, (now - part->stamp));
 	}
 	part->stamp = now;
@@ -1526,21 +1649,30 @@ void part_round_stats(struct request_queue *q, int cpu, struct hd_struct *part)
 	int stats = 0;
 
 	if (part->stamp != now)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		stats |= 1;
+}
 
 	if (part->partno) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		part2 = &part_to_disk(part)->part0;
 		if (part2->stamp != now)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			stats |= 2;
+}
 	}
 
 	if (!stats)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	part_in_flight(q, part, inflight);
 
 	if (stats & 2)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		part_round_stats_single(q, cpu, part2, now, inflight[1]);
+}
 	if (stats & 1)
 		part_round_stats_single(q, cpu, part, now, inflight[0]);
 }
diff --git a/block/blk-exec.c b/block/blk-exec.c
index 5c0f3dc..e2be716 100644
--- a/block/blk-exec.c
+++ b/block/blk-exec.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Functions related to setting various queue properties from drivers
  */
@@ -61,6 +63,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	 * be reused after dying flag is set
 	 */
 	if (q->mq_ops) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_mq_sched_insert_request(rq, at_head, true, false, false);
 		return;
 	}
@@ -68,6 +71,7 @@ void blk_execute_rq_nowait(struct request_queue *q, struct gendisk *bd_disk,
 	spin_lock_irq(q->queue_lock);
 
 	if (unlikely(blk_queue_dying(q))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rq->rq_flags |= RQF_QUIET;
 		__blk_end_request_all(rq, BLK_STS_IOERR);
 		spin_unlock_irq(q->queue_lock);
diff --git a/block/blk-flush.c b/block/blk-flush.c
index 4938bec..da275b9 100644
--- a/block/blk-flush.c
+++ b/block/blk-flush.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Functions to sequence PREFLUSH and FUA writes.
  *
@@ -101,7 +103,9 @@ static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 	unsigned int policy = 0;
 
 	if (blk_rq_sectors(rq))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		policy |= REQ_FSEQ_DATA;
+}
 
 	if (fflags & (1UL << QUEUE_FLAG_WC)) {
 		if (rq->cmd_flags & REQ_PREFLUSH)
@@ -115,6 +119,7 @@ static unsigned int blk_flush_policy(unsigned long fflags, struct request *rq)
 
 static unsigned int blk_flush_cur_seq(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 1 << ffz(rq->flush.seq);
 }
 
@@ -134,6 +139,7 @@ static void blk_flush_restore_request(struct request *rq)
 
 static bool blk_flush_queue_rq(struct request *rq, bool add_front)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rq->q->mq_ops) {
 		blk_mq_add_to_requeue_list(rq, add_front, true);
 		return false;
@@ -170,6 +176,7 @@ static bool blk_flush_complete_seq(struct request *rq,
 	struct list_head *pending = &fq->flush_queue[fq->flush_pending_idx];
 	bool queued = false, kicked;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(rq->flush.seq & seq);
 	rq->flush.seq |= seq;
 
@@ -346,6 +353,7 @@ static void flush_data_end_io(struct request *rq, blk_status_t error)
 	struct request_queue *q = rq->q;
 	struct blk_flush_queue *fq = blk_get_flush_queue(q, NULL);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 
 	/*
@@ -422,7 +430,9 @@ void blk_insert_flush(struct request *rq)
 	struct blk_flush_queue *fq = blk_get_flush_queue(q, rq->mq_ctx);
 
 	if (!q->mq_ops)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		lockdep_assert_held(q->queue_lock);
+}
 
 	/*
 	 * @policy now records what operations need to be done.  Adjust
@@ -509,7 +519,9 @@ int blkdev_issue_flush(struct block_device *bdev, gfp_t gfp_mask,
 	int ret = 0;
 
 	if (bdev->bd_disk == NULL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENXIO;
+}
 
 	q = bdev_get_queue(bdev);
 	if (!q)
diff --git a/block/blk-integrity.c b/block/blk-integrity.c
index feb3057..03ef38b4 100644
--- a/block/blk-integrity.c
+++ b/block/blk-integrity.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * blk-integrity.c - Block layer data integrity extensions
  *
diff --git a/block/blk-ioc.c b/block/blk-ioc.c
index f23311e..1ae9e05 100644
--- a/block/blk-ioc.c
+++ b/block/blk-ioc.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to io context handling
@@ -25,6 +27,7 @@ static struct kmem_cache *iocontext_cachep;
  */
 void get_io_context(struct io_context *ioc)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(atomic_long_read(&ioc->refcount) <= 0);
 	atomic_long_inc(&ioc->refcount);
 }
@@ -32,6 +35,7 @@ EXPORT_SYMBOL(get_io_context);
 
 static void icq_free_icq_rcu(struct rcu_head *head)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct io_cq *icq = container_of(head, struct io_cq, __rcu_head);
 
 	kmem_cache_free(icq->__rcu_icq_cache, icq);
@@ -46,7 +50,9 @@ static void ioc_exit_icq(struct io_cq *icq)
 	struct elevator_type *et = icq->q->elevator->type;
 
 	if (icq->flags & ICQ_EXITED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (et->uses_mq && et->ops.mq.exit_icq)
 		et->ops.mq.exit_icq(icq);
@@ -66,6 +72,7 @@ static void ioc_destroy_icq(struct io_cq *icq)
 	struct request_queue *q = icq->q;
 	struct elevator_type *et = q->elevator->type;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&ioc->lock);
 
 	radix_tree_delete(&ioc->icq_tree, icq->q->id);
@@ -96,6 +103,7 @@ static void ioc_destroy_icq(struct io_cq *icq)
  */
 static void ioc_release_fn(struct work_struct *work)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct io_context *ioc = container_of(work, struct io_context,
 					      release_work);
 	unsigned long flags;
@@ -141,7 +149,9 @@ void put_io_context(struct io_context *ioc)
 	bool free_ioc = false;
 
 	if (ioc == NULL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	BUG_ON(atomic_long_read(&ioc->refcount) <= 0);
 
@@ -152,10 +162,13 @@ void put_io_context(struct io_context *ioc)
 	if (atomic_long_dec_and_test(&ioc->refcount)) {
 		spin_lock_irqsave(&ioc->lock, flags);
 		if (!hlist_empty(&ioc->icq_list))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			queue_work(system_power_efficient_wq,
 					&ioc->release_work);
+}
 		else
 			free_ioc = true;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irqrestore(&ioc->lock, flags);
 	}
 
@@ -178,6 +191,7 @@ void put_io_context_active(struct io_context *ioc)
 	struct io_cq *icq;
 
 	if (!atomic_dec_and_test(&ioc->active_ref)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_io_context(ioc);
 		return;
 	}
@@ -190,23 +204,30 @@ void put_io_context_active(struct io_context *ioc)
 retry:
 	spin_lock_irqsave_nested(&ioc->lock, flags, 1);
 	hlist_for_each_entry(icq, &ioc->icq_list, ioc_node) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (icq->flags & ICQ_EXITED)
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		et = icq->q->elevator->type;
 		if (et->uses_mq) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ioc_exit_icq(icq);
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (spin_trylock(icq->q->queue_lock)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				ioc_exit_icq(icq);
 				spin_unlock(icq->q->queue_lock);
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				spin_unlock_irqrestore(&ioc->lock, flags);
 				cpu_relax();
 				goto retry;
 			}
 		}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock_irqrestore(&ioc->lock, flags);
 
 	put_io_context(ioc);
@@ -230,6 +251,7 @@ static void __ioc_clear_queue(struct list_head *icq_list)
 {
 	unsigned long flags;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!list_empty(icq_list)) {
 		struct io_cq *icq = list_entry(icq_list->next,
 					       struct io_cq, q_node);
@@ -255,6 +277,7 @@ void ioc_clear_queue(struct request_queue *q)
 	list_splice_init(&q->icq_list, &icq_list);
 
 	if (q->mq_ops) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock_irq(q->queue_lock);
 		__ioc_clear_queue(&icq_list);
 	} else {
@@ -271,7 +294,9 @@ int create_task_io_context(struct task_struct *task, gfp_t gfp_flags, int node)
 	ioc = kmem_cache_alloc_node(iocontext_cachep, gfp_flags | __GFP_ZERO,
 				    node);
 	if (unlikely(!ioc))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	/* initialize */
 	atomic_long_set(&ioc->refcount, 1);
@@ -321,6 +346,7 @@ struct io_context *get_task_io_context(struct task_struct *task,
 {
 	struct io_context *ioc;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	might_sleep_if(gfpflags_allow_blocking(gfp_flags));
 
 	do {
@@ -350,6 +376,7 @@ struct io_cq *ioc_lookup_icq(struct io_context *ioc, struct request_queue *q)
 {
 	struct io_cq *icq;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 
 	/*
@@ -396,7 +423,9 @@ struct io_cq *ioc_create_icq(struct io_context *ioc, struct request_queue *q,
 	icq = kmem_cache_alloc_node(et->icq_cache, gfp_mask | __GFP_ZERO,
 				    q->node);
 	if (!icq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (radix_tree_maybe_preload(gfp_mask) < 0) {
 		kmem_cache_free(et->icq_cache, icq);
diff --git a/block/blk-map.c b/block/blk-map.c
index e31be14..c54cb40 100644
--- a/block/blk-map.c
+++ b/block/blk-map.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to mapping data to requests
@@ -24,19 +26,25 @@ int blk_rq_append_bio(struct request *rq, struct bio **bio)
 	if (!rq->bio) {
 		blk_rq_bio_prep(rq->q, rq, *bio);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!ll_back_merge_fn(rq->q, rq, *bio)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (orig_bio != *bio) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				bio_put(*bio);
 				*bio = orig_bio;
 			}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EINVAL;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rq->biotail->bi_next = *bio;
 		rq->biotail = *bio;
 		rq->__data_len += (*bio)->bi_iter.bi_size;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 EXPORT_SYMBOL(blk_rq_append_bio);
@@ -46,6 +54,7 @@ static int __blk_rq_unmap_user(struct bio *bio)
 	int ret = 0;
 
 	if (bio) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (bio_flagged(bio, BIO_USER_MAPPED))
 			bio_unmap_user(bio);
 		else
@@ -64,7 +73,9 @@ static int __blk_rq_map_user_iov(struct request *rq,
 	int ret;
 
 	if (copy)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bio = bio_copy_user_iov(q, map_data, iter, gfp_mask);
+}
 	else
 		bio = bio_map_user_iov(q, iter, gfp_mask);
 
@@ -131,6 +142,7 @@ int blk_rq_map_user_iov(struct request_queue *q, struct request *rq,
 	if (!iter_is_iovec(iter))
 		goto fail;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (map_data)
 		copy = true;
 	else if (iov_iter_alignment(iter) & align)
@@ -165,6 +177,7 @@ int blk_rq_map_user(struct request_queue *q, struct request *rq,
 {
 	struct iovec iov;
 	struct iov_iter i;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int ret = import_single_range(rq_data_dir(rq), ubuf, len, &iov, &i);
 
 	if (unlikely(ret < 0))
@@ -188,6 +201,7 @@ int blk_rq_unmap_user(struct bio *bio)
 	struct bio *mapped_bio;
 	int ret = 0, ret2;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (bio) {
 		mapped_bio = bio;
 		if (unlikely(bio_flagged(bio, BIO_BOUNCED)))
@@ -229,9 +243,13 @@ int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,
 	int ret;
 
 	if (len > (queue_max_hw_sectors(q) << 9))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	if (!len || !kbuf)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	do_copy = !blk_rq_aligned(q, addr, len) || object_is_on_stack(kbuf);
 	if (do_copy)
@@ -240,7 +258,9 @@ int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,
 		bio = bio_map_kern(q, kbuf, len, gfp_mask);
 
 	if (IS_ERR(bio))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PTR_ERR(bio);
+}
 
 	bio->bi_opf &= ~REQ_OP_MASK;
 	bio->bi_opf |= req_op(rq);
@@ -248,6 +268,7 @@ int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,
 	if (do_copy)
 		rq->rq_flags |= RQF_COPY_USER;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	orig_bio = bio;
 	ret = blk_rq_append_bio(rq, &bio);
 	if (unlikely(ret)) {
@@ -256,6 +277,7 @@ int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,
 		return ret;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 EXPORT_SYMBOL(blk_rq_map_kern);
diff --git a/block/blk-merge.c b/block/blk-merge.c
index f5dedd5..b9b3d93 100644
--- a/block/blk-merge.c
+++ b/block/blk-merge.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to segment and merge handling
@@ -58,6 +60,7 @@ static struct bio *blk_bio_discard_split(struct request_queue *q,
 static struct bio *blk_bio_write_zeroes_split(struct request_queue *q,
 		struct bio *bio, struct bio_set *bs, unsigned *nsegs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	*nsegs = 1;
 
 	if (!q->limits.max_write_zeroes_sectors)
@@ -74,6 +77,7 @@ static struct bio *blk_bio_write_same_split(struct request_queue *q,
 					    struct bio_set *bs,
 					    unsigned *nsegs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	*nsegs = 1;
 
 	if (!q->limits.max_write_same_sectors)
@@ -88,6 +92,7 @@ static struct bio *blk_bio_write_same_split(struct request_queue *q,
 static inline unsigned get_max_io_size(struct request_queue *q,
 				       struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned sectors = blk_max_size_offset(q, bio->bi_iter.bi_sector);
 	unsigned mask = queue_logical_block_size(q) - 1;
 
@@ -110,6 +115,7 @@ static struct bio *blk_bio_segment_split(struct request_queue *q,
 	struct bio *new = NULL;
 	const unsigned max_sectors = get_max_io_size(q, bio);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment(bv, bio, iter) {
 		/*
 		 * If the queue doesn't support SG gaps and adding this
@@ -230,7 +236,9 @@ static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 	struct bvec_iter iter;
 
 	if (!bio)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	switch (bio_op(bio)) {
 	case REQ_OP_DISCARD:
@@ -241,6 +249,7 @@ static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 		return 1;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	fbio = bio;
 	cluster = blk_queue_cluster(q);
 	seg_size = 0;
@@ -258,11 +267,14 @@ static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 				if (seg_size + bv.bv_len
 				    > queue_max_segment_size(q))
 					goto new_segment;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (!BIOVEC_PHYS_MERGEABLE(&bvprv, &bv))
 					goto new_segment;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				if (!BIOVEC_SEG_BOUNDARY(q, &bvprv, &bv))
 					goto new_segment;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				seg_size += bv.bv_len;
 				bvprv = bv;
 				continue;
@@ -277,6 +289,7 @@ static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 			prev = 1;
 			seg_size = bv.bv_len;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bbio = bio;
 	}
 
@@ -285,11 +298,13 @@ static unsigned int __blk_recalc_rq_segments(struct request_queue *q,
 	if (seg_size > bbio->bi_seg_back_size)
 		bbio->bi_seg_back_size = seg_size;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return nr_phys_segs;
 }
 
 void blk_recalc_rq_segments(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bool no_sg_merge = !!test_bit(QUEUE_FLAG_NO_SG_MERGE,
 			&rq->q->queue_flags);
 
@@ -303,7 +318,9 @@ void blk_recount_segments(struct request_queue *q, struct bio *bio)
 
 	/* estimate segment number by bi_vcnt for non-cloned bio */
 	if (bio_flagged(bio, BIO_CLONED))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		seg_cnt = bio_segments(bio);
+}
 	else
 		seg_cnt = bio->bi_vcnt;
 
@@ -318,6 +335,7 @@ void blk_recount_segments(struct request_queue *q, struct bio *bio)
 		bio->bi_next = nxt;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_set_flag(bio, BIO_SEG_VALID);
 }
 EXPORT_SYMBOL(blk_recount_segments);
@@ -431,9 +449,13 @@ int blk_rq_map_sg(struct request_queue *q, struct request *rq,
 	int nsegs = 0;
 
 	if (rq->rq_flags & RQF_SPECIAL_PAYLOAD)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nsegs = __blk_bvec_map_sg(q, rq->special_vec, sglist, &sg);
+}
 	else if (rq->bio && bio_op(rq->bio) == REQ_OP_WRITE_SAME)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		nsegs = __blk_bvec_map_sg(q, bio_iovec(rq->bio), sglist, &sg);
+}
 	else if (rq->bio)
 		nsegs = __blk_bios_map_sg(q, rq->bio, sglist, &sg);
 
@@ -448,7 +470,9 @@ int blk_rq_map_sg(struct request_queue *q, struct request *rq,
 
 	if (q->dma_drain_size && q->dma_drain_needed(rq)) {
 		if (op_is_write(req_op(rq)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			memset(q->dma_drain_buffer, 0, q->dma_drain_size);
+}
 
 		sg_unmark_end(sg);
 		sg = sg_next(sg);
@@ -477,6 +501,7 @@ static inline int ll_new_hw_segment(struct request_queue *q,
 				    struct request *req,
 				    struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int nr_phys_segs = bio_phys_segments(q, bio);
 
 	if (req->nr_phys_segments + nr_phys_segs > queue_max_segments(q))
@@ -500,6 +525,7 @@ static inline int ll_new_hw_segment(struct request_queue *q,
 int ll_back_merge_fn(struct request_queue *q, struct request *req,
 		     struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (req_gap_back_merge(req, bio))
 		return 0;
 	if (blk_integrity_rq(req) &&
@@ -522,6 +548,7 @@ int ll_front_merge_fn(struct request_queue *q, struct request *req,
 		      struct bio *bio)
 {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (req_gap_front_merge(req, bio))
 		return 0;
 	if (blk_integrity_rq(req) &&
@@ -548,6 +575,7 @@ static bool req_no_special_merge(struct request *req)
 {
 	struct request_queue *q = req->q;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return !q->mq_ops && req->special;
 }
 
@@ -610,7 +638,9 @@ void blk_rq_set_mixed_merge(struct request *rq)
 	struct bio *bio;
 
 	if (rq->rq_flags & RQF_MIXED_MERGE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * @rq will no longer represent mixable attributes for all the
@@ -627,6 +657,7 @@ void blk_rq_set_mixed_merge(struct request *rq)
 
 static void blk_account_io_merge(struct request *req)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (blk_do_io_stat(req)) {
 		struct hd_struct *part;
 		int cpu;
@@ -649,6 +680,7 @@ static void blk_account_io_merge(struct request *req)
 static struct request *attempt_merge(struct request_queue *q,
 				     struct request *req, struct request *next)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!q->mq_ops)
 		lockdep_assert_held(q->queue_lock);
 
@@ -737,6 +769,7 @@ static struct request *attempt_merge(struct request_queue *q,
 
 struct request *attempt_back_merge(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request *next = elv_latter_request(q, rq);
 
 	if (next)
@@ -747,6 +780,7 @@ struct request *attempt_back_merge(struct request_queue *q, struct request *rq)
 
 struct request *attempt_front_merge(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request *prev = elv_former_request(q, rq);
 
 	if (prev)
@@ -761,6 +795,7 @@ int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 	struct elevator_queue *e = q->elevator;
 	struct request *free;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!e->uses_mq && e->type->ops.sq.elevator_allow_rq_merge_fn)
 		if (!e->type->ops.sq.elevator_allow_rq_merge_fn(q, rq, next))
 			return 0;
@@ -776,6 +811,7 @@ int blk_attempt_req_merge(struct request_queue *q, struct request *rq,
 
 bool blk_rq_merge_ok(struct request *rq, struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!rq_mergeable(rq) || !bio_mergeable(bio))
 		return false;
 
@@ -811,6 +847,7 @@ bool blk_rq_merge_ok(struct request *rq, struct bio *bio)
 
 enum elv_merge blk_try_merge(struct request *rq, struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (req_op(rq) == REQ_OP_DISCARD &&
 	    queue_max_discard_segments(rq->q) > 1)
 		return ELEVATOR_DISCARD_MERGE;
diff --git a/block/blk-mq-cpumap.c b/block/blk-mq-cpumap.c
index 9f8cffc..21362694 100644
--- a/block/blk-mq-cpumap.c
+++ b/block/blk-mq-cpumap.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * CPU <-> hardware queue mapping helpers
  *
@@ -20,7 +22,9 @@ static int cpu_to_queue_index(unsigned int nr_queues, const int cpu)
 	 * Non present CPU will be mapped to queue index 0.
 	 */
 	if (!cpu_present(cpu))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	return cpu % nr_queues;
 }
 
@@ -28,6 +32,7 @@ static int get_first_sibling(unsigned int cpu)
 {
 	unsigned int ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = cpumask_first(topology_sibling_cpumask(cpu));
 	if (ret < nr_cpu_ids)
 		return ret;
@@ -51,9 +56,12 @@ int blk_mq_map_queues(struct blk_mq_tag_set *set)
 		if (cpu < nr_queues) {
 			map[cpu] = cpu_to_queue_index(nr_queues, cpu);
 		} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			first_sibling = get_first_sibling(cpu);
 			if (first_sibling == cpu)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				map[cpu] = cpu_to_queue_index(nr_queues, cpu);
+}
 			else
 				map[cpu] = map[first_sibling];
 		}
@@ -73,8 +81,11 @@ int blk_mq_hw_queue_to_node(unsigned int *mq_map, unsigned int index)
 
 	for_each_possible_cpu(i) {
 		if (index == mq_map[i])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return local_memory_node(cpu_to_node(i));
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NUMA_NO_NODE;
 }
diff --git a/block/blk-mq-debugfs.c b/block/blk-mq-debugfs.c
index de294d7..bdf8c9a 100644
--- a/block/blk-mq-debugfs.c
+++ b/block/blk-mq-debugfs.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Copyright (C) 2017 Facebook
  *
@@ -30,6 +32,7 @@ static int blk_flags_show(struct seq_file *m, const unsigned long flags,
 	bool sep = false;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < sizeof(flags) * BITS_PER_BYTE; i++) {
 		if (!(flags & BIT(i)))
 			continue;
@@ -127,6 +130,7 @@ static ssize_t queue_state_write(void *data, const char __user *buf,
 
 static void print_stat(struct seq_file *m, struct blk_rq_stat *stat)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (stat->nr_samples) {
 		seq_printf(m, "samples=%d, mean=%lld, min=%llu, max=%llu",
 			   stat->nr_samples, stat->mean, stat->min, stat->max);
@@ -140,6 +144,7 @@ static int queue_write_hint_show(void *data, struct seq_file *m)
 	struct request_queue *q = data;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < BLK_MAX_WRITE_HINTS; i++)
 		seq_printf(m, "hint%d: %llu\n", i, q->write_hints[i]);
 
@@ -152,6 +157,7 @@ static ssize_t queue_write_hint_store(void *data, const char __user *buf,
 	struct request_queue *q = data;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < BLK_MAX_WRITE_HINTS; i++)
 		q->write_hints[i] = 0;
 
@@ -163,6 +169,7 @@ static int queue_poll_stat_show(void *data, struct seq_file *m)
 	struct request_queue *q = data;
 	int bucket;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (bucket = 0; bucket < BLK_MQ_POLL_STATS_BKTS/2; bucket++) {
 		seq_printf(m, "read  (%d Bytes): ", 1 << (9+bucket));
 		print_stat(m, &q->poll_stat[2*bucket]);
@@ -307,6 +314,7 @@ int __blk_mq_debugfs_rq_show(struct seq_file *m, struct request *rq)
 	const unsigned int op = rq->cmd_flags & REQ_OP_MASK;
 
 	seq_printf(m, "%p {.op=", rq);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (op < ARRAY_SIZE(op_name) && op_name[op])
 		seq_printf(m, "%s", op_name[op]);
 	else
@@ -330,6 +338,7 @@ EXPORT_SYMBOL_GPL(__blk_mq_debugfs_rq_show);
 
 int blk_mq_debugfs_rq_show(struct seq_file *m, void *v)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __blk_mq_debugfs_rq_show(m, list_entry_rq(v));
 }
 EXPORT_SYMBOL_GPL(blk_mq_debugfs_rq_show);
@@ -437,6 +446,7 @@ static int hctx_ctx_map_show(void *data, struct seq_file *m)
 static void blk_mq_debugfs_tags_show(struct seq_file *m,
 				     struct blk_mq_tags *tags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	seq_printf(m, "nr_tags=%u\n", tags->nr_tags);
 	seq_printf(m, "nr_reserved_tags=%u\n", tags->nr_reserved_tags);
 	seq_printf(m, "active_queues=%d\n",
@@ -460,6 +470,7 @@ static int hctx_tags_show(void *data, struct seq_file *m)
 	res = mutex_lock_interruptible(&q->sysfs_lock);
 	if (res)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hctx->tags)
 		blk_mq_debugfs_tags_show(m, hctx->tags);
 	mutex_unlock(&q->sysfs_lock);
@@ -477,6 +488,7 @@ static int hctx_tags_bitmap_show(void *data, struct seq_file *m)
 	res = mutex_lock_interruptible(&q->sysfs_lock);
 	if (res)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hctx->tags)
 		sbitmap_bitmap_show(&hctx->tags->bitmap_tags.sb, m);
 	mutex_unlock(&q->sysfs_lock);
@@ -494,6 +506,7 @@ static int hctx_sched_tags_show(void *data, struct seq_file *m)
 	res = mutex_lock_interruptible(&q->sysfs_lock);
 	if (res)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hctx->sched_tags)
 		blk_mq_debugfs_tags_show(m, hctx->sched_tags);
 	mutex_unlock(&q->sysfs_lock);
@@ -511,6 +524,7 @@ static int hctx_sched_tags_bitmap_show(void *data, struct seq_file *m)
 	res = mutex_lock_interruptible(&q->sysfs_lock);
 	if (res)
 		goto out;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hctx->sched_tags)
 		sbitmap_bitmap_show(&hctx->sched_tags->bitmap_tags.sb, m);
 	mutex_unlock(&q->sysfs_lock);
@@ -545,6 +559,7 @@ static int hctx_dispatched_show(void *data, struct seq_file *m)
 
 	seq_printf(m, "%8u\t%lu\n", 0U, hctx->dispatched[0]);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 1; i < BLK_MQ_MAX_DISPATCH_ORDER - 1; i++) {
 		unsigned int d = 1U << (i - 1);
 
@@ -561,6 +576,7 @@ static ssize_t hctx_dispatched_write(void *data, const char __user *buf,
 	struct blk_mq_hw_ctx *hctx = data;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < BLK_MQ_MAX_DISPATCH_ORDER; i++)
 		hctx->dispatched[i] = 0;
 	return count;
@@ -705,7 +721,9 @@ static ssize_t blk_mq_debugfs_write(struct file *file, const char __user *buf,
 	void *data = d_inode(file->f_path.dentry->d_parent)->i_private;
 
 	if (!attr->write)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 
 	return attr->write(data, buf, count, ppos);
 }
@@ -718,6 +736,7 @@ static int blk_mq_debugfs_open(struct inode *inode, struct file *file)
 	int ret;
 
 	if (attr->seq_ops) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = seq_open(file, attr->seq_ops);
 		if (!ret) {
 			m = file->private_data;
@@ -737,7 +756,9 @@ static int blk_mq_debugfs_release(struct inode *inode, struct file *file)
 	const struct blk_mq_debugfs_attr *attr = inode->i_private;
 
 	if (attr->show)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return single_release(inode, file);
+}
 	else
 		return seq_release(inode, file);
 }
@@ -794,6 +815,7 @@ static bool debugfs_create_files(struct dentry *parent, void *data,
 					 (void *)attr, &blk_mq_debugfs_fops))
 			return false;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return true;
 }
 
@@ -803,12 +825,16 @@ int blk_mq_debugfs_register(struct request_queue *q)
 	int i;
 
 	if (!blk_debugfs_root)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOENT;
+}
 
 	q->debugfs_dir = debugfs_create_dir(kobject_name(q->kobj.parent),
 					    blk_debugfs_root);
 	if (!q->debugfs_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (!debugfs_create_files(q->debugfs_dir, q,
 				  blk_mq_debugfs_queue_attrs))
@@ -831,6 +857,7 @@ int blk_mq_debugfs_register(struct request_queue *q)
 			goto err;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 err:
@@ -840,6 +867,7 @@ int blk_mq_debugfs_register(struct request_queue *q)
 
 void blk_mq_debugfs_unregister(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	debugfs_remove_recursive(q->debugfs_dir);
 	q->sched_debugfs_dir = NULL;
 	q->debugfs_dir = NULL;
@@ -854,11 +882,16 @@ static int blk_mq_debugfs_register_ctx(struct blk_mq_hw_ctx *hctx,
 	snprintf(name, sizeof(name), "cpu%u", ctx->cpu);
 	ctx_dir = debugfs_create_dir(name, hctx->debugfs_dir);
 	if (!ctx_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (!debugfs_create_files(ctx_dir, ctx, blk_mq_debugfs_ctx_attrs))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
@@ -870,12 +903,16 @@ int blk_mq_debugfs_register_hctx(struct request_queue *q,
 	int i;
 
 	if (!q->debugfs_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOENT;
+}
 
 	snprintf(name, sizeof(name), "hctx%u", hctx->queue_num);
 	hctx->debugfs_dir = debugfs_create_dir(name, q->debugfs_dir);
 	if (!hctx->debugfs_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (!debugfs_create_files(hctx->debugfs_dir, hctx,
 				  blk_mq_debugfs_hctx_attrs))
@@ -886,6 +923,7 @@ int blk_mq_debugfs_register_hctx(struct request_queue *q,
 			goto err;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 err:
@@ -895,6 +933,7 @@ int blk_mq_debugfs_register_hctx(struct request_queue *q,
 
 void blk_mq_debugfs_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	debugfs_remove_recursive(hctx->debugfs_dir);
 	hctx->sched_debugfs_dir = NULL;
 	hctx->debugfs_dir = NULL;
@@ -905,6 +944,7 @@ int blk_mq_debugfs_register_hctxs(struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (blk_mq_debugfs_register_hctx(q, hctx))
 			return -ENOMEM;
@@ -918,6 +958,7 @@ void blk_mq_debugfs_unregister_hctxs(struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_debugfs_unregister_hctx(hctx);
 }
@@ -927,19 +968,26 @@ int blk_mq_debugfs_register_sched(struct request_queue *q)
 	struct elevator_type *e = q->elevator->type;
 
 	if (!q->debugfs_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOENT;
+}
 
 	if (!e->queue_debugfs_attrs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	q->sched_debugfs_dir = debugfs_create_dir("sched", q->debugfs_dir);
 	if (!q->sched_debugfs_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	if (!debugfs_create_files(q->sched_debugfs_dir, q,
 				  e->queue_debugfs_attrs))
 		goto err;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 err:
@@ -949,6 +997,7 @@ int blk_mq_debugfs_register_sched(struct request_queue *q)
 
 void blk_mq_debugfs_unregister_sched(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	debugfs_remove_recursive(q->sched_debugfs_dir);
 	q->sched_debugfs_dir = NULL;
 }
@@ -959,25 +1008,35 @@ int blk_mq_debugfs_register_sched_hctx(struct request_queue *q,
 	struct elevator_type *e = q->elevator->type;
 
 	if (!hctx->debugfs_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOENT;
+}
 
 	if (!e->hctx_debugfs_attrs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	hctx->sched_debugfs_dir = debugfs_create_dir("sched",
 						     hctx->debugfs_dir);
 	if (!hctx->sched_debugfs_dir)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!debugfs_create_files(hctx->sched_debugfs_dir, hctx,
 				  e->hctx_debugfs_attrs))
 		return -ENOMEM;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
 void blk_mq_debugfs_unregister_sched_hctx(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	debugfs_remove_recursive(hctx->sched_debugfs_dir);
 	hctx->sched_debugfs_dir = NULL;
 }
diff --git a/block/blk-mq-sched.c b/block/blk-mq-sched.c
index eca011f..8f1d6ce 100644
--- a/block/blk-mq-sched.c
+++ b/block/blk-mq-sched.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * blk-mq scheduling framework
  *
@@ -22,6 +24,7 @@ void blk_mq_sched_free_hctx_data(struct request_queue *q,
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (exit && hctx->sched_data)
 			exit(hctx);
@@ -42,6 +45,7 @@ void blk_mq_sched_assign_ioc(struct request *rq, struct bio *bio)
 	spin_unlock_irq(q->queue_lock);
 
 	if (!icq) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		icq = ioc_create_icq(ioc, q, GFP_ATOMIC);
 		if (!icq)
 			return;
@@ -56,6 +60,7 @@ void blk_mq_sched_assign_ioc(struct request *rq, struct bio *bio)
  */
 static void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return;
 
@@ -70,6 +75,7 @@ static void blk_mq_sched_mark_restart_hctx(struct blk_mq_hw_ctx *hctx)
 
 static bool blk_mq_sched_restart_hctx(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
 		return false;
 
@@ -93,6 +99,7 @@ void blk_mq_sched_dispatch_requests(struct blk_mq_hw_ctx *hctx)
 {
 	struct request_queue *q = hctx->queue;
 	struct elevator_queue *e = q->elevator;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const bool has_sched_dispatch = e && e->type->ops.mq.dispatch_request;
 	bool do_sched_dispatch = true;
 	LIST_HEAD(rq_list);
@@ -189,6 +196,7 @@ static bool blk_mq_attempt_merge(struct request_queue *q,
 	struct request *rq;
 	int checked = 8;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&ctx->lock);
 
 	list_for_each_entry_reverse(rq, &ctx->rq_list, queuelist) {
@@ -231,6 +239,7 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
 	bool ret = false;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (e && e->type->ops.mq.bio_merge) {
 		blk_mq_put_ctx(ctx);
 		return e->type->ops.mq.bio_merge(hctx, bio);
@@ -249,12 +258,14 @@ bool __blk_mq_sched_bio_merge(struct request_queue *q, struct bio *bio)
 
 bool blk_mq_sched_try_insert_merge(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return rq_mergeable(rq) && elv_attempt_insert_merge(q, rq);
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_try_insert_merge);
 
 void blk_mq_sched_request_inserted(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	trace_block_rq_insert(rq->q, rq);
 }
 EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
@@ -262,6 +273,7 @@ EXPORT_SYMBOL_GPL(blk_mq_sched_request_inserted);
 static bool blk_mq_sched_bypass_insert(struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rq->tag == -1) {
 		rq->rq_flags |= RQF_SORTED;
 		return false;
@@ -347,6 +359,7 @@ void blk_mq_sched_restart(struct blk_mq_hw_ctx *const hctx)
 static void blk_mq_sched_insert_flush(struct blk_mq_hw_ctx *hctx,
 				      struct request *rq, bool can_block)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (blk_mq_get_driver_tag(rq, &hctx, can_block)) {
 		blk_insert_flush(rq);
 		blk_mq_run_hw_queue(hctx, true);
@@ -362,6 +375,7 @@ void blk_mq_sched_insert_request(struct request *rq, bool at_head,
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rq->tag == -1 && op_is_flush(rq->cmd_flags)) {
 		blk_mq_sched_insert_flush(hctx, rq, can_block);
 		return;
@@ -390,6 +404,7 @@ void blk_mq_sched_insert_requests(struct request_queue *q,
 				  struct blk_mq_ctx *ctx,
 				  struct list_head *list, bool run_queue_async)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct blk_mq_hw_ctx *hctx = blk_mq_map_queue(q, ctx->cpu);
 	struct elevator_queue *e = hctx->queue->elevator;
 
@@ -422,6 +437,7 @@ static void blk_mq_sched_free_tags(struct blk_mq_tag_set *set,
 				   struct blk_mq_hw_ctx *hctx,
 				   unsigned int hctx_idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hctx->sched_tags) {
 		blk_mq_free_rqs(set, hctx->sched_tags, hctx_idx);
 		blk_mq_free_rq_map(hctx->sched_tags);
@@ -439,12 +455,17 @@ static int blk_mq_sched_alloc_tags(struct request_queue *q,
 	hctx->sched_tags = blk_mq_alloc_rq_map(set, hctx_idx, q->nr_requests,
 					       set->reserved_tags);
 	if (!hctx->sched_tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	ret = blk_mq_alloc_rqs(set, hctx->sched_tags, hctx_idx, q->nr_requests);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_mq_sched_free_tags(set, hctx, hctx_idx);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -454,6 +475,7 @@ static void blk_mq_sched_tags_teardown(struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_sched_free_tags(set, hctx, i);
 }
@@ -465,20 +487,29 @@ int blk_mq_sched_init_hctx(struct request_queue *q, struct blk_mq_hw_ctx *hctx,
 	int ret;
 
 	if (!e)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = blk_mq_sched_alloc_tags(q, hctx, hctx_idx);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (e->type->ops.mq.init_hctx) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = e->type->ops.mq.init_hctx(hctx, hctx_idx);
 		if (ret) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			blk_mq_sched_free_tags(q->tag_set, hctx, hctx_idx);
 			return ret;
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_mq_debugfs_register_sched_hctx(q, hctx);
 
 	return 0;
@@ -490,7 +521,9 @@ void blk_mq_sched_exit_hctx(struct request_queue *q, struct blk_mq_hw_ctx *hctx,
 	struct elevator_queue *e = q->elevator;
 
 	if (!e)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	blk_mq_debugfs_unregister_sched_hctx(hctx);
 
@@ -510,6 +543,7 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 	int ret;
 
 	if (!e) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		q->elevator = NULL;
 		return 0;
 	}
@@ -536,8 +570,10 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (e->ops.mq.init_hctx) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = e->ops.mq.init_hctx(hctx, i);
 			if (ret) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				eq = q->elevator;
 				blk_mq_exit_sched(q, eq);
 				kobject_put(&eq->kobj);
@@ -547,6 +583,7 @@ int blk_mq_init_sched(struct request_queue *q, struct elevator_type *e)
 		blk_mq_debugfs_register_sched_hctx(q, hctx);
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 err:
@@ -560,6 +597,7 @@ void blk_mq_exit_sched(struct request_queue *q, struct elevator_queue *e)
 	struct blk_mq_hw_ctx *hctx;
 	unsigned int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
 		blk_mq_debugfs_unregister_sched_hctx(hctx);
 		if (e->type->ops.mq.exit_hctx && hctx->sched_data) {
diff --git a/block/blk-mq-sysfs.c b/block/blk-mq-sysfs.c
index 79969c3..2afbc79 100644
--- a/block/blk-mq-sysfs.c
+++ b/block/blk-mq-sysfs.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/backing-dev.h>
@@ -19,6 +21,7 @@ static void blk_mq_sysfs_release(struct kobject *kobj)
 
 static void blk_mq_hw_sysfs_release(struct kobject *kobj)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct blk_mq_hw_ctx *hctx = container_of(kobj, struct blk_mq_hw_ctx,
 						  kobj);
 	free_cpumask_var(hctx->cpumask);
@@ -46,6 +49,7 @@ static ssize_t blk_mq_sysfs_show(struct kobject *kobj, struct attribute *attr,
 	struct request_queue *q;
 	ssize_t res;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	entry = container_of(attr, struct blk_mq_ctx_sysfs_entry, attr);
 	ctx = container_of(kobj, struct blk_mq_ctx, kobj);
 	q = ctx->queue;
@@ -69,6 +73,7 @@ static ssize_t blk_mq_sysfs_store(struct kobject *kobj, struct attribute *attr,
 	struct request_queue *q;
 	ssize_t res;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	entry = container_of(attr, struct blk_mq_ctx_sysfs_entry, attr);
 	ctx = container_of(kobj, struct blk_mq_ctx, kobj);
 	q = ctx->queue;
@@ -92,6 +97,7 @@ static ssize_t blk_mq_hw_sysfs_show(struct kobject *kobj,
 	struct request_queue *q;
 	ssize_t res;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	entry = container_of(attr, struct blk_mq_hw_ctx_sysfs_entry, attr);
 	hctx = container_of(kobj, struct blk_mq_hw_ctx, kobj);
 	q = hctx->queue;
@@ -116,6 +122,7 @@ static ssize_t blk_mq_hw_sysfs_store(struct kobject *kobj,
 	struct request_queue *q;
 	ssize_t res;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	entry = container_of(attr, struct blk_mq_hw_ctx_sysfs_entry, attr);
 	hctx = container_of(kobj, struct blk_mq_hw_ctx, kobj);
 	q = hctx->queue;
@@ -134,12 +141,14 @@ static ssize_t blk_mq_hw_sysfs_store(struct kobject *kobj,
 static ssize_t blk_mq_hw_sysfs_nr_tags_show(struct blk_mq_hw_ctx *hctx,
 					    char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%u\n", hctx->tags->nr_tags);
 }
 
 static ssize_t blk_mq_hw_sysfs_nr_reserved_tags_show(struct blk_mq_hw_ctx *hctx,
 						     char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%u\n", hctx->tags->nr_reserved_tags);
 }
 
@@ -148,6 +157,7 @@ static ssize_t blk_mq_hw_sysfs_cpus_show(struct blk_mq_hw_ctx *hctx, char *page)
 	unsigned int i, first = 1;
 	ssize_t ret = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_cpu(i, hctx->cpumask) {
 		if (first)
 			ret += sprintf(ret + page, "%u", i);
@@ -218,7 +228,9 @@ static void blk_mq_unregister_hctx(struct blk_mq_hw_ctx *hctx)
 	int i;
 
 	if (!hctx->nr_ctx)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	hctx_for_each_ctx(hctx, ctx, i)
 		kobject_del(&ctx->kobj);
@@ -233,11 +245,15 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 	int i, ret;
 
 	if (!hctx->nr_ctx)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	ret = kobject_add(&hctx->kobj, &q->mq_kobj, "%u", hctx->queue_num);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	hctx_for_each_ctx(hctx, ctx, i) {
 		ret = kobject_add(&ctx->kobj, &hctx->kobj, "cpu%u", ctx->cpu);
@@ -245,6 +261,7 @@ static int blk_mq_register_hctx(struct blk_mq_hw_ctx *hctx)
 			break;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -253,6 +270,7 @@ static void __blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&q->sysfs_lock);
 
 	queue_for_each_hw_ctx(q, hctx, i)
@@ -267,6 +285,7 @@ static void __blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 
 void blk_mq_unregister_dev(struct device *dev, struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mutex_lock(&q->sysfs_lock);
 	__blk_mq_unregister_dev(dev, q);
 	mutex_unlock(&q->sysfs_lock);
@@ -282,6 +301,7 @@ void blk_mq_sysfs_deinit(struct request_queue *q)
 	struct blk_mq_ctx *ctx;
 	int cpu;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_possible_cpu(cpu) {
 		ctx = per_cpu_ptr(q->queue_ctx, cpu);
 		kobject_put(&ctx->kobj);
@@ -308,6 +328,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	int ret, i;
 
 	WARN_ON_ONCE(!q->kobj.parent);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&q->sysfs_lock);
 
 	ret = kobject_add(&q->mq_kobj, kobject_get(&dev->kobj), "%s", "mq");
@@ -331,6 +352,7 @@ int __blk_mq_register_dev(struct device *dev, struct request_queue *q)
 	while (--i >= 0)
 		blk_mq_unregister_hctx(q->queue_hw_ctx[i]);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kobject_uevent(&q->mq_kobj, KOBJ_REMOVE);
 	kobject_del(&q->mq_kobj);
 	kobject_put(&dev->kobj);
@@ -358,6 +380,7 @@ void blk_mq_sysfs_unregister(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_unregister_hctx(hctx);
 
@@ -374,7 +397,9 @@ int blk_mq_sysfs_register(struct request_queue *q)
 	if (!q->mq_sysfs_init_done)
 		goto unlock;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = blk_mq_register_hctx(hctx);
 		if (ret)
 			break;
diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c
index 6714507..73f8da1 100644
--- a/block/blk-mq-tag.c
+++ b/block/blk-mq-tag.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Tag allocation using scalable bitmaps. Uses active queue tracking to support
  * fairer distribution of tags between multiple submitters when a shared tag map
@@ -15,6 +17,7 @@
 
 bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!tags)
 		return true;
 
@@ -26,6 +29,7 @@ bool blk_mq_has_free_tags(struct blk_mq_tags *tags)
  */
 bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state) &&
 	    !test_and_set_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
 		atomic_inc(&hctx->tags->active_queues);
@@ -38,6 +42,7 @@ bool __blk_mq_tag_busy(struct blk_mq_hw_ctx *hctx)
  */
 void blk_mq_tag_wakeup_all(struct blk_mq_tags *tags, bool include_reserve)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	sbitmap_queue_wake_all(&tags->bitmap_tags);
 	if (include_reserve)
 		sbitmap_queue_wake_all(&tags->breserved_tags);
@@ -52,7 +57,9 @@ void __blk_mq_tag_idle(struct blk_mq_hw_ctx *hctx)
 	struct blk_mq_tags *tags = hctx->tags;
 
 	if (!test_and_clear_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	atomic_dec(&tags->active_queues);
 
@@ -68,6 +75,7 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 {
 	unsigned int depth, users;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!hctx || !(hctx->flags & BLK_MQ_F_TAG_SHARED))
 		return true;
 	if (!test_bit(BLK_MQ_S_TAG_ACTIVE, &hctx->state))
@@ -93,6 +101,7 @@ static inline bool hctx_may_queue(struct blk_mq_hw_ctx *hctx,
 static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 			    struct sbitmap_queue *bt)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!(data->flags & BLK_MQ_REQ_INTERNAL) &&
 	    !hctx_may_queue(data->hctx, bt))
 		return -1;
@@ -104,6 +113,7 @@ static int __blk_mq_get_tag(struct blk_mq_alloc_data *data,
 
 unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct sbitmap_queue *bt;
 	struct sbq_wait_state *ws;
@@ -184,6 +194,7 @@ unsigned int blk_mq_get_tag(struct blk_mq_alloc_data *data)
 void blk_mq_put_tag(struct blk_mq_hw_ctx *hctx, struct blk_mq_tags *tags,
 		    struct blk_mq_ctx *ctx, unsigned int tag)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!blk_mq_tag_is_reserved(tags, tag)) {
 		const int real_tag = tag - tags->nr_reserved_tags;
 
@@ -211,7 +222,9 @@ static bool bt_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	struct request *rq;
 
 	if (!reserved)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bitnr += tags->nr_reserved_tags;
+}
 	rq = tags->rqs[bitnr];
 
 	/*
@@ -251,7 +264,9 @@ static bool bt_tags_iter(struct sbitmap *bitmap, unsigned int bitnr, void *data)
 	struct request *rq;
 
 	if (!reserved)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bitnr += tags->nr_reserved_tags;
+}
 
 	/*
 	 * We can hit rq == NULL here, because the tagging functions
@@ -275,12 +290,15 @@ static void bt_tags_for_each(struct blk_mq_tags *tags, struct sbitmap_queue *bt,
 	};
 
 	if (tags->rqs)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sbitmap_for_each_set(&bt->sb, bt_tags_iter, &iter_data);
 }
+}
 
 static void blk_mq_all_tag_busy_iter(struct blk_mq_tags *tags,
 		busy_tag_iter_fn *fn, void *priv)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (tags->nr_reserved_tags)
 		bt_tags_for_each(tags, &tags->breserved_tags, fn, priv, true);
 	bt_tags_for_each(tags, &tags->bitmap_tags, fn, priv, false);
@@ -291,6 +309,7 @@ void blk_mq_tagset_busy_iter(struct blk_mq_tag_set *tagset,
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < tagset->nr_hw_queues; i++) {
 		if (tagset->tags && tagset->tags[i])
 			blk_mq_all_tag_busy_iter(tagset->tags[i], fn, priv);
@@ -303,6 +322,7 @@ int blk_mq_reinit_tagset(struct blk_mq_tag_set *set,
 {
 	int i, j, ret = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(!reinit_request))
 		goto out;
 
@@ -346,7 +366,9 @@ void blk_mq_queue_tag_busy_iter(struct request_queue *q, busy_iter_fn *fn,
 			continue;
 
 		if (tags->nr_reserved_tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			bt_for_each(hctx, &tags->breserved_tags, fn, priv, true);
+}
 		bt_for_each(hctx, &tags->bitmap_tags, fn, priv, false);
 	}
 
@@ -371,6 +393,7 @@ static struct blk_mq_tags *blk_mq_init_bitmap_tags(struct blk_mq_tags *tags,
 		     node))
 		goto free_bitmap_tags;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return tags;
 free_bitmap_tags:
 	sbitmap_queue_free(&tags->bitmap_tags);
@@ -386,13 +409,16 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 	struct blk_mq_tags *tags;
 
 	if (total_tags > BLK_MQ_TAG_MAX) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("blk-mq: tag depth too large\n");
 		return NULL;
 	}
 
 	tags = kzalloc_node(sizeof(*tags), GFP_KERNEL, node);
 	if (!tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	tags->nr_tags = total_tags;
 	tags->nr_reserved_tags = reserved_tags;
@@ -402,6 +428,7 @@ struct blk_mq_tags *blk_mq_init_tags(unsigned int total_tags,
 
 void blk_mq_free_tags(struct blk_mq_tags *tags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	sbitmap_queue_free(&tags->bitmap_tags);
 	sbitmap_queue_free(&tags->breserved_tags);
 	kfree(tags);
@@ -414,7 +441,9 @@ int blk_mq_tag_update_depth(struct blk_mq_hw_ctx *hctx,
 	struct blk_mq_tags *tags = *tagsptr;
 
 	if (tdepth <= tags->nr_reserved_tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	tdepth -= tags->nr_reserved_tags;
 
@@ -479,6 +508,7 @@ u32 blk_mq_unique_tag(struct request *rq)
 	int hwq = 0;
 
 	if (q->mq_ops) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		hctx = blk_mq_map_queue(q, rq->mq_ctx->cpu);
 		hwq = hctx->queue_num;
 	}
diff --git a/block/blk-mq.c b/block/blk-mq.c
index b60798a..905067a 100644
--- a/block/blk-mq.c
+++ b/block/blk-mq.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Block multiqueue core code
  *
@@ -44,6 +46,7 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
 {
 	int ddir, bytes, bucket;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ddir = rq_data_dir(rq);
 	bytes = blk_rq_bytes(rq);
 
@@ -62,6 +65,7 @@ static int blk_mq_poll_stats_bkt(const struct request *rq)
  */
 bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sbitmap_any_bit_set(&hctx->ctx_map) ||
 			!list_empty_careful(&hctx->dispatch) ||
 			blk_mq_sched_has_work(hctx);
@@ -73,6 +77,7 @@ bool blk_mq_hctx_has_pending(struct blk_mq_hw_ctx *hctx)
 static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 				     struct blk_mq_ctx *ctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!sbitmap_test_bit(&hctx->ctx_map, ctx->index_hw))
 		sbitmap_set_bit(&hctx->ctx_map, ctx->index_hw);
 }
@@ -80,6 +85,7 @@ static void blk_mq_hctx_mark_pending(struct blk_mq_hw_ctx *hctx,
 static void blk_mq_hctx_clear_pending(struct blk_mq_hw_ctx *hctx,
 				      struct blk_mq_ctx *ctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	sbitmap_clear_bit(&hctx->ctx_map, ctx->index_hw);
 }
 
@@ -94,6 +100,7 @@ static void blk_mq_check_inflight(struct blk_mq_hw_ctx *hctx,
 {
 	struct mq_inflight *mi = priv;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(REQ_ATOM_STARTED, &rq->atomic_flags) &&
 	    !test_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags)) {
 		/*
@@ -124,6 +131,7 @@ void blk_freeze_queue_start(struct request_queue *q)
 
 	freeze_depth = atomic_inc_return(&q->mq_freeze_depth);
 	if (freeze_depth == 1) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		percpu_ref_kill(&q->q_usage_counter);
 		blk_mq_run_hw_queues(q, false);
 	}
@@ -132,6 +140,7 @@ EXPORT_SYMBOL_GPL(blk_freeze_queue_start);
 
 void blk_mq_freeze_queue_wait(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	wait_event(q->mq_freeze_wq, percpu_ref_is_zero(&q->q_usage_counter));
 }
 EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);
@@ -139,6 +148,7 @@ EXPORT_SYMBOL_GPL(blk_mq_freeze_queue_wait);
 int blk_mq_freeze_queue_wait_timeout(struct request_queue *q,
 				     unsigned long timeout)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return wait_event_timeout(q->mq_freeze_wq,
 					percpu_ref_is_zero(&q->q_usage_counter),
 					timeout);
@@ -179,6 +189,7 @@ void blk_mq_unfreeze_queue(struct request_queue *q)
 	int freeze_depth;
 
 	freeze_depth = atomic_dec_return(&q->mq_freeze_depth);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON_ONCE(freeze_depth < 0);
 	if (!freeze_depth) {
 		percpu_ref_reinit(&q->q_usage_counter);
@@ -195,6 +206,7 @@ void blk_mq_quiesce_queue_nowait(struct request_queue *q)
 {
 	unsigned long flags;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irqsave(q->queue_lock, flags);
 	queue_flag_set(QUEUE_FLAG_QUIESCED, q);
 	spin_unlock_irqrestore(q->queue_lock, flags);
@@ -218,6 +230,7 @@ void blk_mq_quiesce_queue(struct request_queue *q)
 
 	blk_mq_quiesce_queue_nowait(q);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (hctx->flags & BLK_MQ_F_BLOCKING)
 			synchronize_srcu(hctx->queue_rq_srcu);
@@ -240,6 +253,7 @@ void blk_mq_unquiesce_queue(struct request_queue *q)
 {
 	unsigned long flags;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irqsave(q->queue_lock, flags);
 	queue_flag_clear(QUEUE_FLAG_QUIESCED, q);
 	spin_unlock_irqrestore(q->queue_lock, flags);
@@ -254,6 +268,7 @@ void blk_mq_wake_waiters(struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	unsigned int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		if (blk_mq_hw_queue_mapped(hctx))
 			blk_mq_tag_wakeup_all(hctx->tags, true);
@@ -268,6 +283,7 @@ void blk_mq_wake_waiters(struct request_queue *q)
 
 bool blk_mq_can_queue(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return blk_mq_has_free_tags(hctx->tags);
 }
 EXPORT_SYMBOL(blk_mq_can_queue);
@@ -275,6 +291,7 @@ EXPORT_SYMBOL(blk_mq_can_queue);
 static struct request *blk_mq_rq_ctx_init(struct blk_mq_alloc_data *data,
 		unsigned int tag, unsigned int op)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct blk_mq_tags *tags = blk_mq_tags_from_data(data);
 	struct request *rq = tags->static_rqs[tag];
 
@@ -343,7 +360,9 @@ static struct request *blk_mq_get_request(struct request_queue *q,
 	blk_queue_enter_live(q);
 	data->q = q;
 	if (likely(!data->ctx))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		data->ctx = local_ctx = blk_mq_get_ctx(q);
+}
 	if (likely(!data->hctx))
 		data->hctx = blk_mq_map_queue(q, data->ctx->cpu);
 	if (op & REQ_NOWAIT)
@@ -394,7 +413,9 @@ struct request *blk_mq_alloc_request(struct request_queue *q, unsigned int op,
 
 	ret = blk_queue_enter(q, flags & BLK_MQ_REQ_NOWAIT);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(ret);
+}
 
 	rq = blk_mq_get_request(q, NULL, op, &alloc_data);
 	blk_queue_exit(q);
@@ -466,6 +487,7 @@ void blk_mq_free_request(struct request *rq)
 	const int sched_tag = rq->internal_tag;
 
 	if (rq->rq_flags & RQF_ELVPRIV) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (e && e->type->ops.mq.finish_request)
 			e->type->ops.mq.finish_request(rq);
 		if (rq->elv.icq) {
@@ -493,6 +515,7 @@ EXPORT_SYMBOL_GPL(blk_mq_free_request);
 
 inline void __blk_mq_end_request(struct request *rq, blk_status_t error)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_account_io_done(rq);
 
 	if (rq->end_io) {
@@ -508,6 +531,7 @@ EXPORT_SYMBOL(__blk_mq_end_request);
 
 void blk_mq_end_request(struct request *rq, blk_status_t error)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (blk_update_request(rq, error, blk_rq_bytes(rq)))
 		BUG();
 	__blk_mq_end_request(rq, error);
@@ -528,7 +552,9 @@ static void __blk_mq_complete_request(struct request *rq)
 	int cpu;
 
 	if (rq->internal_tag != -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_mq_sched_completed_request(rq);
+}
 	if (rq->rq_flags & RQF_STATS) {
 		blk_mq_poll_stats_start(rq->q);
 		blk_stat_add(rq);
@@ -567,7 +593,9 @@ void blk_mq_complete_request(struct request *rq)
 	struct request_queue *q = rq->q;
 
 	if (unlikely(blk_should_fake_timeout(q)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (!blk_mark_rq_complete(rq))
 		__blk_mq_complete_request(rq);
 }
@@ -575,6 +603,7 @@ EXPORT_SYMBOL(blk_mq_complete_request);
 
 int blk_mq_request_started(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return test_bit(REQ_ATOM_STARTED, &rq->atomic_flags);
 }
 EXPORT_SYMBOL_GPL(blk_mq_request_started);
@@ -587,6 +616,7 @@ void blk_mq_start_request(struct request *rq)
 
 	trace_block_rq_issue(q, rq);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(QUEUE_FLAG_STATS, &q->queue_flags)) {
 		blk_stat_set_issue(&rq->issue_stat, blk_rq_sectors(rq));
 		rq->rq_flags |= RQF_STATS;
@@ -641,6 +671,7 @@ static void __blk_mq_requeue_request(struct request *rq)
 	blk_mq_sched_requeue_request(rq);
 
 	if (test_and_clear_bit(REQ_ATOM_STARTED, &rq->atomic_flags)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (q->dma_drain_size && blk_rq_bytes(rq))
 			rq->nr_phys_segments--;
 	}
@@ -648,6 +679,7 @@ static void __blk_mq_requeue_request(struct request *rq)
 
 void blk_mq_requeue_request(struct request *rq, bool kick_requeue_list)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__blk_mq_requeue_request(rq);
 
 	BUG_ON(blk_queued_rq(rq));
@@ -712,6 +744,7 @@ EXPORT_SYMBOL(blk_mq_add_to_requeue_list);
 
 void blk_mq_kick_requeue_list(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kblockd_schedule_delayed_work(&q->requeue_work, 0);
 }
 EXPORT_SYMBOL(blk_mq_kick_requeue_list);
@@ -726,6 +759,7 @@ EXPORT_SYMBOL(blk_mq_delay_kick_requeue_list);
 
 struct request *blk_mq_tag_to_rq(struct blk_mq_tags *tags, unsigned int tag)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (tag < tags->nr_tags) {
 		prefetch(tags->rqs[tag]);
 		return tags->rqs[tag];
@@ -781,6 +815,7 @@ static void blk_mq_check_expired(struct blk_mq_hw_ctx *hctx,
 {
 	struct blk_mq_timeout_data *data = priv;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!test_bit(REQ_ATOM_STARTED, &rq->atomic_flags))
 		return;
 
@@ -884,6 +919,7 @@ EXPORT_SYMBOL_GPL(blk_mq_flush_busy_ctxs);
 
 static inline unsigned int queued_to_index(unsigned int queued)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!queued)
 		return 0;
 
@@ -925,6 +961,7 @@ bool blk_mq_get_driver_tag(struct request *rq, struct blk_mq_hw_ctx **hctx,
 static void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 				    struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_mq_put_tag(hctx, hctx->tags, rq->mq_ctx, rq->tag);
 	rq->tag = -1;
 
@@ -937,6 +974,7 @@ static void __blk_mq_put_driver_tag(struct blk_mq_hw_ctx *hctx,
 static void blk_mq_put_driver_tag_hctx(struct blk_mq_hw_ctx *hctx,
 				       struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rq->tag == -1 || rq->internal_tag == -1)
 		return;
 
@@ -947,6 +985,7 @@ static void blk_mq_put_driver_tag(struct request *rq)
 {
 	struct blk_mq_hw_ctx *hctx;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rq->tag == -1 || rq->internal_tag == -1)
 		return;
 
@@ -965,6 +1004,7 @@ static bool reorder_tags_to_front(struct list_head *list)
 {
 	struct request *rq, *tmp, *first = NULL;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry_safe_reverse(rq, tmp, list, queuelist) {
 		if (rq == first)
 			break;
@@ -983,6 +1023,7 @@ static int blk_mq_dispatch_wake(wait_queue_entry_t *wait, unsigned mode, int fla
 {
 	struct blk_mq_hw_ctx *hctx;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	hctx = container_of(wait, struct blk_mq_hw_ctx, dispatch_wait);
 
 	list_del(&wait->entry);
@@ -1023,7 +1064,9 @@ bool blk_mq_dispatch_rq_list(struct request_queue *q, struct list_head *list)
 	int errors, queued;
 
 	if (list_empty(list))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	/*
 	 * Now process all the entries, sending them to the driver.
@@ -1172,6 +1215,7 @@ static void __blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx)
  */
 static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (hctx->queue->nr_hw_queues == 1)
 		return WORK_CPU_UNBOUND;
 
@@ -1192,6 +1236,7 @@ static int blk_mq_hctx_next_cpu(struct blk_mq_hw_ctx *hctx)
 static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 					unsigned long msecs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(!blk_mq_hw_queue_mapped(hctx)))
 		return;
 
@@ -1216,12 +1261,14 @@ static void __blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async,
 
 void blk_mq_delay_run_hw_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__blk_mq_delay_run_hw_queue(hctx, true, msecs);
 }
 EXPORT_SYMBOL(blk_mq_delay_run_hw_queue);
 
 void blk_mq_run_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__blk_mq_delay_run_hw_queue(hctx, async, 0);
 }
 EXPORT_SYMBOL(blk_mq_run_hw_queue);
@@ -1231,6 +1278,7 @@ void blk_mq_run_hw_queues(struct request_queue *q, bool async)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (!blk_mq_hctx_has_pending(hctx) ||
 		    blk_mq_hctx_stopped(hctx))
@@ -1253,6 +1301,7 @@ bool blk_mq_queue_stopped(struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		if (blk_mq_hctx_stopped(hctx))
 			return true;
@@ -1272,6 +1321,7 @@ EXPORT_SYMBOL(blk_mq_queue_stopped);
  */
 void blk_mq_stop_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	cancel_delayed_work(&hctx->run_work);
 
 	set_bit(BLK_MQ_S_STOPPED, &hctx->state);
@@ -1292,6 +1342,7 @@ void blk_mq_stop_hw_queues(struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_stop_hw_queue(hctx);
 }
@@ -1299,6 +1350,7 @@ EXPORT_SYMBOL(blk_mq_stop_hw_queues);
 
 void blk_mq_start_hw_queue(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	clear_bit(BLK_MQ_S_STOPPED, &hctx->state);
 
 	blk_mq_run_hw_queue(hctx, false);
@@ -1310,6 +1362,7 @@ void blk_mq_start_hw_queues(struct request_queue *q)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_start_hw_queue(hctx);
 }
@@ -1317,6 +1370,7 @@ EXPORT_SYMBOL(blk_mq_start_hw_queues);
 
 void blk_mq_start_stopped_hw_queue(struct blk_mq_hw_ctx *hctx, bool async)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!blk_mq_hctx_stopped(hctx))
 		return;
 
@@ -1330,6 +1384,7 @@ void blk_mq_start_stopped_hw_queues(struct request_queue *q, bool async)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i)
 		blk_mq_start_stopped_hw_queue(hctx, async);
 }
@@ -1339,6 +1394,7 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 {
 	struct blk_mq_hw_ctx *hctx;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	hctx = container_of(work, struct blk_mq_hw_ctx, run_work.work);
 
 	/*
@@ -1360,6 +1416,7 @@ static void blk_mq_run_work_fn(struct work_struct *work)
 
 void blk_mq_delay_queue(struct blk_mq_hw_ctx *hctx, unsigned long msecs)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(!blk_mq_hw_queue_mapped(hctx)))
 		return;
 
@@ -1382,6 +1439,7 @@ static inline void __blk_mq_insert_req_list(struct blk_mq_hw_ctx *hctx,
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&ctx->lock);
 
 	trace_block_rq_insert(hctx->queue, rq);
@@ -1397,6 +1455,7 @@ void __blk_mq_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 {
 	struct blk_mq_ctx *ctx = rq->mq_ctx;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&ctx->lock);
 
 	__blk_mq_insert_req_list(hctx, rq, at_head);
@@ -1442,6 +1501,7 @@ void blk_mq_insert_requests(struct blk_mq_hw_ctx *hctx, struct blk_mq_ctx *ctx,
 
 static int plug_ctx_cmp(void *priv, struct list_head *a, struct list_head *b)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request *rqa = container_of(a, struct request, queuelist);
 	struct request *rqb = container_of(b, struct request, queuelist);
 
@@ -1467,6 +1527,7 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 	this_ctx = NULL;
 	depth = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!list_empty(&list)) {
 		rq = list_entry_rq(list.next);
 		list_del_init(&rq->queuelist);
@@ -1501,6 +1562,7 @@ void blk_mq_flush_plug_list(struct blk_plug *plug, bool from_schedule)
 
 static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_init_request_from_bio(rq, bio);
 
 	blk_account_io_start(rq, true);
@@ -1508,6 +1570,7 @@ static void blk_mq_bio_to_request(struct request *rq, struct bio *bio)
 
 static inline bool hctx_allow_merges(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (hctx->flags & BLK_MQ_F_SHOULD_MERGE) &&
 		!blk_queue_nomerges(hctx->queue);
 }
@@ -1516,6 +1579,7 @@ static inline void blk_mq_queue_io(struct blk_mq_hw_ctx *hctx,
 				   struct blk_mq_ctx *ctx,
 				   struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&ctx->lock);
 	__blk_mq_insert_request(hctx, rq, false);
 	spin_unlock(&ctx->lock);
@@ -1523,6 +1587,7 @@ static inline void blk_mq_queue_io(struct blk_mq_hw_ctx *hctx,
 
 static blk_qc_t request_to_qc_t(struct blk_mq_hw_ctx *hctx, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rq->tag != -1)
 		return blk_tag_to_qc_t(rq->tag, hctx->queue_num, false);
 
@@ -1582,6 +1647,7 @@ static void __blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 		struct request *rq, blk_qc_t *cookie)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!(hctx->flags & BLK_MQ_F_BLOCKING)) {
 		rcu_read_lock();
 		__blk_mq_try_issue_directly(hctx, rq, cookie, false);
@@ -1599,6 +1665,7 @@ static void blk_mq_try_issue_directly(struct blk_mq_hw_ctx *hctx,
 
 static blk_qc_t blk_mq_make_request(struct request_queue *q, struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const int is_sync = op_is_sync(bio->bi_opf);
 	const int is_flush_fua = op_is_flush(bio->bi_opf);
 	struct blk_mq_alloc_data data = { .flags = 0 };
@@ -1724,6 +1791,7 @@ void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 {
 	struct page *page;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (tags->rqs && set->ops->exit_request) {
 		int i;
 
@@ -1751,6 +1819,7 @@ void blk_mq_free_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 
 void blk_mq_free_rq_map(struct blk_mq_tags *tags)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kfree(tags->rqs);
 	tags->rqs = NULL;
 	kfree(tags->static_rqs);
@@ -1769,17 +1838,22 @@ struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 
 	node = blk_mq_hw_queue_to_node(set->mq_map, hctx_idx);
 	if (node == NUMA_NO_NODE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		node = set->numa_node;
+}
 
 	tags = blk_mq_init_tags(nr_tags, reserved_tags, node,
 				BLK_MQ_FLAG_TO_ALLOC_POLICY(set->flags));
 	if (!tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	tags->rqs = kzalloc_node(nr_tags * sizeof(struct request *),
 				 GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,
 				 node);
 	if (!tags->rqs) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_mq_free_tags(tags);
 		return NULL;
 	}
@@ -1788,11 +1862,13 @@ struct blk_mq_tags *blk_mq_alloc_rq_map(struct blk_mq_tag_set *set,
 				 GFP_NOIO | __GFP_NOWARN | __GFP_NORETRY,
 				 node);
 	if (!tags->static_rqs) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(tags->rqs);
 		blk_mq_free_tags(tags);
 		return NULL;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return tags;
 }
 
@@ -1810,7 +1886,9 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 
 	node = blk_mq_hw_queue_to_node(set->mq_map, hctx_idx);
 	if (node == NUMA_NO_NODE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		node = set->numa_node;
+}
 
 	INIT_LIST_HEAD(&tags->page_list);
 
@@ -1837,10 +1915,13 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 				this_order);
 			if (page)
 				break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (!this_order--)
 				break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (order_to_size(this_order) < rq_size)
 				break;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		} while (1);
 
 		if (!page)
@@ -1865,6 +1946,7 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 			if (set->ops->init_request) {
 				if (set->ops->init_request(set, rq, hctx_idx,
 						node)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 					tags->static_rqs[i] = NULL;
 					goto fail;
 				}
@@ -1874,6 +1956,7 @@ int blk_mq_alloc_rqs(struct blk_mq_tag_set *set, struct blk_mq_tags *tags,
 			i++;
 		}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 fail:
@@ -1892,6 +1975,7 @@ static int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)
 	struct blk_mq_ctx *ctx;
 	LIST_HEAD(tmp);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	hctx = hlist_entry_safe(node, struct blk_mq_hw_ctx, cpuhp_dead);
 	ctx = __blk_mq_get_ctx(hctx->queue, cpu);
 
@@ -1915,6 +1999,7 @@ static int blk_mq_hctx_notify_dead(unsigned int cpu, struct hlist_node *node)
 
 static void blk_mq_remove_cpuhp(struct blk_mq_hw_ctx *hctx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	cpuhp_state_remove_instance_nocalls(CPUHP_BLK_MQ_DEAD,
 					    &hctx->cpuhp_dead);
 }
@@ -1924,6 +2009,7 @@ static void blk_mq_exit_hctx(struct request_queue *q,
 		struct blk_mq_tag_set *set,
 		struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_mq_debugfs_unregister_hctx(hctx);
 
 	blk_mq_tag_idle(hctx);
@@ -1950,6 +2036,7 @@ static void blk_mq_exit_hw_queues(struct request_queue *q,
 	struct blk_mq_hw_ctx *hctx;
 	unsigned int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (i == nr_queue)
 			break;
@@ -1965,7 +2052,9 @@ static int blk_mq_init_hctx(struct request_queue *q,
 
 	node = hctx->numa_node;
 	if (node == NUMA_NO_NODE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		node = hctx->numa_node = set->numa_node;
+}
 
 	INIT_DELAYED_WORK(&hctx->run_work, blk_mq_run_work_fn);
 	spin_lock_init(&hctx->lock);
@@ -2021,7 +2110,9 @@ static int blk_mq_init_hctx(struct request_queue *q,
 	blk_mq_sched_exit_hctx(q, hctx, hctx_idx);
  exit_hctx:
 	if (set->ops->exit_hctx)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set->ops->exit_hctx(hctx, hctx_idx);
+}
  free_bitmap:
 	sbitmap_free(&hctx->ctx_map);
  free_ctxs:
@@ -2056,7 +2147,9 @@ static void blk_mq_init_cpu_queues(struct request_queue *q,
 		 * not, we remain on the home node of the device
 		 */
 		if (nr_hw_queues > 1 && hctx->numa_node == NUMA_NO_NODE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			hctx->numa_node = local_memory_node(cpu_to_node(i));
+}
 	}
 }
 
@@ -2067,13 +2160,18 @@ static bool __blk_mq_alloc_rq_map(struct blk_mq_tag_set *set, int hctx_idx)
 	set->tags[hctx_idx] = blk_mq_alloc_rq_map(set, hctx_idx,
 					set->queue_depth, set->reserved_tags);
 	if (!set->tags[hctx_idx])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return false;
+}
 
 	ret = blk_mq_alloc_rqs(set, set->tags[hctx_idx], hctx_idx,
 				set->queue_depth);
 	if (!ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_mq_free_rq_map(set->tags[hctx_idx]);
 	set->tags[hctx_idx] = NULL;
 	return false;
@@ -2082,6 +2180,7 @@ static bool __blk_mq_alloc_rq_map(struct blk_mq_tag_set *set, int hctx_idx)
 static void blk_mq_free_map_and_requests(struct blk_mq_tag_set *set,
 					 unsigned int hctx_idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (set->tags[hctx_idx]) {
 		blk_mq_free_rqs(set, set->tags[hctx_idx], hctx_idx);
 		blk_mq_free_rq_map(set->tags[hctx_idx]);
@@ -2146,8 +2245,11 @@ static void blk_mq_map_swqueue(struct request_queue *q)
 			 * allocation
 			 */
 			if (i && set->tags[i])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				blk_mq_free_map_and_requests(set, i);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			hctx->tags = NULL;
 			continue;
 		}
@@ -2179,6 +2281,7 @@ static void queue_set_hctx_shared(struct request_queue *q, bool shared)
 	struct blk_mq_hw_ctx *hctx;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_for_each_hw_ctx(q, hctx, i) {
 		if (shared) {
 			if (test_bit(BLK_MQ_S_SCHED_RESTART, &hctx->state))
@@ -2197,6 +2300,7 @@ static void blk_mq_update_tag_set_depth(struct blk_mq_tag_set *set,
 {
 	struct request_queue *q;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&set->tag_list_lock);
 
 	list_for_each_entry(q, &set->tag_list, tag_set_list) {
@@ -2233,12 +2337,15 @@ static void blk_mq_add_queue_tag_set(struct blk_mq_tag_set *set,
 
 	/* Check to see if we're transitioning to shared (from 1 to 2 queues). */
 	if (!list_empty(&set->tag_list) && !(set->flags & BLK_MQ_F_TAG_SHARED)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set->flags |= BLK_MQ_F_TAG_SHARED;
 		/* update existing queue */
 		blk_mq_update_tag_set_depth(set, true);
 	}
 	if (set->flags & BLK_MQ_F_TAG_SHARED)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		queue_set_hctx_shared(q, true);
+}
 	list_add_tail_rcu(&q->tag_set_list, &set->tag_list);
 
 	mutex_unlock(&set->tag_list_lock);
@@ -2281,12 +2388,17 @@ struct request_queue *blk_mq_init_queue(struct blk_mq_tag_set *set)
 
 	uninit_q = blk_alloc_queue_node(GFP_KERNEL, set->numa_node);
 	if (!uninit_q)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENOMEM);
+}
 
 	q = blk_mq_init_allocated_queue(set, uninit_q);
 	if (IS_ERR(q))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_cleanup_queue(uninit_q);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return q;
 }
 EXPORT_SYMBOL(blk_mq_init_queue);
@@ -2295,13 +2407,17 @@ static int blk_mq_hw_ctx_size(struct blk_mq_tag_set *tag_set)
 {
 	int hw_ctx_size = sizeof(struct blk_mq_hw_ctx);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG_ON(ALIGN(offsetof(struct blk_mq_hw_ctx, queue_rq_srcu),
 			   __alignof__(struct blk_mq_hw_ctx)) !=
 		     sizeof(struct blk_mq_hw_ctx));
 
 	if (tag_set->flags & BLK_MQ_F_BLOCKING)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		hw_ctx_size += sizeof(struct srcu_struct);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return hw_ctx_size;
 }
 
@@ -2326,6 +2442,7 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 
 		if (!zalloc_cpumask_var_node(&hctxs[i]->cpumask, GFP_KERNEL,
 						node)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			kfree(hctxs[i]);
 			hctxs[i] = NULL;
 			break;
@@ -2336,6 +2453,7 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 		hctxs[i]->queue_num = i;
 
 		if (blk_mq_init_hctx(q, set, hctxs[i], i)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			free_cpumask_var(hctxs[i]->cpumask);
 			kfree(hctxs[i]);
 			hctxs[i] = NULL;
@@ -2343,12 +2461,17 @@ static void blk_mq_realloc_hw_ctxs(struct blk_mq_tag_set *set,
 		}
 		blk_mq_hctx_kobj_init(hctxs[i]);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (j = i; j < q->nr_hw_queues; j++) {
 		struct blk_mq_hw_ctx *hctx = hctxs[j];
 
 		if (hctx) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (hctx->tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				blk_mq_free_map_and_requests(set, j);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			blk_mq_exit_hctx(q, set, hctx, j);
 			kobject_put(&hctx->kobj);
 			hctxs[j] = NULL;
@@ -2397,7 +2520,9 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 	q->queue_flags |= QUEUE_FLAG_MQ_DEFAULT;
 
 	if (!(set->flags & BLK_MQ_F_SG_MERGE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		q->queue_flags |= 1 << QUEUE_FLAG_NO_SG_MERGE;
+}
 
 	q->sg_reserved_size = INT_MAX;
 
@@ -2429,9 +2554,12 @@ struct request_queue *blk_mq_init_allocated_queue(struct blk_mq_tag_set *set,
 
 		ret = blk_mq_sched_init(q);
 		if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return ERR_PTR(ret);
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return q;
 
 err_hctxs:
@@ -2455,6 +2583,7 @@ void blk_mq_free_queue(struct request_queue *q)
 /* Basically redo blk_mq_init_queue with queue frozen */
 static void blk_mq_queue_reinit(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	WARN_ON_ONCE(!atomic_read(&q->mq_freeze_depth));
 
 	blk_mq_debugfs_unregister_hctxs(q);
@@ -2480,12 +2609,14 @@ static int __blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 		if (!__blk_mq_alloc_rq_map(set, i))
 			goto out_unwind;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 out_unwind:
 	while (--i >= 0)
 		blk_mq_free_rq_map(set->tags[i]);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return -ENOMEM;
 }
 
@@ -2505,29 +2636,38 @@ static int blk_mq_alloc_rq_maps(struct blk_mq_tag_set *set)
 		if (!err)
 			break;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set->queue_depth >>= 1;
 		if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			err = -ENOMEM;
 			break;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	} while (set->queue_depth);
 
 	if (!set->queue_depth || err) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_err("blk-mq: failed to allocate request map\n");
 		return -ENOMEM;
 	}
 
 	if (depth != set->queue_depth)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_info("blk-mq: reduced tag depth (%u -> %u)\n",
 						depth, set->queue_depth);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 
 static int blk_mq_update_queue_map(struct blk_mq_tag_set *set)
 {
 	if (set->ops->map_queues)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return set->ops->map_queues(set);
+}
 	else
 		return blk_mq_map_queues(set);
 }
@@ -2542,19 +2682,29 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 {
 	int ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUILD_BUG_ON(BLK_MQ_MAX_DEPTH > 1 << BLK_MQ_UNIQUE_TAG_BITS);
 
 	if (!set->nr_hw_queues)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	if (!set->queue_depth)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	if (set->queue_depth < set->reserved_tags + BLK_MQ_TAG_MIN)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (!set->ops->queue_rq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (set->queue_depth > BLK_MQ_MAX_DEPTH) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_info("blk-mq: reduced tag depth to %u\n",
 			BLK_MQ_MAX_DEPTH);
 		set->queue_depth = BLK_MQ_MAX_DEPTH;
@@ -2566,20 +2716,27 @@ int blk_mq_alloc_tag_set(struct blk_mq_tag_set *set)
 	 * 64 tags to prevent using too much memory.
 	 */
 	if (is_kdump_kernel()) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set->nr_hw_queues = 1;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set->queue_depth = min(64U, set->queue_depth);
 	}
 	/*
 	 * There is no use for more h/w queues than cpus.
 	 */
 	if (set->nr_hw_queues > nr_cpu_ids)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set->nr_hw_queues = nr_cpu_ids;
+}
 
 	set->tags = kzalloc_node(nr_cpu_ids * sizeof(struct blk_mq_tags *),
 				 GFP_KERNEL, set->numa_node);
 	if (!set->tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = -ENOMEM;
 	set->mq_map = kzalloc_node(sizeof(*set->mq_map) * nr_cpu_ids,
 			GFP_KERNEL, set->numa_node);
@@ -2613,6 +2770,7 @@ void blk_mq_free_tag_set(struct blk_mq_tag_set *set)
 {
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < nr_cpu_ids; i++)
 		blk_mq_free_map_and_requests(set, i);
 
@@ -2631,7 +2789,9 @@ int blk_mq_update_nr_requests(struct request_queue *q, unsigned int nr)
 	int i, ret;
 
 	if (!set)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	blk_mq_freeze_queue(q);
 
@@ -2668,6 +2828,7 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 {
 	struct request_queue *q;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(&set->tag_list_lock);
 
 	if (nr_hw_queues > nr_cpu_ids)
@@ -2691,6 +2852,7 @@ static void __blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set,
 
 void blk_mq_update_nr_hw_queues(struct blk_mq_tag_set *set, int nr_hw_queues)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mutex_lock(&set->tag_list_lock);
 	__blk_mq_update_nr_hw_queues(set, nr_hw_queues);
 	mutex_unlock(&set->tag_list_lock);
@@ -2700,6 +2862,7 @@ EXPORT_SYMBOL_GPL(blk_mq_update_nr_hw_queues);
 /* Enable polling stats and return whether they were already enabled. */
 static bool blk_poll_stats_enable(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags) ||
 	    test_and_set_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
 		return true;
@@ -2725,6 +2888,7 @@ static void blk_mq_poll_stats_fn(struct blk_stat_callback *cb)
 	struct request_queue *q = cb->data;
 	int bucket;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (bucket = 0; bucket < BLK_MQ_POLL_STATS_BKTS; bucket++) {
 		if (cb->stat[bucket].nr_samples)
 			q->poll_stat[bucket] = cb->stat[bucket];
@@ -2743,7 +2907,9 @@ static unsigned long blk_mq_poll_nsecs(struct request_queue *q,
 	 * future users
 	 */
 	if (!blk_poll_stats_enable(q))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/*
 	 * As an optimistic guess, use half of the mean service time
@@ -2773,6 +2939,7 @@ static bool blk_mq_poll_hybrid_sleep(struct request_queue *q,
 	unsigned int nsecs;
 	ktime_t kt;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(REQ_ATOM_POLL_SLEPT, &rq->atomic_flags))
 		return false;
 
@@ -2835,7 +3002,9 @@ static bool __blk_mq_poll(struct blk_mq_hw_ctx *hctx, struct request *rq)
 	 * straight to the busy poll loop.
 	 */
 	if (blk_mq_poll_hybrid_sleep(q, hctx, rq))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return true;
+}
 
 	hctx->poll_considered++;
 
@@ -2871,6 +3040,7 @@ bool blk_mq_poll(struct request_queue *q, blk_qc_t cookie)
 	struct blk_plug *plug;
 	struct request *rq;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!q->mq_ops || !q->mq_ops->poll || !blk_qc_t_valid(cookie) ||
 	    !test_bit(QUEUE_FLAG_POLL, &q->queue_flags))
 		return false;
diff --git a/block/blk-mq.h b/block/blk-mq.h
index 4933af9..2bb7e7f 100644
--- a/block/blk-mq.h
+++ b/block/blk-mq.h
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef INT_BLK_MQ_H
 #define INT_BLK_MQ_H
diff --git a/block/blk-settings.c b/block/blk-settings.c
index 8559e95..f968651 100644
--- a/block/blk-settings.c
+++ b/block/blk-settings.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Functions related to setting various queue properties from drivers
  */
@@ -124,6 +126,7 @@ EXPORT_SYMBOL(blk_set_default_limits);
  */
 void blk_set_stacking_limits(struct queue_limits *lim)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_set_default_limits(lim);
 
 	/* Inherit limits from component devices */
@@ -200,7 +203,9 @@ void blk_queue_bounce_limit(struct request_queue *q, u64 max_addr)
 	 * way to test this here.
 	 */
 	if (b_pfn < (min_t(u64, 0xffffffffUL, BLK_BOUNCE_HIGH) >> PAGE_SHIFT))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		dma = 1;
+}
 	q->limits.bounce_pfn = max(max_low_pfn, b_pfn);
 #else
 	if (b_pfn < blk_max_low_pfn)
@@ -208,6 +213,7 @@ void blk_queue_bounce_limit(struct request_queue *q, u64 max_addr)
 	q->limits.bounce_pfn = b_pfn;
 #endif
 	if (dma) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		init_emergency_isa_pool();
 		q->bounce_gfp = GFP_NOIO | GFP_DMA;
 		q->limits.bounce_pfn = b_pfn;
@@ -240,6 +246,7 @@ void blk_queue_max_hw_sectors(struct request_queue *q, unsigned int max_hw_secto
 	unsigned int max_sectors;
 
 	if ((max_hw_sectors << 9) < PAGE_SIZE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		max_hw_sectors = 1 << (PAGE_SHIFT - 9);
 		printk(KERN_INFO "%s: set to minimum %d\n",
 		       __func__, max_hw_sectors);
@@ -268,6 +275,7 @@ EXPORT_SYMBOL(blk_queue_max_hw_sectors);
  **/
 void blk_queue_chunk_sectors(struct request_queue *q, unsigned int chunk_sectors)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!is_power_of_2(chunk_sectors));
 	q->limits.chunk_sectors = chunk_sectors;
 }
@@ -294,6 +302,7 @@ EXPORT_SYMBOL(blk_queue_max_discard_sectors);
 void blk_queue_max_write_same_sectors(struct request_queue *q,
 				      unsigned int max_write_same_sectors)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q->limits.max_write_same_sectors = max_write_same_sectors;
 }
 EXPORT_SYMBOL(blk_queue_max_write_same_sectors);
@@ -307,6 +316,7 @@ EXPORT_SYMBOL(blk_queue_max_write_same_sectors);
 void blk_queue_max_write_zeroes_sectors(struct request_queue *q,
 		unsigned int max_write_zeroes_sectors)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q->limits.max_write_zeroes_sectors = max_write_zeroes_sectors;
 }
 EXPORT_SYMBOL(blk_queue_max_write_zeroes_sectors);
@@ -323,6 +333,7 @@ EXPORT_SYMBOL(blk_queue_max_write_zeroes_sectors);
 void blk_queue_max_segments(struct request_queue *q, unsigned short max_segments)
 {
 	if (!max_segments) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		max_segments = 1;
 		printk(KERN_INFO "%s: set to minimum %d\n",
 		       __func__, max_segments);
@@ -344,6 +355,7 @@ EXPORT_SYMBOL(blk_queue_max_segments);
 void blk_queue_max_discard_segments(struct request_queue *q,
 		unsigned short max_segments)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q->limits.max_discard_segments = max_segments;
 }
 EXPORT_SYMBOL_GPL(blk_queue_max_discard_segments);
@@ -360,6 +372,7 @@ EXPORT_SYMBOL_GPL(blk_queue_max_discard_segments);
 void blk_queue_max_segment_size(struct request_queue *q, unsigned int max_size)
 {
 	if (max_size < PAGE_SIZE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		max_size = PAGE_SIZE;
 		printk(KERN_INFO "%s: set to minimum %d\n",
 		       __func__, max_size);
@@ -381,6 +394,7 @@ EXPORT_SYMBOL(blk_queue_max_segment_size);
  **/
 void blk_queue_logical_block_size(struct request_queue *q, unsigned short size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q->limits.logical_block_size = size;
 
 	if (q->limits.physical_block_size < size)
@@ -406,7 +420,9 @@ void blk_queue_physical_block_size(struct request_queue *q, unsigned int size)
 	q->limits.physical_block_size = size;
 
 	if (q->limits.physical_block_size < q->limits.logical_block_size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		q->limits.physical_block_size = q->limits.logical_block_size;
+}
 
 	if (q->limits.io_min < q->limits.physical_block_size)
 		q->limits.io_min = q->limits.physical_block_size;
@@ -426,6 +442,7 @@ EXPORT_SYMBOL(blk_queue_physical_block_size);
  */
 void blk_queue_alignment_offset(struct request_queue *q, unsigned int offset)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q->limits.alignment_offset =
 		offset & (q->limits.physical_block_size - 1);
 	q->limits.misaligned = 0;
@@ -445,6 +462,7 @@ EXPORT_SYMBOL(blk_queue_alignment_offset);
  */
 void blk_limits_io_min(struct queue_limits *limits, unsigned int min)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	limits->io_min = min;
 
 	if (limits->io_min < limits->logical_block_size)
@@ -471,6 +489,7 @@ EXPORT_SYMBOL(blk_limits_io_min);
  */
 void blk_queue_io_min(struct request_queue *q, unsigned int min)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_limits_io_min(&q->limits, min);
 }
 EXPORT_SYMBOL(blk_queue_io_min);
@@ -490,6 +509,7 @@ EXPORT_SYMBOL(blk_queue_io_min);
  */
 void blk_limits_io_opt(struct queue_limits *limits, unsigned int opt)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	limits->io_opt = opt;
 }
 EXPORT_SYMBOL(blk_limits_io_opt);
@@ -509,6 +529,7 @@ EXPORT_SYMBOL(blk_limits_io_opt);
  */
 void blk_queue_io_opt(struct request_queue *q, unsigned int opt)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_limits_io_opt(&q->limits, opt);
 }
 EXPORT_SYMBOL(blk_queue_io_opt);
@@ -520,6 +541,7 @@ EXPORT_SYMBOL(blk_queue_io_opt);
  **/
 void blk_queue_stack_limits(struct request_queue *t, struct request_queue *b)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_stack_limits(&t->limits, &b->limits, 0);
 }
 EXPORT_SYMBOL(blk_queue_stack_limits);
@@ -550,6 +572,7 @@ int blk_stack_limits(struct queue_limits *t, struct queue_limits *b,
 {
 	unsigned int top, bottom, alignment, ret = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	t->max_sectors = min_not_zero(t->max_sectors, b->max_sectors);
 	t->max_hw_sectors = min_not_zero(t->max_hw_sectors, b->max_hw_sectors);
 	t->max_dev_sectors = min_not_zero(t->max_dev_sectors, b->max_dev_sectors);
@@ -685,6 +708,7 @@ EXPORT_SYMBOL(blk_stack_limits);
 int bdev_stack_limits(struct queue_limits *t, struct block_device *bdev,
 		      sector_t start)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request_queue *bq = bdev_get_queue(bdev);
 
 	start += get_start_sect(bdev);
@@ -732,6 +756,7 @@ EXPORT_SYMBOL(disk_stack_limits);
  **/
 void blk_queue_dma_pad(struct request_queue *q, unsigned int mask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q->dma_pad_mask = mask;
 }
 EXPORT_SYMBOL(blk_queue_dma_pad);
@@ -779,7 +804,9 @@ int blk_queue_dma_drain(struct request_queue *q,
 			       void *buf, unsigned int size)
 {
 	if (queue_max_segments(q) < 2)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	/* make room for appending the drain */
 	blk_queue_max_segments(q, queue_max_segments(q) - 1);
 	q->dma_drain_needed = dma_drain_needed;
@@ -798,6 +825,7 @@ EXPORT_SYMBOL_GPL(blk_queue_dma_drain);
 void blk_queue_segment_boundary(struct request_queue *q, unsigned long mask)
 {
 	if (mask < PAGE_SIZE - 1) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mask = PAGE_SIZE - 1;
 		printk(KERN_INFO "%s: set to minimum %lx\n",
 		       __func__, mask);
@@ -814,6 +842,7 @@ EXPORT_SYMBOL(blk_queue_segment_boundary);
  **/
 void blk_queue_virt_boundary(struct request_queue *q, unsigned long mask)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	q->limits.virt_boundary_mask = mask;
 }
 EXPORT_SYMBOL(blk_queue_virt_boundary);
@@ -861,7 +890,9 @@ void blk_queue_flush_queueable(struct request_queue *q, bool queueable)
 {
 	spin_lock_irq(q->queue_lock);
 	if (queueable)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		clear_bit(QUEUE_FLAG_FLUSH_NQ, &q->queue_flags);
+}
 	else
 		set_bit(QUEUE_FLAG_FLUSH_NQ, &q->queue_flags);
 	spin_unlock_irq(q->queue_lock);
@@ -891,6 +922,7 @@ EXPORT_SYMBOL(blk_set_queue_depth);
  */
 void blk_queue_write_cache(struct request_queue *q, bool wc, bool fua)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irq(q->queue_lock);
 	if (wc)
 		queue_flag_set(QUEUE_FLAG_WC, q);
diff --git a/block/blk-softirq.c b/block/blk-softirq.c
index 01e2b35..aff9dfc 100644
--- a/block/blk-softirq.c
+++ b/block/blk-softirq.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to softirq rq completions
@@ -45,6 +47,7 @@ static void trigger_softirq(void *data)
 	unsigned long flags;
 	struct list_head *list;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	local_irq_save(flags);
 	list = this_cpu_ptr(&blk_cpu_done);
 	list_add_tail(&rq->ipi_list, list);
@@ -60,6 +63,7 @@ static void trigger_softirq(void *data)
  */
 static int raise_blk_irq(int cpu, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (cpu_online(cpu)) {
 		call_single_data_t *data = &rq->csd;
 
@@ -111,11 +115,17 @@ void __blk_complete_request(struct request *req)
 	 * Select completion CPU
 	 */
 	if (req->cpu != -1) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ccpu = req->cpu;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			shared = cpus_share_cache(cpu, ccpu);
+}
 	} else
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ccpu = cpu;
+}
 
 	/*
 	 * If current CPU and requested CPU share a cache, run the softirq on
@@ -158,8 +168,11 @@ void __blk_complete_request(struct request *req)
  **/
 void blk_complete_request(struct request *req)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (unlikely(blk_should_fake_timeout(req->q)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (!blk_mark_rq_complete(req))
 		__blk_complete_request(req);
 }
diff --git a/block/blk-stat.c b/block/blk-stat.c
index c52356d..58fc7a9 100644
--- a/block/blk-stat.c
+++ b/block/blk-stat.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Block stat tracking code
  *
@@ -21,6 +23,7 @@ struct blk_queue_stats {
 
 static void blk_stat_init(struct blk_rq_stat *stat)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	stat->min = -1ULL;
 	stat->max = stat->nr_samples = stat->mean = 0;
 	stat->batch = stat->nr_batch = 0;
@@ -28,6 +31,7 @@ static void blk_stat_init(struct blk_rq_stat *stat)
 
 static void blk_stat_flush_batch(struct blk_rq_stat *stat)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const s32 nr_batch = READ_ONCE(stat->nr_batch);
 	const s32 nr_samples = READ_ONCE(stat->nr_samples);
 
@@ -47,6 +51,7 @@ static void blk_stat_flush_batch(struct blk_rq_stat *stat)
 
 static void blk_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	blk_stat_flush_batch(src);
 
 	if (!src->nr_samples)
@@ -67,6 +72,7 @@ static void blk_stat_sum(struct blk_rq_stat *dst, struct blk_rq_stat *src)
 
 static void __blk_stat_add(struct blk_rq_stat *stat, u64 value)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	stat->min = min(stat->min, value);
 	stat->max = max(stat->max, value);
 
@@ -88,7 +94,9 @@ void blk_stat_add(struct request *rq)
 
 	now = __blk_stat_time(ktime_to_ns(ktime_get()));
 	if (now < blk_stat_time(&rq->issue_stat))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	value = now - blk_stat_time(&rq->issue_stat);
 
@@ -96,17 +104,22 @@ void blk_stat_add(struct request *rq)
 
 	rcu_read_lock();
 	list_for_each_entry_rcu(cb, &q->stats->callbacks, list) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!blk_stat_is_active(cb))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bucket = cb->bucket_fn(rq);
 		if (bucket < 0)
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		stat = &get_cpu_ptr(cb->cpu_stat)[bucket];
 		__blk_stat_add(stat, value);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_cpu_ptr(cb->cpu_stat);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_unlock();
 }
 
@@ -116,6 +129,7 @@ static void blk_stat_timer_fn(unsigned long data)
 	unsigned int bucket;
 	int cpu;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (bucket = 0; bucket < cb->buckets; bucket++)
 		blk_stat_init(&cb->stat[bucket]);
 
@@ -141,17 +155,21 @@ blk_stat_alloc_callback(void (*timer_fn)(struct blk_stat_callback *),
 
 	cb = kmalloc(sizeof(*cb), GFP_KERNEL);
 	if (!cb)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	cb->stat = kmalloc_array(buckets, sizeof(struct blk_rq_stat),
 				 GFP_KERNEL);
 	if (!cb->stat) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(cb);
 		return NULL;
 	}
 	cb->cpu_stat = __alloc_percpu(buckets * sizeof(struct blk_rq_stat),
 				      __alignof__(struct blk_rq_stat));
 	if (!cb->cpu_stat) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(cb->stat);
 		kfree(cb);
 		return NULL;
@@ -173,6 +191,7 @@ void blk_stat_add_callback(struct request_queue *q,
 	unsigned int bucket;
 	int cpu;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for_each_possible_cpu(cpu) {
 		struct blk_rq_stat *cpu_stat;
 
@@ -191,6 +210,7 @@ EXPORT_SYMBOL_GPL(blk_stat_add_callback);
 void blk_stat_remove_callback(struct request_queue *q,
 			      struct blk_stat_callback *cb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&q->stats->lock);
 	list_del_rcu(&cb->list);
 	if (list_empty(&q->stats->callbacks) && !q->stats->enable_accounting)
@@ -205,6 +225,7 @@ static void blk_stat_free_callback_rcu(struct rcu_head *head)
 {
 	struct blk_stat_callback *cb;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	cb = container_of(head, struct blk_stat_callback, rcu);
 	free_percpu(cb->cpu_stat);
 	kfree(cb->stat);
@@ -213,6 +234,7 @@ static void blk_stat_free_callback_rcu(struct rcu_head *head)
 
 void blk_stat_free_callback(struct blk_stat_callback *cb)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (cb)
 		call_rcu(&cb->rcu, blk_stat_free_callback_rcu);
 }
@@ -232,7 +254,9 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 
 	stats = kmalloc(sizeof(*stats), GFP_KERNEL);
 	if (!stats)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	INIT_LIST_HEAD(&stats->callbacks);
 	spin_lock_init(&stats->lock);
@@ -243,6 +267,7 @@ struct blk_queue_stats *blk_alloc_queue_stats(void)
 
 void blk_free_queue_stats(struct blk_queue_stats *stats)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!stats)
 		return;
 
diff --git a/block/blk-stat.h b/block/blk-stat.h
index 2dd3634..5c28586 100644
--- a/block/blk-stat.h
+++ b/block/blk-stat.h
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef BLK_STAT_H
 #define BLK_STAT_H
diff --git a/block/blk-sysfs.c b/block/blk-sysfs.c
index e54be40..632dd5bd 100644
--- a/block/blk-sysfs.c
+++ b/block/blk-sysfs.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to sysfs handling
@@ -26,6 +28,7 @@ struct queue_sysfs_entry {
 static ssize_t
 queue_var_show(unsigned long var, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%lu\n", var);
 }
 
@@ -36,6 +39,7 @@ queue_var_store(unsigned long *var, const char *page, size_t count)
 	unsigned long v;
 
 	err = kstrtoul(page, 10, &v);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (err || v > UINT_MAX)
 		return -EINVAL;
 
@@ -51,7 +55,9 @@ static ssize_t queue_var_store64(s64 *var, const char *page)
 
 	err = kstrtos64(page, 10, &v);
 	if (err < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return err;
+}
 
 	*var = v;
 	return 0;
@@ -59,6 +65,7 @@ static ssize_t queue_var_store64(s64 *var, const char *page)
 
 static ssize_t queue_requests_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(q->nr_requests, (page));
 }
 
@@ -68,6 +75,7 @@ queue_requests_store(struct request_queue *q, const char *page, size_t count)
 	unsigned long nr;
 	int ret, err;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!q->request_fn && !q->mq_ops)
 		return -EINVAL;
 
@@ -104,7 +112,9 @@ queue_ra_store(struct request_queue *q, const char *page, size_t count)
 	ssize_t ret = queue_var_store(&ra_kb, page, count);
 
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	q->backing_dev_info->ra_pages = ra_kb >> (PAGE_SHIFT - 10);
 
@@ -113,6 +123,7 @@ queue_ra_store(struct request_queue *q, const char *page, size_t count)
 
 static ssize_t queue_max_sectors_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int max_sectors_kb = queue_max_sectors(q) >> 1;
 
 	return queue_var_show(max_sectors_kb, (page));
@@ -120,22 +131,26 @@ static ssize_t queue_max_sectors_show(struct request_queue *q, char *page)
 
 static ssize_t queue_max_segments_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(queue_max_segments(q), (page));
 }
 
 static ssize_t queue_max_discard_segments_show(struct request_queue *q,
 		char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(queue_max_discard_segments(q), (page));
 }
 
 static ssize_t queue_max_integrity_segments_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(q->limits.max_integrity_segments, (page));
 }
 
 static ssize_t queue_max_segment_size_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (blk_queue_cluster(q))
 		return queue_var_show(queue_max_segment_size(q), (page));
 
@@ -144,43 +159,51 @@ static ssize_t queue_max_segment_size_show(struct request_queue *q, char *page)
 
 static ssize_t queue_logical_block_size_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(queue_logical_block_size(q), page);
 }
 
 static ssize_t queue_physical_block_size_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(queue_physical_block_size(q), page);
 }
 
 static ssize_t queue_chunk_sectors_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(q->limits.chunk_sectors, page);
 }
 
 static ssize_t queue_io_min_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(queue_io_min(q), page);
 }
 
 static ssize_t queue_io_opt_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(queue_io_opt(q), page);
 }
 
 static ssize_t queue_discard_granularity_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(q->limits.discard_granularity, page);
 }
 
 static ssize_t queue_discard_max_hw_show(struct request_queue *q, char *page)
 {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%llu\n",
 		(unsigned long long)q->limits.max_hw_discard_sectors << 9);
 }
 
 static ssize_t queue_discard_max_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%llu\n",
 		       (unsigned long long)q->limits.max_discard_sectors << 9);
 }
@@ -192,7 +215,9 @@ static ssize_t queue_discard_max_store(struct request_queue *q,
 	ssize_t ret = queue_var_store(&max_discard, page, count);
 
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	if (max_discard & (q->limits.discard_granularity - 1))
 		return -EINVAL;
@@ -210,17 +235,20 @@ static ssize_t queue_discard_max_store(struct request_queue *q,
 
 static ssize_t queue_discard_zeroes_data_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(0, page);
 }
 
 static ssize_t queue_write_same_max_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%llu\n",
 		(unsigned long long)q->limits.max_write_same_sectors << 9);
 }
 
 static ssize_t queue_write_zeroes_max_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%llu\n",
 		(unsigned long long)q->limits.max_write_zeroes_sectors << 9);
 }
@@ -234,7 +262,9 @@ queue_max_sectors_store(struct request_queue *q, const char *page, size_t count)
 	ssize_t ret = queue_var_store(&max_sectors_kb, page, count);
 
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	max_hw_sectors_kb = min_not_zero(max_hw_sectors_kb, (unsigned long)
 					 q->limits.max_dev_sectors >> 1);
@@ -252,6 +282,7 @@ queue_max_sectors_store(struct request_queue *q, const char *page, size_t count)
 
 static ssize_t queue_max_hw_sectors_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int max_hw_sectors_kb = queue_max_hw_sectors(q) >> 1;
 
 	return queue_var_show(max_hw_sectors_kb, (page));
@@ -292,6 +323,7 @@ QUEUE_SYSFS_BIT_FNS(iostats, IO_STAT, 0);
 
 static ssize_t queue_zoned_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	switch (blk_queue_zoned_model(q)) {
 	case BLK_ZONED_HA:
 		return sprintf(page, "host-aware\n");
@@ -304,6 +336,7 @@ static ssize_t queue_zoned_show(struct request_queue *q, char *page)
 
 static ssize_t queue_nomerges_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show((blk_queue_nomerges(q) << 1) |
 			       blk_queue_noxmerges(q), page);
 }
@@ -315,7 +348,9 @@ static ssize_t queue_nomerges_store(struct request_queue *q, const char *page,
 	ssize_t ret = queue_var_store(&nm, page, count);
 
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	spin_lock_irq(q->queue_lock);
 	queue_flag_clear(QUEUE_FLAG_NOMERGES, q);
@@ -331,6 +366,7 @@ static ssize_t queue_nomerges_store(struct request_queue *q, const char *page,
 
 static ssize_t queue_rq_affinity_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bool set = test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags);
 	bool force = test_bit(QUEUE_FLAG_SAME_FORCE, &q->queue_flags);
 
@@ -346,7 +382,9 @@ queue_rq_affinity_store(struct request_queue *q, const char *page, size_t count)
 
 	ret = queue_var_store(&val, page, count);
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	spin_lock_irq(q->queue_lock);
 	if (val == 2) {
@@ -369,7 +407,9 @@ static ssize_t queue_poll_delay_show(struct request_queue *q, char *page)
 	int val;
 
 	if (q->poll_nsec == -1)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		val = -1;
+}
 	else
 		val = q->poll_nsec / 1000;
 
@@ -381,6 +421,7 @@ static ssize_t queue_poll_delay_store(struct request_queue *q, const char *page,
 {
 	int err, val;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!q->mq_ops || !q->mq_ops->poll)
 		return -EINVAL;
 
@@ -398,6 +439,7 @@ static ssize_t queue_poll_delay_store(struct request_queue *q, const char *page,
 
 static ssize_t queue_poll_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(test_bit(QUEUE_FLAG_POLL, &q->queue_flags), page);
 }
 
@@ -407,6 +449,7 @@ static ssize_t queue_poll_store(struct request_queue *q, const char *page,
 	unsigned long poll_on;
 	ssize_t ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!q->mq_ops || !q->mq_ops->poll)
 		return -EINVAL;
 
@@ -426,6 +469,7 @@ static ssize_t queue_poll_store(struct request_queue *q, const char *page,
 
 static ssize_t queue_wb_lat_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!q->rq_wb)
 		return -EINVAL;
 
@@ -441,7 +485,9 @@ static ssize_t queue_wb_lat_store(struct request_queue *q, const char *page,
 
 	ret = queue_var_store64(&val, page);
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 	if (val < -1)
 		return -EINVAL;
 
@@ -470,6 +516,7 @@ static ssize_t queue_wb_lat_store(struct request_queue *q, const char *page,
 
 static ssize_t queue_wc_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (test_bit(QUEUE_FLAG_WC, &q->queue_flags))
 		return sprintf(page, "write back\n");
 
@@ -482,7 +529,9 @@ static ssize_t queue_wc_store(struct request_queue *q, const char *page,
 	int set = -1;
 
 	if (!strncmp(page, "write back", 10))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set = 1;
+}
 	else if (!strncmp(page, "write through", 13) ||
 		 !strncmp(page, "none", 4))
 		set = 0;
@@ -502,6 +551,7 @@ static ssize_t queue_wc_store(struct request_queue *q, const char *page,
 
 static ssize_t queue_dax_show(struct request_queue *q, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return queue_var_show(blk_queue_dax(q), page);
 }
 
@@ -731,6 +781,7 @@ static struct attribute *default_attrs[] = {
 static ssize_t
 queue_attr_show(struct kobject *kobj, struct attribute *attr, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct queue_sysfs_entry *entry = to_queue(attr);
 	struct request_queue *q =
 		container_of(kobj, struct request_queue, kobj);
@@ -752,6 +803,7 @@ static ssize_t
 queue_attr_store(struct kobject *kobj, struct attribute *attr,
 		    const char *page, size_t length)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct queue_sysfs_entry *entry = to_queue(attr);
 	struct request_queue *q;
 	ssize_t res;
@@ -772,6 +824,7 @@ queue_attr_store(struct kobject *kobj, struct attribute *attr,
 
 static void blk_free_queue_rcu(struct rcu_head *rcu_head)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request_queue *q = container_of(rcu_head, struct request_queue,
 					       rcu_head);
 	kmem_cache_free(blk_requestq_cachep, q);
@@ -796,6 +849,7 @@ static void blk_free_queue_rcu(struct rcu_head *rcu_head)
  */
 static void __blk_release_queue(struct work_struct *work)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request_queue *q = container_of(work, typeof(*q), release_work);
 
 	if (test_bit(QUEUE_FLAG_POLL_STATS, &q->queue_flags))
@@ -863,7 +917,9 @@ int blk_register_queue(struct gendisk *disk)
 	struct request_queue *q = disk->queue;
 
 	if (WARN_ON(!q))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENXIO;
+}
 
 	WARN_ONCE(test_bit(QUEUE_FLAG_REGISTERED, &q->queue_flags),
 		  "%s is registering an already registered queue\n",
@@ -880,6 +936,7 @@ int blk_register_queue(struct gendisk *disk)
 	 * request_queues for non-existent devices never get registered.
 	 */
 	if (!blk_queue_init_done(q)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		queue_flag_set_unlocked(QUEUE_FLAG_INIT_DONE, q);
 		percpu_ref_switch_to_percpu(&q->q_usage_counter);
 		blk_queue_bypass_end(q);
@@ -887,13 +944,16 @@ int blk_register_queue(struct gendisk *disk)
 
 	ret = blk_trace_init_sysfs(dev);
 	if (ret)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	/* Prevent changes through sysfs until registration is completed. */
 	mutex_lock(&q->sysfs_lock);
 
 	ret = kobject_add(&q->kobj, kobject_get(&dev->kobj), "%s", "queue");
 	if (ret < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_trace_remove_sysfs(dev);
 		goto unlock;
 	}
@@ -912,6 +972,7 @@ int blk_register_queue(struct gendisk *disk)
 	if (q->request_fn || (q->mq_ops && q->elevator)) {
 		ret = elv_register_queue(q);
 		if (ret) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			kobject_uevent(&q->kobj, KOBJ_REMOVE);
 			kobject_del(&q->kobj);
 			blk_trace_remove_sysfs(dev);
@@ -919,6 +980,7 @@ int blk_register_queue(struct gendisk *disk)
 			goto unlock;
 		}
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ret = 0;
 unlock:
 	mutex_unlock(&q->sysfs_lock);
@@ -929,6 +991,7 @@ void blk_unregister_queue(struct gendisk *disk)
 {
 	struct request_queue *q = disk->queue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON(!q))
 		return;
 
diff --git a/block/blk-tag.c b/block/blk-tag.c
index 09f19c6..5989aa6 100644
--- a/block/blk-tag.c
+++ b/block/blk-tag.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Functions related to tagged command queuing
@@ -23,6 +25,7 @@
  **/
 struct request *blk_queue_find_tag(struct request_queue *q, int tag)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return blk_map_queue_find_tag(q->queue_tags, tag);
 }
 EXPORT_SYMBOL(blk_queue_find_tag);
@@ -36,6 +39,7 @@ EXPORT_SYMBOL(blk_queue_find_tag);
  */
 void blk_free_tags(struct blk_queue_tag *bqt)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (atomic_dec_and_test(&bqt->refcnt)) {
 		BUG_ON(find_first_bit(bqt->tag_map, bqt->max_depth) <
 							bqt->max_depth);
@@ -64,7 +68,9 @@ void __blk_queue_free_tags(struct request_queue *q)
 	struct blk_queue_tag *bqt = q->queue_tags;
 
 	if (!bqt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	blk_free_tags(bqt);
 
@@ -82,6 +88,7 @@ void __blk_queue_free_tags(struct request_queue *q)
  **/
 void blk_queue_free_tags(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	queue_flag_clear_unlocked(QUEUE_FLAG_QUEUED, q);
 }
 EXPORT_SYMBOL(blk_queue_free_tags);
@@ -94,6 +101,7 @@ init_tag_map(struct request_queue *q, struct blk_queue_tag *tags, int depth)
 	int nr_ulongs;
 
 	if (q && depth > q->nr_requests * 2) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		depth = q->nr_requests * 2;
 		printk(KERN_ERR "%s: adjusted depth to %d\n",
 		       __func__, depth);
@@ -131,6 +139,7 @@ static struct blk_queue_tag *__blk_queue_init_tags(struct request_queue *q,
 	if (init_tag_map(q, tags, depth))
 		goto fail;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	atomic_set(&tags->refcnt, 1);
 	tags->alloc_policy = alloc_policy;
 	tags->next_tag = 0;
@@ -169,15 +178,22 @@ int blk_queue_init_tags(struct request_queue *q, int depth,
 	BUG_ON(tags && q->queue_tags && tags != q->queue_tags);
 
 	if (!tags && !q->queue_tags) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		tags = __blk_queue_init_tags(q, depth, alloc_policy);
 
 		if (!tags)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 
 	} else if (q->queue_tags) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rc = blk_queue_resize_tags(q, depth);
 		if (rc)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return rc;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		queue_flag_set(QUEUE_FLAG_QUEUED, q);
 		return 0;
 	} else
@@ -209,7 +225,9 @@ int blk_queue_resize_tags(struct request_queue *q, int new_depth)
 	int max_depth, nr_ulongs;
 
 	if (!bqt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENXIO;
+}
 
 	/*
 	 * if we already have large enough real_max_depth.  just
@@ -265,6 +283,7 @@ void blk_queue_end_tag(struct request_queue *q, struct request *rq)
 	struct blk_queue_tag *bqt = q->queue_tags;
 	unsigned tag = rq->tag; /* negative tags invalid */
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 
 	BUG_ON(tag >= bqt->real_max_depth);
@@ -275,12 +294,15 @@ void blk_queue_end_tag(struct request_queue *q, struct request *rq)
 	rq->internal_tag = -1;
 
 	if (unlikely(bqt->tag_index[tag] == NULL))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		printk(KERN_ERR "%s: tag %d is missing\n",
 		       __func__, tag);
+}
 
 	bqt->tag_index[tag] = NULL;
 
 	if (unlikely(!test_bit(tag, bqt->tag_map))) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		printk(KERN_ERR "%s: attempt to clear non-busy tag (%d)\n",
 		       __func__, tag);
 		return;
@@ -313,6 +335,7 @@ int blk_queue_start_tag(struct request_queue *q, struct request *rq)
 	unsigned max_depth;
 	int tag;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 
 	if (unlikely((rq->rq_flags & RQF_QUEUED))) {
@@ -320,6 +343,7 @@ int blk_queue_start_tag(struct request_queue *q, struct request *rq)
 		       "%s: request %p for device [%s] already tagged %d",
 		       __func__, rq,
 		       rq->rq_disk ? rq->rq_disk->disk_name : "?", rq->tag);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG();
 	}
 
@@ -343,24 +367,32 @@ int blk_queue_start_tag(struct request_queue *q, struct request *rq)
 			max_depth -= 2;
 		}
 		if (q->in_flight[BLK_RW_ASYNC] > max_depth)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return 1;
+}
 	}
 
 	do {
 		if (bqt->alloc_policy == BLK_TAG_ALLOC_FIFO) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			tag = find_first_zero_bit(bqt->tag_map, max_depth);
 			if (tag >= max_depth)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return 1;
+}
 		} else {
 			int start = bqt->next_tag;
 			int size = min_t(int, bqt->max_depth, max_depth + start);
 			tag = find_next_zero_bit(bqt->tag_map, size, start);
 			if (tag >= size && start + size > bqt->max_depth) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				size = start + size - bqt->max_depth;
 				tag = find_first_zero_bit(bqt->tag_map, size);
 			}
 			if (tag >= size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return 1;
+}
 		}
 
 	} while (test_and_set_bit_lock(tag, bqt->tag_map));
@@ -392,6 +424,7 @@ void blk_queue_invalidate_tags(struct request_queue *q)
 {
 	struct list_head *tmp, *n;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	lockdep_assert_held(q->queue_lock);
 
 	list_for_each_safe(tmp, n, &q->tag_busy_list)
diff --git a/block/blk-throttle.c b/block/blk-throttle.c
index a8cd7b3..8553b10 100644
--- a/block/blk-throttle.c
+++ b/block/blk-throttle.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Interface for controlling IO bandwidth on a request queue
diff --git a/block/blk-timeout.c b/block/blk-timeout.c
index 6427be7..d575045 100644
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Functions related to generic timeout handling of requests.
  */
@@ -112,6 +114,7 @@ static void blk_rq_timed_out(struct request *req)
 static void blk_rq_check_expired(struct request *rq, unsigned long *next_timeout,
 			  unsigned int *next_set)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (time_after_eq(jiffies, rq->deadline)) {
 		list_del_init(&rq->timeout_list);
 
@@ -140,7 +143,9 @@ void blk_timeout_work(struct work_struct *work)
 		blk_rq_check_expired(rq, &next, &next_set);
 
 	if (next_set)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mod_timer(&q->timeout, round_jiffies_up(next));
+}
 
 	spin_unlock_irqrestore(q->queue_lock, flags);
 }
@@ -157,11 +162,15 @@ void blk_timeout_work(struct work_struct *work)
 void blk_abort_request(struct request *req)
 {
 	if (blk_mark_rq_complete(req))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	if (req->q->mq_ops) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_mq_rq_timed_out(req, false);
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_delete_timer(req);
 		blk_rq_timed_out(req);
 	}
@@ -174,7 +183,9 @@ unsigned long blk_rq_timeout(unsigned long timeout)
 
 	maxt = round_jiffies_up(jiffies + BLK_MAX_TIMEOUT);
 	if (time_after(timeout, maxt))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		timeout = maxt;
+}
 
 	return timeout;
 }
@@ -193,11 +204,15 @@ void blk_add_timer(struct request *req)
 	unsigned long expiry;
 
 	if (!q->mq_ops)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		lockdep_assert_held(q->queue_lock);
+}
 
 	/* blk-mq has its own handler, so we don't need ->rq_timed_out_fn */
 	if (!q->mq_ops && !q->rq_timed_out_fn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	BUG_ON(!list_empty(&req->timeout_list));
 
@@ -206,7 +221,9 @@ void blk_add_timer(struct request *req)
 	 * command from being retried forever.
 	 */
 	if (!req->timeout)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		req->timeout = q->rq_timeout;
+}
 
 	req->deadline = jiffies + req->timeout;
 
diff --git a/block/blk.h b/block/blk.h
index b2c287c..544bac9 100644
--- a/block/blk.h
+++ b/block/blk.h
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* SPDX-License-Identifier: GPL-2.0 */
 #ifndef BLK_INTERNAL_H
 #define BLK_INTERNAL_H
diff --git a/block/bounce.c b/block/bounce.c
index 1d05c42..66a38f4 100644
--- a/block/bounce.c
+++ b/block/bounce.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /* bounce buffer handling for block devices
  *
@@ -85,6 +87,7 @@ static void bounce_copy_vec(struct bio_vec *to, unsigned char *vfrom)
  */
 static void *mempool_alloc_pages_isa(gfp_t gfp_mask, void *data)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return mempool_alloc_pages(gfp_mask | GFP_DMA, data);
 }
 
@@ -94,6 +97,7 @@ static void *mempool_alloc_pages_isa(gfp_t gfp_mask, void *data)
  */
 int init_emergency_isa_pool(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (isa_page_pool)
 		return 0;
 
@@ -116,6 +120,7 @@ static void copy_to_high_bio_irq(struct bio *to, struct bio *from)
 	struct bio_vec tovec, *fromvec = from->bi_io_vec;
 	struct bvec_iter iter;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bio_for_each_segment(tovec, to, iter) {
 		if (tovec.bv_page != fromvec->bv_page) {
 			/*
@@ -161,12 +166,14 @@ static void bounce_end_io(struct bio *bio, mempool_t *pool)
 
 static void bounce_end_io_write(struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bounce_end_io(bio, page_pool);
 }
 
 static void bounce_end_io_write_isa(struct bio *bio)
 {
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bounce_end_io(bio, isa_page_pool);
 }
 
@@ -175,18 +182,22 @@ static void __bounce_end_io_read(struct bio *bio, mempool_t *pool)
 	struct bio *bio_orig = bio->bi_private;
 
 	if (!bio->bi_status)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		copy_to_high_bio_irq(bio_orig, bio);
+}
 
 	bounce_end_io(bio, pool);
 }
 
 static void bounce_end_io_read(struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__bounce_end_io_read(bio, page_pool);
 }
 
 static void bounce_end_io_read_isa(struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__bounce_end_io_read(bio, isa_page_pool);
 }
 
@@ -194,6 +205,7 @@ static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,
 			       mempool_t *pool)
 {
 	struct bio *bio;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int rw = bio_data_dir(*bio_orig);
 	struct bio_vec *to, from;
 	struct bvec_iter iter;
@@ -267,7 +279,9 @@ void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)
 	 * Data-less bio, nothing to bounce
 	 */
 	if (!bio_has_data(*bio_orig))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * for non-isa bounce case, just check if the bounce pfn is equal
@@ -276,10 +290,15 @@ void blk_queue_bounce(struct request_queue *q, struct bio **bio_orig)
 	 */
 	if (!(q->bounce_gfp & GFP_DMA)) {
 		if (q->limits.bounce_pfn >= blk_max_pfn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pool = page_pool;
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(!isa_page_pool);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pool = isa_page_pool;
 	}
 
diff --git a/block/bsg.c b/block/bsg.c
index ee1335c..1782700 100644
--- a/block/bsg.c
+++ b/block/bsg.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * bsg.c - block layer implementation of the sg v4 interface
  *
@@ -94,6 +96,7 @@ static void bsg_free_command(struct bsg_command *bc)
 
 	kmem_cache_free(bsg_cmd_cachep, bc);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irqsave(&bd->lock, flags);
 	bd->queued_cmds--;
 	spin_unlock_irqrestore(&bd->lock, flags);
@@ -103,6 +106,7 @@ static void bsg_free_command(struct bsg_command *bc)
 
 static struct bsg_command *bsg_alloc_command(struct bsg_device *bd)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct bsg_command *bc = ERR_PTR(-EINVAL);
 
 	spin_lock_irq(&bd->lock);
@@ -132,6 +136,7 @@ static struct bsg_command *bsg_alloc_command(struct bsg_device *bd)
 
 static inline struct hlist_head *bsg_dev_idx_hash(int index)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return &bsg_device_list[index & (BSG_LIST_ARRAY_SIZE - 1)];
 }
 
@@ -139,6 +144,7 @@ static int blk_fill_sgv4_hdr_rq(struct request_queue *q, struct request *rq,
 				struct sg_io_v4 *hdr, struct bsg_device *bd,
 				fmode_t has_write_perm)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct scsi_request *req = scsi_req(rq);
 
 	if (hdr->request_len > BLK_MAX_CDB) {
@@ -182,7 +188,9 @@ bsg_validate_sgv4_hdr(struct sg_io_v4 *hdr, int *op)
 	int ret = 0;
 
 	if (hdr->guard != 'Q')
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	switch (hdr->protocol) {
 	case BSG_PROTOCOL_SCSI:
@@ -220,7 +228,9 @@ bsg_map_hdr(struct bsg_device *bd, struct sg_io_v4 *hdr, fmode_t has_write_perm)
 	 * longer use this request_queue. Return no such address.
 	 */
 	if (!bcd->class_dev)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(-ENXIO);
+}
 
 	dprintk("map hdr %llx/%u %llx/%u\n", (unsigned long long) hdr->dout_xferp,
 		hdr->dout_xfer_len, (unsigned long long) hdr->din_xferp,
@@ -304,6 +314,7 @@ static void bsg_rq_end_io(struct request *rq, blk_status_t status)
 
 	bc->hdr.duration = jiffies_to_msecs(jiffies - bc->hdr.duration);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irqsave(&bd->lock, flags);
 	list_move_tail(&bc->list, &bd->done_list);
 	bd->done_cmds++;
@@ -327,7 +338,9 @@ static void bsg_add_command(struct bsg_device *bd, struct request_queue *q,
 	bc->rq = rq;
 	bc->bio = rq->bio;
 	if (rq->next_rq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bc->bidi_bio = rq->next_rq->bio;
+}
 	bc->hdr.duration = jiffies;
 	spin_lock_irq(&bd->lock);
 	list_add_tail(&bc->list, &bd->busy_list);
@@ -345,6 +358,7 @@ static struct bsg_command *bsg_next_done_cmd(struct bsg_device *bd)
 
 	spin_lock_irq(&bd->lock);
 	if (bd->done_cmds) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bc = list_first_entry(&bd->done_list, struct bsg_command, list);
 		list_del(&bc->list);
 		bd->done_cmds--;
@@ -363,6 +377,7 @@ static struct bsg_command *bsg_get_done_cmd(struct bsg_device *bd)
 	int ret;
 
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bc = bsg_next_done_cmd(bd);
 		if (bc)
 			break;
@@ -387,6 +402,7 @@ static struct bsg_command *bsg_get_done_cmd(struct bsg_device *bd)
 static int blk_complete_sgv4_hdr_rq(struct request *rq, struct sg_io_v4 *hdr,
 				    struct bio *bio, struct bio *bidi_bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct scsi_request *req = scsi_req(rq);
 	int ret = 0;
 
@@ -446,6 +462,7 @@ static bool bsg_complete(struct bsg_device *bd)
 	bool spin;
 
 	do {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_lock_irq(&bd->lock);
 
 		BUG_ON(bd->done_cmds > bd->queued_cmds);
@@ -511,7 +528,9 @@ __bsg_read(char __user *buf, size_t count, struct bsg_device *bd,
 	int nr_commands, ret;
 
 	if (count % sizeof(struct sg_io_v4))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	ret = 0;
 	nr_commands = count / sizeof(struct sg_io_v4);
@@ -548,6 +567,7 @@ __bsg_read(char __user *buf, size_t count, struct bsg_device *bd,
 
 static inline void bsg_set_block(struct bsg_device *bd, struct file *file)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (file->f_flags & O_NONBLOCK)
 		clear_bit(BSG_F_BLOCK, &bd->flags);
 	else
@@ -559,6 +579,7 @@ static inline void bsg_set_block(struct bsg_device *bd, struct file *file)
  */
 static inline int err_block_err(int ret)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret && ret != -ENOSPC && ret != -ENODATA && ret != -EAGAIN)
 		return 1;
 
@@ -580,6 +601,7 @@ bsg_read(struct file *file, char __user *buf, size_t count, loff_t *ppos)
 	ret = __bsg_read(buf, count, bd, NULL, &bytes_read);
 	*ppos = bytes_read;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!bytes_read || err_block_err(ret))
 		bytes_read = ret;
 
@@ -595,7 +617,9 @@ static int __bsg_write(struct bsg_device *bd, const char __user *buf,
 	int ret, nr_commands;
 
 	if (count % sizeof(struct sg_io_v4))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	nr_commands = count / sizeof(struct sg_io_v4);
 	rq = NULL;
@@ -650,7 +674,9 @@ bsg_write(struct file *file, const char __user *buf, size_t count, loff_t *ppos)
 	dprintk("%s: write %zd bytes\n", bd->name, count);
 
 	if (unlikely(uaccess_kernel()))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	bsg_set_block(bd, file);
 
@@ -676,7 +702,9 @@ static struct bsg_device *bsg_alloc_device(void)
 
 	bd = kzalloc(sizeof(struct bsg_device), GFP_KERNEL);
 	if (unlikely(!bd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	spin_lock_init(&bd->lock);
 
@@ -712,6 +740,7 @@ static int bsg_put_device(struct bsg_device *bd)
 
 	do_free = atomic_dec_and_test(&bd->ref_count);
 	if (!do_free) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mutex_unlock(&bsg_mutex);
 		goto out;
 	}
@@ -750,6 +779,7 @@ static struct bsg_device *bsg_add_device(struct inode *inode,
 	unsigned char buf[32];
 #endif
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!blk_queue_scsi_passthrough(rq)) {
 		WARN_ONCE(true, "Attempt to register a non-SCSI queue\n");
 		return ERR_PTR(-EINVAL);
@@ -786,6 +816,7 @@ static struct bsg_device *__bsg_get_device(int minor, struct request_queue *q)
 
 	mutex_lock(&bsg_mutex);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	hlist_for_each_entry(bd, bsg_dev_idx_hash(minor), dev_list) {
 		if (bd->queue == q) {
 			atomic_inc(&bd->ref_count);
@@ -809,7 +840,9 @@ static struct bsg_device *bsg_get_device(struct inode *inode, struct file *file)
 	mutex_lock(&bsg_mutex);
 	bcd = idr_find(&bsg_minor_idr, iminor(inode));
 	if (bcd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kref_get(&bcd->ref);
+}
 	mutex_unlock(&bsg_mutex);
 
 	if (!bcd)
@@ -833,7 +866,9 @@ static int bsg_open(struct inode *inode, struct file *file)
 	bd = bsg_get_device(inode, file);
 
 	if (IS_ERR(bd))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PTR_ERR(bd);
+}
 
 	file->private_data = bd;
 	return 0;
@@ -857,7 +892,9 @@ static unsigned int bsg_poll(struct file *file, poll_table *wait)
 
 	spin_lock_irq(&bd->lock);
 	if (!list_empty(&bd->done_list))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mask |= POLLIN | POLLRDNORM;
+}
 	if (bd->queued_cmds < bd->max_queue)
 		mask |= POLLOUT;
 	spin_unlock_irq(&bd->lock);
@@ -953,7 +990,9 @@ void bsg_unregister_queue(struct request_queue *q)
 	struct bsg_class_device *bcd = &q->bsg_dev;
 
 	if (!bcd->class_dev)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	mutex_lock(&bsg_mutex);
 	idr_remove(&bsg_minor_idr, bcd->minor);
@@ -976,7 +1015,9 @@ int bsg_register_queue(struct request_queue *q, struct device *parent,
 	const char *devname;
 
 	if (name)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		devname = name;
+}
 	else
 		devname = dev_name(parent);
 
@@ -984,7 +1025,9 @@ int bsg_register_queue(struct request_queue *q, struct device *parent,
 	 * we need a proper transport to send commands, not a stacked device
 	 */
 	if (!queue_is_rq_based(q))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	bcd = &q->bsg_dev;
 	memset(bcd, 0, sizeof(*bcd));
@@ -993,7 +1036,9 @@ int bsg_register_queue(struct request_queue *q, struct device *parent,
 
 	ret = idr_alloc(&bsg_minor_idr, bcd, 0, BSG_MAX_DEVS, GFP_KERNEL);
 	if (ret < 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (ret == -ENOSPC) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			printk(KERN_ERR "bsg: too many bsg devices\n");
 			ret = -EINVAL;
 		}
@@ -1008,6 +1053,7 @@ int bsg_register_queue(struct request_queue *q, struct device *parent,
 	dev = MKDEV(bsg_major, bcd->minor);
 	class_dev = device_create(bsg_class, parent, dev, NULL, "%s", devname);
 	if (IS_ERR(class_dev)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = PTR_ERR(class_dev);
 		goto put_dev;
 	}
@@ -1048,6 +1094,7 @@ static int __init bsg_init(void)
 	bsg_cmd_cachep = kmem_cache_create("bsg_cmd",
 				sizeof(struct bsg_command), 0, 0, NULL);
 	if (!bsg_cmd_cachep) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		printk(KERN_ERR "bsg: failed creating slab cache\n");
 		return -ENOMEM;
 	}
@@ -1057,6 +1104,7 @@ static int __init bsg_init(void)
 
 	bsg_class = class_create(THIS_MODULE, "bsg");
 	if (IS_ERR(bsg_class)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = PTR_ERR(bsg_class);
 		goto destroy_kmemcache;
 	}
diff --git a/block/cfq-iosched.c b/block/cfq-iosched.c
index 9f342ef..d2c0add 100644
--- a/block/cfq-iosched.c
+++ b/block/cfq-iosched.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  CFQ, or complete fairness queueing, disk scheduler.
  *
@@ -406,6 +408,7 @@ static struct cfq_rb_root *st_for(struct cfq_group *cfqg,
 					    enum wl_class_t class,
 					    enum wl_type_t type)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!cfqg)
 		return NULL;
 
@@ -2181,6 +2184,7 @@ static struct cftype cfq_blkcg_files[] = {
 static struct cfq_group *cfq_lookup_cfqg(struct cfq_data *cfqd,
 					 struct blkcg *blkcg)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return cfqd->root_group;
 }
 
@@ -4885,6 +4889,7 @@ static int __init cfq_init(void)
 	if (ret)
 		goto err_free_pool;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 err_free_pool:
diff --git a/block/deadline-iosched.c b/block/deadline-iosched.c
index b83f774..b46a3c6 100644
--- a/block/deadline-iosched.c
+++ b/block/deadline-iosched.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  Deadline i/o scheduler.
  *
@@ -55,6 +57,7 @@ static void deadline_move_request(struct deadline_data *, struct request *);
 static inline struct rb_root *
 deadline_rb_root(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return &dd->sort_list[rq_data_dir(rq)];
 }
 
@@ -64,6 +67,7 @@ deadline_rb_root(struct deadline_data *dd, struct request *rq)
 static inline struct request *
 deadline_latter_request(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct rb_node *node = rb_next(&rq->rb_node);
 
 	if (node)
@@ -75,6 +79,7 @@ deadline_latter_request(struct request *rq)
 static void
 deadline_add_rq_rb(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct rb_root *root = deadline_rb_root(dd, rq);
 
 	elv_rb_add(root, rq);
@@ -83,6 +88,7 @@ deadline_add_rq_rb(struct deadline_data *dd, struct request *rq)
 static inline void
 deadline_del_rq_rb(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const int data_dir = rq_data_dir(rq);
 
 	if (dd->next_rq[data_dir] == rq)
@@ -98,6 +104,7 @@ static void
 deadline_add_request(struct request_queue *q, struct request *rq)
 {
 	struct deadline_data *dd = q->elevator->elevator_data;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const int data_dir = rq_data_dir(rq);
 
 	deadline_add_rq_rb(dd, rq);
@@ -132,6 +139,7 @@ deadline_merge(struct request_queue *q, struct request **req, struct bio *bio)
 	if (dd->front_merges) {
 		sector_t sector = bio_end_sector(bio);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		__rq = elv_rb_find(&dd->sort_list[bio_data_dir(bio)], sector);
 		if (__rq) {
 			BUG_ON(sector != blk_rq_pos(__rq));
@@ -155,6 +163,7 @@ static void deadline_merged_request(struct request_queue *q,
 	 * if the merge was a front merge, we need to reposition request
 	 */
 	if (type == ELEVATOR_FRONT_MERGE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		elv_rb_del(deadline_rb_root(dd, req), req);
 		deadline_add_rq_rb(dd, req);
 	}
@@ -200,6 +209,7 @@ deadline_move_to_dispatch(struct deadline_data *dd, struct request *rq)
 static void
 deadline_move_request(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const int data_dir = rq_data_dir(rq);
 
 	dd->next_rq[READ] = NULL;
@@ -219,6 +229,7 @@ deadline_move_request(struct deadline_data *dd, struct request *rq)
  */
 static inline int deadline_check_fifo(struct deadline_data *dd, int ddir)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request *rq = rq_entry_fifo(dd->fifo_list[ddir].next);
 
 	/*
@@ -246,7 +257,9 @@ static int deadline_dispatch_requests(struct request_queue *q, int force)
 	 * batches are currently reads XOR writes
 	 */
 	if (dd->next_rq[WRITE])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rq = dd->next_rq[WRITE];
+}
 	else
 		rq = dd->next_rq[READ];
 
@@ -260,11 +273,14 @@ static int deadline_dispatch_requests(struct request_queue *q, int force)
 	 */
 
 	if (reads) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		BUG_ON(RB_EMPTY_ROOT(&dd->sort_list[READ]));
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (writes && (dd->starved++ >= dd->writes_starved))
 			goto dispatch_writes;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		data_dir = READ;
 
 		goto dispatch_find_request;
@@ -278,6 +294,7 @@ static int deadline_dispatch_requests(struct request_queue *q, int force)
 dispatch_writes:
 		BUG_ON(RB_EMPTY_ROOT(&dd->sort_list[WRITE]));
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		dd->starved = 0;
 
 		data_dir = WRITE;
@@ -285,6 +302,7 @@ static int deadline_dispatch_requests(struct request_queue *q, int force)
 		goto dispatch_find_request;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 
 dispatch_find_request:
@@ -306,6 +324,7 @@ static int deadline_dispatch_requests(struct request_queue *q, int force)
 		rq = dd->next_rq[data_dir];
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	dd->batching = 0;
 
 dispatch_request:
@@ -322,6 +341,7 @@ static void deadline_exit_queue(struct elevator_queue *e)
 {
 	struct deadline_data *dd = e->elevator_data;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!list_empty(&dd->fifo_list[READ]));
 	BUG_ON(!list_empty(&dd->fifo_list[WRITE]));
 
@@ -338,10 +358,13 @@ static int deadline_init_queue(struct request_queue *q, struct elevator_type *e)
 
 	eq = elevator_alloc(q, e);
 	if (!eq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	dd = kzalloc_node(sizeof(*dd), GFP_KERNEL, q->node);
 	if (!dd) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kobject_put(&eq->kobj);
 		return -ENOMEM;
 	}
@@ -370,6 +393,7 @@ static int deadline_init_queue(struct request_queue *q, struct elevator_type *e)
 static ssize_t
 deadline_var_show(int var, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%d\n", var);
 }
 
@@ -458,6 +482,7 @@ static int __init deadline_init(void)
 
 static void __exit deadline_exit(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	elv_unregister(&iosched_deadline);
 }
 
diff --git a/block/elevator.c b/block/elevator.c
index 153926a..6d81c72 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  Block device elevator/IO-scheduler.
  *
@@ -60,6 +62,7 @@ static int elv_iosched_allow_bio_merge(struct request *rq, struct bio *bio)
 	struct request_queue *q = rq->q;
 	struct elevator_queue *e = q->elevator;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (e->uses_mq && e->type->ops.mq.allow_merge)
 		return e->type->ops.mq.allow_merge(q, rq, bio);
 	else if (!e->uses_mq && e->type->ops.sq.elevator_allow_bio_merge_fn)
@@ -73,6 +76,7 @@ static int elv_iosched_allow_bio_merge(struct request *rq, struct bio *bio)
  */
 bool elv_bio_merge_ok(struct request *rq, struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!blk_rq_merge_ok(rq, bio))
 		return false;
 
@@ -89,14 +93,18 @@ static struct elevator_type *elevator_find(const char *name)
 
 	list_for_each_entry(e, &elv_list, list) {
 		if (!strcmp(e->elevator_name, name))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return e;
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
 static void elevator_put(struct elevator_type *e)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	module_put(e->elevator_owner);
 }
 
@@ -108,6 +116,7 @@ static struct elevator_type *elevator_get(const char *name, bool try_loading)
 
 	e = elevator_find(name);
 	if (!e && try_loading) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(&elv_list_lock);
 		request_module("%s-iosched", name);
 		spin_lock(&elv_list_lock);
@@ -115,8 +124,11 @@ static struct elevator_type *elevator_get(const char *name, bool try_loading)
 	}
 
 	if (e && !try_module_get(e->elevator_owner))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		e = NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock(&elv_list_lock);
 
 	return e;
@@ -142,15 +154,20 @@ void __init load_default_elevator_module(void)
 	struct elevator_type *e;
 
 	if (!chosen_elevator[0])
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock(&elv_list_lock);
 	e = elevator_find(chosen_elevator);
 	spin_unlock(&elv_list_lock);
 
 	if (!e)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		request_module("%s-iosched", chosen_elevator);
 }
+}
 
 static struct kobj_type elv_ktype;
 
@@ -161,7 +178,9 @@ struct elevator_queue *elevator_alloc(struct request_queue *q,
 
 	eq = kzalloc_node(sizeof(*eq), GFP_KERNEL, q->node);
 	if (unlikely(!eq))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	eq->type = e;
 	kobject_init(&eq->kobj, &elv_ktype);
@@ -177,6 +196,7 @@ static void elevator_release(struct kobject *kobj)
 {
 	struct elevator_queue *e;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	e = container_of(kobj, struct elevator_queue, kobj);
 	elevator_put(e->type);
 	kfree(e);
@@ -194,7 +214,9 @@ int elevator_init(struct request_queue *q, char *name)
 	lockdep_assert_held(&q->sysfs_lock);
 
 	if (unlikely(q->elevator))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	INIT_LIST_HEAD(&q->queue_head);
 	q->last_merge = NULL;
@@ -202,9 +224,12 @@ int elevator_init(struct request_queue *q, char *name)
 	q->boundary_rq = NULL;
 
 	if (name) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		e = elevator_get(name, true);
 		if (!e)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EINVAL;
+}
 	}
 
 	/*
@@ -214,10 +239,13 @@ int elevator_init(struct request_queue *q, char *name)
 	 * allowed from async.
 	 */
 	if (!e && !q->mq_ops && *chosen_elevator) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		e = elevator_get(chosen_elevator, false);
 		if (!e)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			printk(KERN_ERR "I/O scheduler %s not found\n",
 							chosen_elevator);
+}
 	}
 
 	if (!e) {
@@ -231,11 +259,14 @@ int elevator_init(struct request_queue *q, char *name)
 			if (q->nr_hw_queues == 1)
 				e = elevator_get("mq-deadline", false);
 			if (!e)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				return 0;
+}
 		} else
 			e = elevator_get(CONFIG_DEFAULT_IOSCHED, false);
 
 		if (!e) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			printk(KERN_ERR
 				"Default I/O scheduler not found. " \
 				"Using noop.\n");
@@ -248,13 +279,17 @@ int elevator_init(struct request_queue *q, char *name)
 	else
 		err = e->ops.sq.elevator_init_fn(q, e);
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		elevator_put(e);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return err;
 }
 EXPORT_SYMBOL(elevator_init);
 
 void elevator_exit(struct request_queue *q, struct elevator_queue *e)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	mutex_lock(&e->sysfs_lock);
 	if (e->uses_mq && e->type->ops.mq.exit_sched)
 		blk_mq_exit_sched(q, e);
@@ -268,12 +303,14 @@ EXPORT_SYMBOL(elevator_exit);
 
 static inline void __elv_rqhash_del(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	hash_del(&rq->hash);
 	rq->rq_flags &= ~RQF_HASHED;
 }
 
 void elv_rqhash_del(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ELV_ON_HASH(rq))
 		__elv_rqhash_del(rq);
 }
@@ -283,6 +320,7 @@ void elv_rqhash_add(struct request_queue *q, struct request *rq)
 {
 	struct elevator_queue *e = q->elevator;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(ELV_ON_HASH(rq));
 	hash_add(e->hash, &rq->hash, rq_hash_key(rq));
 	rq->rq_flags |= RQF_HASHED;
@@ -291,6 +329,7 @@ EXPORT_SYMBOL_GPL(elv_rqhash_add);
 
 void elv_rqhash_reposition(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	__elv_rqhash_del(rq);
 	elv_rqhash_add(q, rq);
 }
@@ -301,6 +340,7 @@ struct request *elv_rqhash_find(struct request_queue *q, sector_t offset)
 	struct hlist_node *next;
 	struct request *rq;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	hash_for_each_possible_safe(e->hash, rq, next, hash, offset) {
 		BUG_ON(!ELV_ON_HASH(rq));
 
@@ -326,6 +366,7 @@ void elv_rb_add(struct rb_root *root, struct request *rq)
 	struct rb_node *parent = NULL;
 	struct request *__rq;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (*p) {
 		parent = *p;
 		__rq = rb_entry(parent, struct request, rb_node);
@@ -343,6 +384,7 @@ EXPORT_SYMBOL(elv_rb_add);
 
 void elv_rb_del(struct rb_root *root, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(RB_EMPTY_NODE(&rq->rb_node));
 	rb_erase(&rq->rb_node, root);
 	RB_CLEAR_NODE(&rq->rb_node);
@@ -354,6 +396,7 @@ struct request *elv_rb_find(struct rb_root *root, sector_t sector)
 	struct rb_node *n = root->rb_node;
 	struct request *rq;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (n) {
 		rq = rb_entry(n, struct request, rb_node);
 
@@ -380,7 +423,9 @@ void elv_dispatch_sort(struct request_queue *q, struct request *rq)
 	struct list_head *entry;
 
 	if (q->last_merge == rq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		q->last_merge = NULL;
+}
 
 	elv_rqhash_del(q, rq);
 
@@ -418,6 +463,7 @@ EXPORT_SYMBOL(elv_dispatch_sort);
  */
 void elv_dispatch_add_tail(struct request_queue *q, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (q->last_merge == rq)
 		q->last_merge = NULL;
 
@@ -490,6 +536,7 @@ bool elv_attempt_insert_merge(struct request_queue *q, struct request *rq)
 	struct request *__rq;
 	bool ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (blk_queue_nomerges(q))
 		return false;
 
@@ -524,6 +571,7 @@ void elv_merged_request(struct request_queue *q, struct request *rq,
 {
 	struct elevator_queue *e = q->elevator;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (e->uses_mq && e->type->ops.mq.request_merged)
 		e->type->ops.mq.request_merged(q, rq, type);
 	else if (!e->uses_mq && e->type->ops.sq.elevator_merged_fn)
@@ -541,6 +589,7 @@ void elv_merge_requests(struct request_queue *q, struct request *rq,
 	struct elevator_queue *e = q->elevator;
 	bool next_sorted = false;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (e->uses_mq && e->type->ops.mq.requests_merged)
 		e->type->ops.mq.requests_merged(q, rq, next);
 	else if (e->type->ops.sq.elevator_merge_req_fn) {
@@ -564,6 +613,7 @@ void elv_bio_merged(struct request_queue *q, struct request *rq,
 {
 	struct elevator_queue *e = q->elevator;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (WARN_ON_ONCE(e->uses_mq))
 		return;
 
@@ -574,6 +624,7 @@ void elv_bio_merged(struct request_queue *q, struct request *rq,
 #ifdef CONFIG_PM
 static void blk_pm_requeue_request(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (rq->q->dev && !(rq->rq_flags & RQF_PM))
 		rq->q->nr_pending--;
 }
@@ -865,7 +916,9 @@ int elv_register_queue(struct request_queue *q)
 		kobject_uevent(&e->kobj, KOBJ_ADD);
 		e->registered = 1;
 		if (!e->uses_mq && e->type->ops.sq.elevator_registered_fn)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			e->type->ops.sq.elevator_registered_fn(q);
+}
 	}
 	return error;
 }
@@ -873,6 +926,7 @@ EXPORT_SYMBOL(elv_register_queue);
 
 void elv_unregister_queue(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (q) {
 		struct elevator_queue *e = q->elevator;
 
@@ -900,15 +954,21 @@ int elv_register(struct elevator_type *e)
 		e->icq_cache = kmem_cache_create(e->icq_cache_name, e->icq_size,
 						 e->icq_align, 0, NULL);
 		if (!e->icq_cache)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -ENOMEM;
+}
 	}
 
 	/* register, don't allow duplicate names */
 	spin_lock(&elv_list_lock);
 	if (elevator_find(e->elevator_name)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		spin_unlock(&elv_list_lock);
 		if (e->icq_cache)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			kmem_cache_destroy(e->icq_cache);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EBUSY;
 	}
 	list_add_tail(&e->list, &elv_list);
@@ -953,6 +1013,7 @@ static int elevator_switch_mq(struct request_queue *q,
 	blk_mq_freeze_queue(q);
 
 	if (q->elevator) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (q->elevator->registered)
 			elv_unregister_queue(q);
 		ioc_clear_queue(q);
@@ -994,7 +1055,9 @@ static int elevator_switch(struct request_queue *q, struct elevator_type *new_e)
 	int err;
 
 	if (q->mq_ops)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return elevator_switch_mq(q, new_e);
+}
 
 	/*
 	 * Turn on BYPASS and drain all requests w/ elevator private data.
@@ -1090,6 +1153,7 @@ static int __elevator_change(struct request_queue *q, const char *name)
 
 static inline bool elv_support_iosched(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (q->mq_ops && q->tag_set && (q->tag_set->flags &
 				BLK_MQ_F_NO_SCHED))
 		return false;
@@ -1101,6 +1165,7 @@ ssize_t elv_iosched_store(struct request_queue *q, const char *name,
 {
 	int ret;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!(q->mq_ops || q->request_fn) || !elv_support_iosched(q))
 		return count;
 
@@ -1118,6 +1183,7 @@ ssize_t elv_iosched_show(struct request_queue *q, char *name)
 	struct elevator_type *__e;
 	int len = 0;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!blk_queue_stackable(q))
 		return sprintf(name, "none\n");
 
@@ -1149,6 +1215,7 @@ ssize_t elv_iosched_show(struct request_queue *q, char *name)
 struct request *elv_rb_former_request(struct request_queue *q,
 				      struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct rb_node *rbprev = rb_prev(&rq->rb_node);
 
 	if (rbprev)
@@ -1161,6 +1228,7 @@ EXPORT_SYMBOL(elv_rb_former_request);
 struct request *elv_rb_latter_request(struct request_queue *q,
 				      struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct rb_node *rbnext = rb_next(&rq->rb_node);
 
 	if (rbnext)
diff --git a/block/genhd.c b/block/genhd.c
index dd305c6..b650fce 100644
--- a/block/genhd.c
+++ b/block/genhd.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  gendisk handling
  */
@@ -47,6 +49,7 @@ static void disk_release_events(struct gendisk *disk);
 
 void part_inc_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (q->mq_ops)
 		return;
 
@@ -57,6 +60,7 @@ void part_inc_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
 
 void part_dec_in_flight(struct request_queue *q, struct hd_struct *part, int rw)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (q->mq_ops)
 		return;
 
@@ -76,6 +80,7 @@ void part_in_flight(struct request_queue *q, struct hd_struct *part,
 	inflight[0] = atomic_read(&part->in_flight[0]) +
 			atomic_read(&part->in_flight[1]);
 	if (part->partno) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		part = &part_to_disk(part)->part0;
 		inflight[1] = atomic_read(&part->in_flight[0]) +
 				atomic_read(&part->in_flight[1]);
@@ -87,7 +92,9 @@ struct hd_struct *__disk_get_part(struct gendisk *disk, int partno)
 	struct disk_part_tbl *ptbl = rcu_dereference(disk->part_tbl);
 
 	if (unlikely(partno < 0 || partno >= ptbl->len))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	return rcu_dereference(ptbl->part[partno]);
 }
 
@@ -113,6 +120,7 @@ struct hd_struct *disk_get_part(struct gendisk *disk, int partno)
 	part = __disk_get_part(disk, partno);
 	if (part)
 		get_device(part_to_dev(part));
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_unlock();
 
 	return part;
@@ -142,7 +150,9 @@ void disk_part_iter_init(struct disk_part_iter *piter, struct gendisk *disk,
 	piter->part = NULL;
 
 	if (flags & DISK_PITER_REVERSE)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		piter->idx = ptbl->len - 1;
+}
 	else if (flags & (DISK_PITER_INCL_PART0 | DISK_PITER_INCL_EMPTY_PART0))
 		piter->idx = 0;
 	else
@@ -178,6 +188,7 @@ struct hd_struct *disk_part_iter_next(struct disk_part_iter *piter)
 
 	/* determine iteration parameters */
 	if (piter->flags & DISK_PITER_REVERSE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		inc = -1;
 		if (piter->flags & (DISK_PITER_INCL_PART0 |
 				    DISK_PITER_INCL_EMPTY_PART0))
@@ -185,6 +196,7 @@ struct hd_struct *disk_part_iter_next(struct disk_part_iter *piter)
 		else
 			end = 0;
 	} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		inc = 1;
 		end = ptbl->len;
 	}
@@ -208,6 +220,7 @@ struct hd_struct *disk_part_iter_next(struct disk_part_iter *piter)
 		break;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rcu_read_unlock();
 
 	return piter->part;
@@ -232,6 +245,7 @@ EXPORT_SYMBOL_GPL(disk_part_iter_exit);
 
 static inline int sector_in_part(struct hd_struct *part, sector_t sector)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return part->start_sect <= sector &&
 		sector < part->start_sect + part_nr_sects_read(part);
 }
@@ -257,6 +271,7 @@ struct hd_struct *disk_map_sector_rcu(struct gendisk *disk, sector_t sector)
 	struct hd_struct *part;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	ptbl = rcu_dereference(disk->part_tbl);
 
 	part = rcu_dereference(ptbl->last_lookup);
@@ -976,6 +991,7 @@ static const struct file_operations proc_partitions_operations = {
 
 static struct kobject *base_probe(dev_t devt, int *partno, void *data)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (request_module("block-major-%d-%d", MAJOR(devt), MINOR(devt)) > 0)
 		/* Make old-style 2.4 aliases work */
 		request_module("block-major-%d", MAJOR(devt));
@@ -989,7 +1005,9 @@ static int __init genhd_device_init(void)
 	block_class.dev_kobj = sysfs_dev_block_kobj;
 	error = class_register(&block_class);
 	if (unlikely(error))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return error;
+}
 	bdev_map = kobj_map_init(base_probe, &block_class_lock);
 	blk_dev_init();
 
@@ -1006,6 +1024,7 @@ subsys_initcall(genhd_device_init);
 static ssize_t disk_range_show(struct device *dev,
 			       struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%d\n", disk->minors);
@@ -1014,6 +1033,7 @@ static ssize_t disk_range_show(struct device *dev,
 static ssize_t disk_ext_range_show(struct device *dev,
 				   struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%d\n", disk_max_parts(disk));
@@ -1022,6 +1042,7 @@ static ssize_t disk_ext_range_show(struct device *dev,
 static ssize_t disk_removable_show(struct device *dev,
 				   struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%d\n",
@@ -1031,6 +1052,7 @@ static ssize_t disk_removable_show(struct device *dev,
 static ssize_t disk_ro_show(struct device *dev,
 				   struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%d\n", get_disk_ro(disk) ? 1 : 0);
@@ -1039,6 +1061,7 @@ static ssize_t disk_ro_show(struct device *dev,
 static ssize_t disk_capability_show(struct device *dev,
 				    struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%x\n", disk->flags);
@@ -1048,6 +1071,7 @@ static ssize_t disk_alignment_offset_show(struct device *dev,
 					  struct device_attribute *attr,
 					  char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%d\n", queue_alignment_offset(disk->queue));
@@ -1057,6 +1081,7 @@ static ssize_t disk_discard_alignment_show(struct device *dev,
 					   struct device_attribute *attr,
 					   char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%d\n", queue_discard_alignment(disk->queue));
@@ -1108,11 +1133,15 @@ static struct attribute *disk_attrs[] = {
 
 static umode_t disk_visible(struct kobject *kobj, struct attribute *a, int n)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct device *dev = container_of(kobj, typeof(*dev), kobj);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	if (a == &dev_attr_badblocks.attr && !disk->bb)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	return a->mode;
 }
 
@@ -1146,7 +1175,9 @@ static void disk_replace_part_tbl(struct gendisk *disk,
 	rcu_assign_pointer(disk->part_tbl, new_ptbl);
 
 	if (old_ptbl) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rcu_assign_pointer(old_ptbl->last_lookup, NULL);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree_rcu(old_ptbl, rcu_head);
 	}
 }
@@ -1181,24 +1212,34 @@ int disk_expand_part_tbl(struct gendisk *disk, int partno)
 	 */
 	target = partno + 1;
 	if (target < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	/* disk_max_parts() is zero during initialization, ignore if so */
 	if (disk_max_parts(disk) && target > disk_max_parts(disk))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (target <= len)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	size = sizeof(*new_ptbl) + target * sizeof(new_ptbl->part[0]);
 	new_ptbl = kzalloc_node(size, GFP_KERNEL, disk->node_id);
 	if (!new_ptbl)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	new_ptbl->len = target;
 
 	for (i = 0; i < len; i++)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rcu_assign_pointer(new_ptbl->part[i], old_ptbl->part[i]);
+}
 
 	disk_replace_part_tbl(disk, new_ptbl);
 	return 0;
@@ -1206,6 +1247,7 @@ int disk_expand_part_tbl(struct gendisk *disk, int partno)
 
 static void disk_release(struct device *dev)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	blk_free_devt(dev->devt);
@@ -1227,7 +1269,10 @@ static char *block_devnode(struct device *dev, umode_t *mode,
 	struct gendisk *disk = dev_to_disk(dev);
 
 	if (disk->devnode)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return disk->devnode(disk, mode);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return NULL;
 }
 
@@ -1326,6 +1371,7 @@ dev_t blk_lookup_devt(const char *name, int partno)
 	struct device *dev;
 
 	class_dev_iter_init(&iter, &block_class, NULL, &disk_type);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while ((dev = class_dev_iter_next(&iter))) {
 		struct gendisk *disk = dev_to_disk(dev);
 		struct hd_struct *part;
@@ -1366,6 +1412,7 @@ struct gendisk *alloc_disk_node(int minors, int node_id)
 	struct disk_part_tbl *ptbl;
 
 	if (minors > DISK_MAX_PARTS) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		printk(KERN_ERR
 			"block: can't allocated more than %d partitions\n",
 			DISK_MAX_PARTS);
@@ -1375,11 +1422,13 @@ struct gendisk *alloc_disk_node(int minors, int node_id)
 	disk = kzalloc_node(sizeof(struct gendisk), GFP_KERNEL, node_id);
 	if (disk) {
 		if (!init_part_stats(&disk->part0)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			kfree(disk);
 			return NULL;
 		}
 		disk->node_id = node_id;
 		if (disk_expand_part_tbl(disk, 0)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			free_part_stats(&disk->part0);
 			kfree(disk);
 			return NULL;
@@ -1398,6 +1447,7 @@ struct gendisk *alloc_disk_node(int minors, int node_id)
 		 */
 		seqcount_init(&disk->part0.nr_sects_seq);
 		if (hd_ref_init(&disk->part0)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			hd_free_part(&disk->part0);
 			kfree(disk);
 			return NULL;
@@ -1409,6 +1459,7 @@ struct gendisk *alloc_disk_node(int minors, int node_id)
 		disk_to_dev(disk)->type = &disk_type;
 		device_initialize(disk_to_dev(disk));
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return disk;
 }
 EXPORT_SYMBOL(alloc_disk_node);
@@ -1419,15 +1470,21 @@ struct kobject *get_disk(struct gendisk *disk)
 	struct kobject *kobj;
 
 	if (!disk->fops)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	owner = disk->fops->owner;
 	if (owner && !try_module_get(owner))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	kobj = kobject_get_unless_zero(&disk_to_dev(disk)->kobj);
 	if (kobj == NULL) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		module_put(owner);
 		return NULL;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return kobj;
 
 }
@@ -1448,12 +1505,15 @@ static void set_disk_ro_uevent(struct gendisk *gd, int ro)
 	char *envp[] = { event, NULL };
 
 	if (!ro)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		event[8] = '0';
+}
 	kobject_uevent_env(&disk_to_dev(gd)->kobj, KOBJ_CHANGE, envp);
 }
 
 void set_device_ro(struct block_device *bdev, int flag)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	bdev->bd_part->policy = flag;
 }
 
@@ -1465,6 +1525,7 @@ void set_disk_ro(struct gendisk *disk, int flag)
 	struct hd_struct *part;
 
 	if (disk->part0.policy != flag) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		set_disk_ro_uevent(disk, flag);
 		disk->part0.policy = flag;
 	}
@@ -1480,7 +1541,10 @@ EXPORT_SYMBOL(set_disk_ro);
 int bdev_read_only(struct block_device *bdev)
 {
 	if (!bdev)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return bdev->bd_part->policy;
 }
 
@@ -1545,7 +1609,9 @@ static unsigned long disk_events_poll_jiffies(struct gendisk *disk)
 	 * can't be monitored asynchronously.
 	 */
 	if (ev->poll_msecs >= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		intv_msecs = ev->poll_msecs;
+}
 	else if (disk->events & ~disk->async_events)
 		intv_msecs = disk_events_dfl_poll_msecs;
 
@@ -1574,7 +1640,9 @@ void disk_block_events(struct gendisk *disk)
 	bool cancel;
 
 	if (!ev)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/*
 	 * Outer mutex ensures that the first blocker completes canceling
@@ -1611,8 +1679,10 @@ static void __disk_unblock_events(struct gendisk *disk, bool check_now)
 		queue_delayed_work(system_freezable_power_efficient_wq,
 				&ev->dwork, 0);
 	else if (intv)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		queue_delayed_work(system_freezable_power_efficient_wq,
 				&ev->dwork, intv);
+}
 out_unlock:
 	spin_unlock_irqrestore(&ev->lock, flags);
 }
@@ -1650,13 +1720,18 @@ void disk_flush_events(struct gendisk *disk, unsigned int mask)
 	struct disk_events *ev = disk->ev;
 
 	if (!ev)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_lock_irq(&ev->lock);
 	ev->clearing |= mask;
 	if (!ev->block)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		mod_delayed_work(system_freezable_power_efficient_wq,
 				&ev->dwork, 0);
+}
 	spin_unlock_irq(&ev->lock);
 }
 
@@ -1683,6 +1758,7 @@ unsigned int disk_clear_events(struct gendisk *disk, unsigned int mask)
 		if ((mask & DISK_EVENT_MEDIA_CHANGE) &&
 		    bdops->media_changed && bdops->media_changed(disk))
 			return DISK_EVENT_MEDIA_CHANGE;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
 	}
 
@@ -1721,6 +1797,7 @@ unsigned int disk_clear_events(struct gendisk *disk, unsigned int mask)
  */
 static void disk_events_workfn(struct work_struct *work)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct delayed_work *dwork = to_delayed_work(work);
 	struct disk_events *ev = container_of(dwork, struct disk_events, dwork);
 
@@ -1749,9 +1826,12 @@ static void disk_check_events(struct disk_events *ev,
 
 	intv = disk_events_poll_jiffies(disk);
 	if (!ev->block && intv)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		queue_delayed_work(system_freezable_power_efficient_wq,
 				&ev->dwork, intv);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	spin_unlock_irq(&ev->lock);
 
 	/*
@@ -1781,6 +1861,7 @@ static ssize_t __disk_events_show(unsigned int events, char *buf)
 	ssize_t pos = 0;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < ARRAY_SIZE(disk_events_strs); i++)
 		if (events & (1 << i)) {
 			pos += sprintf(buf + pos, "%s%s",
@@ -1795,6 +1876,7 @@ static ssize_t __disk_events_show(unsigned int events, char *buf)
 static ssize_t disk_events_show(struct device *dev,
 				struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return __disk_events_show(disk->events, buf);
@@ -1803,6 +1885,7 @@ static ssize_t disk_events_show(struct device *dev,
 static ssize_t disk_events_async_show(struct device *dev,
 				      struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return __disk_events_show(disk->async_events, buf);
@@ -1812,6 +1895,7 @@ static ssize_t disk_events_poll_msecs_show(struct device *dev,
 					   struct device_attribute *attr,
 					   char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 
 	return sprintf(buf, "%ld\n", disk->ev->poll_msecs);
@@ -1821,6 +1905,7 @@ static ssize_t disk_events_poll_msecs_store(struct device *dev,
 					    struct device_attribute *attr,
 					    const char *buf, size_t count)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct gendisk *disk = dev_to_disk(dev);
 	long intv;
 
@@ -1864,7 +1949,9 @@ static int disk_events_set_dfl_poll_msecs(const char *val,
 
 	ret = param_set_ulong(val, kp);
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	mutex_lock(&disk_events_mutex);
 
@@ -1895,10 +1982,13 @@ static void disk_alloc_events(struct gendisk *disk)
 	struct disk_events *ev;
 
 	if (!disk->fops->check_events)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	ev = kzalloc(sizeof(*ev), GFP_KERNEL);
 	if (!ev) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("%s: failed to initialize events\n", disk->disk_name);
 		return;
 	}
@@ -1917,12 +2007,16 @@ static void disk_alloc_events(struct gendisk *disk)
 static void disk_add_events(struct gendisk *disk)
 {
 	if (!disk->ev)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	/* FIXME: error handling */
 	if (sysfs_create_files(&disk_to_dev(disk)->kobj, disk_events_attrs) < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_warn("%s: failed to create sysfs files for events\n",
 			disk->disk_name);
+}
 
 	mutex_lock(&disk_events_mutex);
 	list_add_tail(&disk->ev->node, &disk_events);
@@ -1937,6 +2031,7 @@ static void disk_add_events(struct gendisk *disk)
 
 static void disk_del_events(struct gendisk *disk)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!disk->ev)
 		return;
 
diff --git a/block/ioctl.c b/block/ioctl.c
index 0de02ee..d44b250 100644
--- a/block/ioctl.c
+++ b/block/ioctl.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 #include <linux/capability.h>
 #include <linux/blkdev.h>
 #include <linux/export.h>
@@ -22,7 +24,9 @@ static int blkpg_ioctl(struct block_device *bdev, struct blkpg_ioctl_arg __user
 	int partno;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EACCES;
+}
 	if (copy_from_user(&a, arg, sizeof(struct blkpg_ioctl_arg)))
 		return -EFAULT;
 	if (copy_from_user(&p, a.data, sizeof(struct blkpg_partition)))
@@ -163,6 +167,7 @@ int __blkdev_reread_part(struct block_device *bdev)
 {
 	struct gendisk *disk = bdev->bd_disk;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!disk_part_scan_enabled(disk) || bdev != bdev->bd_contains)
 		return -EINVAL;
 	if (!capable(CAP_SYS_ADMIN))
@@ -204,7 +209,9 @@ static int blk_ioctl_discard(struct block_device *bdev, fmode_t mode,
 	uint64_t start, len;
 
 	if (!(mode & FMODE_WRITE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EBADF;
+}
 
 	if (copy_from_user(range, (void __user *)arg, sizeof(range)))
 		return -EFAULT;
@@ -232,7 +239,9 @@ static int blk_ioctl_zeroout(struct block_device *bdev, fmode_t mode,
 	uint64_t start, end, len;
 
 	if (!(mode & FMODE_WRITE))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EBADF;
+}
 
 	if (copy_from_user(range, (void __user *)arg, sizeof(range)))
 		return -EFAULT;
@@ -260,6 +269,7 @@ static int blk_ioctl_zeroout(struct block_device *bdev, fmode_t mode,
 
 static int put_ushort(unsigned long arg, unsigned short val)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(val, (unsigned short __user *)arg);
 }
 
@@ -270,16 +280,19 @@ static int put_int(unsigned long arg, int val)
 
 static int put_uint(unsigned long arg, unsigned int val)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(val, (unsigned int __user *)arg);
 }
 
 static int put_long(unsigned long arg, long val)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(val, (long __user *)arg);
 }
 
 static int put_ulong(unsigned long arg, unsigned long val)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(val, (unsigned long __user *)arg);
 }
 
@@ -294,8 +307,11 @@ int __blkdev_driver_ioctl(struct block_device *bdev, fmode_t mode,
 	struct gendisk *disk = bdev->bd_disk;
 
 	if (disk->fops->ioctl)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return disk->fops->ioctl(bdev, mode, cmd, arg);
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return -ENOTTY;
 }
 /*
@@ -312,7 +328,9 @@ static int blkdev_pr_register(struct block_device *bdev,
 	struct pr_registration reg;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 	if (!ops || !ops->pr_register)
 		return -EOPNOTSUPP;
 	if (copy_from_user(&reg, arg, sizeof(reg)))
@@ -330,7 +348,9 @@ static int blkdev_pr_reserve(struct block_device *bdev,
 	struct pr_reservation rsv;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 	if (!ops || !ops->pr_reserve)
 		return -EOPNOTSUPP;
 	if (copy_from_user(&rsv, arg, sizeof(rsv)))
@@ -348,7 +368,9 @@ static int blkdev_pr_release(struct block_device *bdev,
 	struct pr_reservation rsv;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 	if (!ops || !ops->pr_release)
 		return -EOPNOTSUPP;
 	if (copy_from_user(&rsv, arg, sizeof(rsv)))
@@ -366,7 +388,9 @@ static int blkdev_pr_preempt(struct block_device *bdev,
 	struct pr_preempt p;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 	if (!ops || !ops->pr_preempt)
 		return -EOPNOTSUPP;
 	if (copy_from_user(&p, arg, sizeof(p)))
@@ -384,7 +408,9 @@ static int blkdev_pr_clear(struct block_device *bdev,
 	struct pr_clear c;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EPERM;
+}
 	if (!ops || !ops->pr_clear)
 		return -EOPNOTSUPP;
 	if (copy_from_user(&c, arg, sizeof(c)))
@@ -410,6 +436,7 @@ static int blkdev_pr_clear(struct block_device *bdev,
  */
 static inline int is_unrecognized_ioctl(int ret)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return	ret == -EINVAL ||
 		ret == -ENOTTY ||
 		ret == -ENOIOCTLCMD;
@@ -421,7 +448,9 @@ static int blkdev_flushbuf(struct block_device *bdev, fmode_t mode,
 	int ret;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EACCES;
+}
 
 	ret = __blkdev_driver_ioctl(bdev, mode, cmd, arg);
 	if (!is_unrecognized_ioctl(ret))
@@ -439,7 +468,9 @@ static int blkdev_roset(struct block_device *bdev, fmode_t mode,
 
 	ret = __blkdev_driver_ioctl(bdev, mode, cmd, arg);
 	if (!is_unrecognized_ioctl(ret))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 	if (get_user(n, (int __user *)arg))
@@ -456,7 +487,9 @@ static int blkdev_getgeo(struct block_device *bdev,
 	int ret;
 
 	if (!argp)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 	if (!disk->fops->getgeo)
 		return -ENOTTY;
 
@@ -481,7 +514,9 @@ static int blkdev_bszset(struct block_device *bdev, fmode_t mode,
 	int ret, n;
 
 	if (!capable(CAP_SYS_ADMIN))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EACCES;
+}
 	if (!argp)
 		return -EINVAL;
 	if (get_user(n, argp))
@@ -530,7 +565,10 @@ int blkdev_ioctl(struct block_device *bdev, fmode_t mode, unsigned cmd,
 	case BLKRAGET:
 	case BLKFRAGET:
 		if (!arg)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EINVAL;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return put_long(arg, (bdev->bd_bdi->ra_pages*PAGE_SIZE) / 512);
 	case BLKROGET:
 		return put_int(arg, bdev_read_only(bdev) != 0);
@@ -558,6 +596,7 @@ int blkdev_ioctl(struct block_device *bdev, fmode_t mode, unsigned cmd,
 	case BLKFRASET:
 		if(!capable(CAP_SYS_ADMIN))
 			return -EACCES;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		bdev->bd_bdi->ra_pages = (arg * 512) / PAGE_SIZE;
 		return 0;
 	case BLKBSZSET:
@@ -569,7 +608,10 @@ int blkdev_ioctl(struct block_device *bdev, fmode_t mode, unsigned cmd,
 	case BLKGETSIZE:
 		size = i_size_read(bdev->bd_inode);
 		if ((size >> 9) > ~0UL)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			return -EFBIG;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return put_ulong(arg, size >> 9);
 	case BLKGETSIZE64:
 		return put_u64(arg, i_size_read(bdev->bd_inode));
diff --git a/block/kyber-iosched.c b/block/kyber-iosched.c
index f58cab8..962159a 100644
--- a/block/kyber-iosched.c
+++ b/block/kyber-iosched.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * The Kyber I/O scheduler. Controls latency by throttling queue depths using
  * scalable techniques.
@@ -108,7 +110,9 @@ static int rq_sched_domain(const struct request *rq)
 	unsigned int op = rq->cmd_flags;
 
 	if ((op & REQ_OP_MASK) == REQ_OP_READ)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return KYBER_READ;
+}
 	else if ((op & REQ_OP_MASK) == REQ_OP_WRITE && op_is_sync(op))
 		return KYBER_SYNC_WRITE;
 	else
@@ -132,7 +136,9 @@ static int kyber_lat_status(struct blk_stat_callback *cb,
 	u64 latency;
 
 	if (!cb->stat[sched_domain].nr_samples)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NONE;
+}
 
 	latency = cb->stat[sched_domain].mean;
 	if (latency >= 2 * target)
@@ -214,6 +220,7 @@ static void kyber_adjust_other_depth(struct kyber_queue_data *kqd,
 
 	orig_depth = depth = kqd->domain_tokens[KYBER_OTHER].sb.depth;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (read_status == NONE && write_status == NONE) {
 		depth += 2;
 	} else if (have_samples) {
@@ -291,6 +298,7 @@ static struct kyber_queue_data *kyber_queue_data_alloc(struct request_queue *q)
 	kqd = kmalloc_node(sizeof(*kqd), GFP_KERNEL, q->node);
 	if (!kqd)
 		goto err;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kqd->q = q;
 
 	kqd->cb = blk_stat_alloc_callback(kyber_stat_timer_fn, rq_sched_domain,
@@ -342,7 +350,9 @@ static int kyber_init_sched(struct request_queue *q, struct elevator_type *e)
 
 	eq = elevator_alloc(q, e);
 	if (!eq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	kqd = kyber_queue_data_alloc(q);
 	if (IS_ERR(kqd)) {
@@ -366,6 +376,7 @@ static void kyber_exit_sched(struct elevator_queue *e)
 
 	blk_stat_remove_callback(q, kqd->cb);
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < KYBER_NUM_DOMAINS; i++)
 		sbitmap_queue_free(&kqd->domain_tokens[i]);
 	blk_stat_free_callback(kqd->cb);
@@ -379,7 +390,9 @@ static int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 
 	khd = kmalloc_node(sizeof(*khd), GFP_KERNEL, hctx->numa_node);
 	if (!khd)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	spin_lock_init(&khd->lock);
 
@@ -399,16 +412,19 @@ static int kyber_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 
 static void kyber_exit_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kfree(hctx->sched_data);
 }
 
 static int rq_get_domain_token(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (long)rq->elv.priv[0];
 }
 
 static void rq_set_domain_token(struct request *rq, int token)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rq->elv.priv[0] = (void *)(long)token;
 }
 
@@ -420,6 +436,7 @@ static void rq_clear_domain_token(struct kyber_queue_data *kqd,
 
 	nr = rq_get_domain_token(rq);
 	if (nr != -1) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sched_domain = rq_sched_domain(rq);
 		sbitmap_queue_clear(&kqd->domain_tokens[sched_domain], nr,
 				    rq->mq_ctx->cpu);
@@ -441,6 +458,7 @@ static void kyber_limit_depth(unsigned int op, struct blk_mq_alloc_data *data)
 
 static void kyber_prepare_request(struct request *rq, struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rq_set_domain_token(rq, -1);
 }
 
@@ -495,6 +513,7 @@ static void kyber_flush_busy_ctxs(struct kyber_hctx_data *khd,
 	struct request *rq, *next;
 
 	blk_mq_flush_busy_ctxs(hctx, &rq_list);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_for_each_entry_safe(rq, next, &rq_list, queuelist) {
 		unsigned int sched_domain;
 
@@ -506,6 +525,7 @@ static void kyber_flush_busy_ctxs(struct kyber_hctx_data *khd,
 static int kyber_domain_wake(wait_queue_entry_t *wait, unsigned mode, int flags,
 			     void *key)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct blk_mq_hw_ctx *hctx = READ_ONCE(wait->private);
 
 	list_del_init(&wait->entry);
@@ -525,7 +545,9 @@ static int kyber_get_domain_token(struct kyber_queue_data *kqd,
 
 	nr = __sbitmap_queue_get(domain_tokens);
 	if (nr >= 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return nr;
+}
 
 	/*
 	 * If we failed to get a domain token, make sure the hardware queue is
@@ -559,6 +581,7 @@ kyber_dispatch_cur_domain(struct kyber_queue_data *kqd,
 	int nr;
 
 	rqs = &khd->rqs[khd->cur_domain];
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rq = list_first_entry_or_null(rqs, struct request, queuelist);
 
 	/*
@@ -600,6 +623,7 @@ static struct request *kyber_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	 * from the batch.
 	 */
 	if (khd->batching < kyber_batch_size[khd->cur_domain]) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rq = kyber_dispatch_cur_domain(kqd, khd, hctx, &flushed);
 		if (rq)
 			goto out;
@@ -637,6 +661,7 @@ static bool kyber_has_work(struct blk_mq_hw_ctx *hctx)
 	struct kyber_hctx_data *khd = hctx->sched_data;
 	int i;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < KYBER_NUM_DOMAINS; i++) {
 		if (!list_empty_careful(&khd->rqs[i]))
 			return true;
@@ -835,6 +860,7 @@ static int __init kyber_init(void)
 
 static void __exit kyber_exit(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	elv_unregister(&kyber_sched);
 }
 
diff --git a/block/mq-deadline.c b/block/mq-deadline.c
index a1cad43..1ec0af8 100644
--- a/block/mq-deadline.c
+++ b/block/mq-deadline.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  *  MQ Deadline i/o scheduler - adaptation of the legacy deadline scheduler,
  *  for the blk-mq scheduling framework
@@ -65,6 +67,7 @@ struct deadline_data {
 static inline struct rb_root *
 deadline_rb_root(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return &dd->sort_list[rq_data_dir(rq)];
 }
 
@@ -74,6 +77,7 @@ deadline_rb_root(struct deadline_data *dd, struct request *rq)
 static inline struct request *
 deadline_latter_request(struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct rb_node *node = rb_next(&rq->rb_node);
 
 	if (node)
@@ -85,6 +89,7 @@ deadline_latter_request(struct request *rq)
 static void
 deadline_add_rq_rb(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct rb_root *root = deadline_rb_root(dd, rq);
 
 	elv_rb_add(root, rq);
@@ -93,6 +98,7 @@ deadline_add_rq_rb(struct deadline_data *dd, struct request *rq)
 static inline void
 deadline_del_rq_rb(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const int data_dir = rq_data_dir(rq);
 
 	if (dd->next_rq[data_dir] == rq)
@@ -114,7 +120,9 @@ static void deadline_remove_request(struct request_queue *q, struct request *rq)
 	 * We might not be on the rbtree, if we are doing an insert merge
 	 */
 	if (!RB_EMPTY_NODE(&rq->rb_node))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		deadline_del_rq_rb(dd, rq);
+}
 
 	elv_rqhash_del(q, rq);
 	if (q->last_merge == rq)
@@ -130,6 +138,7 @@ static void dd_request_merged(struct request_queue *q, struct request *req,
 	 * if the merge was a front merge, we need to reposition request
 	 */
 	if (type == ELEVATOR_FRONT_MERGE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		elv_rb_del(deadline_rb_root(dd, req), req);
 		deadline_add_rq_rb(dd, req);
 	}
@@ -162,6 +171,7 @@ static void dd_merged_requests(struct request_queue *q, struct request *req,
 static void
 deadline_move_request(struct deadline_data *dd, struct request *rq)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const int data_dir = rq_data_dir(rq);
 
 	dd->next_rq[READ] = NULL;
@@ -180,6 +190,7 @@ deadline_move_request(struct deadline_data *dd, struct request *rq)
  */
 static inline int deadline_check_fifo(struct deadline_data *dd, int ddir)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct request *rq = rq_entry_fifo(dd->fifo_list[ddir].next);
 
 	/*
@@ -203,6 +214,7 @@ static struct request *__dd_dispatch_request(struct blk_mq_hw_ctx *hctx)
 	int data_dir;
 
 	if (!list_empty(&dd->dispatch)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		rq = list_first_entry(&dd->dispatch, struct request, queuelist);
 		list_del_init(&rq->queuelist);
 		goto done;
@@ -304,6 +316,7 @@ static void dd_exit_queue(struct elevator_queue *e)
 {
 	struct deadline_data *dd = e->elevator_data;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!list_empty(&dd->fifo_list[READ]));
 	BUG_ON(!list_empty(&dd->fifo_list[WRITE]));
 
@@ -320,10 +333,13 @@ static int dd_init_queue(struct request_queue *q, struct elevator_type *e)
 
 	eq = elevator_alloc(q, e);
 	if (!eq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	dd = kzalloc_node(sizeof(*dd), GFP_KERNEL, q->node);
 	if (!dd) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kobject_put(&eq->kobj);
 		return -ENOMEM;
 	}
@@ -353,7 +369,9 @@ static int dd_request_merge(struct request_queue *q, struct request **rq,
 	struct request *__rq;
 
 	if (!dd->front_merges)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ELEVATOR_NO_MERGE;
+}
 
 	__rq = elv_rb_find(&dd->sort_list[bio_data_dir(bio)], sector);
 	if (__rq) {
@@ -380,7 +398,9 @@ static bool dd_bio_merge(struct blk_mq_hw_ctx *hctx, struct bio *bio)
 	spin_unlock(&dd->lock);
 
 	if (free)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		blk_mq_free_request(free);
+}
 
 	return ret;
 }
@@ -393,6 +413,7 @@ static void dd_insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
 {
 	struct request_queue *q = hctx->queue;
 	struct deadline_data *dd = q->elevator->elevator_data;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	const int data_dir = rq_data_dir(rq);
 
 	if (blk_mq_sched_try_insert_merge(q, rq))
@@ -429,6 +450,7 @@ static void dd_insert_requests(struct blk_mq_hw_ctx *hctx,
 	struct deadline_data *dd = q->elevator->elevator_data;
 
 	spin_lock(&dd->lock);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (!list_empty(list)) {
 		struct request *rq;
 
@@ -443,6 +465,7 @@ static bool dd_has_work(struct blk_mq_hw_ctx *hctx)
 {
 	struct deadline_data *dd = hctx->queue->elevator->elevator_data;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return !list_empty_careful(&dd->dispatch) ||
 		!list_empty_careful(&dd->fifo_list[0]) ||
 		!list_empty_careful(&dd->fifo_list[1]);
@@ -454,6 +477,7 @@ static bool dd_has_work(struct blk_mq_hw_ctx *hctx)
 static ssize_t
 deadline_var_show(int var, char *page)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return sprintf(page, "%d\n", var);
 }
 
@@ -668,6 +692,7 @@ static int __init deadline_init(void)
 
 static void __exit deadline_exit(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	elv_unregister(&mq_deadline);
 }
 
diff --git a/block/noop-iosched.c b/block/noop-iosched.c
index 2d1b15d..05acf7e 100644
--- a/block/noop-iosched.c
+++ b/block/noop-iosched.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * elevator noop
  */
@@ -15,6 +17,7 @@ struct noop_data {
 static void noop_merged_requests(struct request_queue *q, struct request *rq,
 				 struct request *next)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	list_del_init(&next->queuelist);
 }
 
@@ -23,6 +26,7 @@ static int noop_dispatch(struct request_queue *q, int force)
 	struct noop_data *nd = q->elevator->elevator_data;
 	struct request *rq;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	rq = list_first_entry_or_null(&nd->queue, struct request, queuelist);
 	if (rq) {
 		list_del_init(&rq->queuelist);
@@ -45,7 +49,9 @@ noop_former_request(struct request_queue *q, struct request *rq)
 	struct noop_data *nd = q->elevator->elevator_data;
 
 	if (rq->queuelist.prev == &nd->queue)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	return list_prev_entry(rq, queuelist);
 }
 
@@ -55,7 +61,9 @@ noop_latter_request(struct request_queue *q, struct request *rq)
 	struct noop_data *nd = q->elevator->elevator_data;
 
 	if (rq->queuelist.next == &nd->queue)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	return list_next_entry(rq, queuelist);
 }
 
@@ -66,7 +74,9 @@ static int noop_init_queue(struct request_queue *q, struct elevator_type *e)
 
 	eq = elevator_alloc(q, e);
 	if (!eq)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENOMEM;
+}
 
 	nd = kmalloc_node(sizeof(*nd), GFP_KERNEL, q->node);
 	if (!nd) {
@@ -87,6 +97,7 @@ static void noop_exit_queue(struct elevator_queue *e)
 {
 	struct noop_data *nd = e->elevator_data;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	BUG_ON(!list_empty(&nd->queue));
 	kfree(nd);
 }
@@ -112,6 +123,7 @@ static int __init noop_init(void)
 
 static void __exit noop_exit(void)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	elv_unregister(&elevator_noop);
 }
 
diff --git a/block/partition-generic.c b/block/partition-generic.c
index 91622db..d512224 100644
--- a/block/partition-generic.c
+++ b/block/partition-generic.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  Code extracted from drivers/block/genhd.c
@@ -36,8 +38,11 @@ char *disk_name(struct gendisk *hd, int partno, char *buf)
 {
 	if (!partno)
 		snprintf(buf, BDEVNAME_SIZE, "%s", hd->disk_name);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	else if (isdigit(hd->disk_name[strlen(hd->disk_name)-1]))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		snprintf(buf, BDEVNAME_SIZE, "%sp%d", hd->disk_name, partno);
+}
 	else
 		snprintf(buf, BDEVNAME_SIZE, "%s%d", hd->disk_name, partno);
 
@@ -46,6 +51,7 @@ char *disk_name(struct gendisk *hd, int partno, char *buf)
 
 const char *bdevname(struct block_device *bdev, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return disk_name(bdev->bd_disk, bdev->bd_part->partno, buf);
 }
 
@@ -58,6 +64,7 @@ EXPORT_SYMBOL(bdevname);
  */
 const char *__bdevname(dev_t dev, char *buffer)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	scnprintf(buffer, BDEVNAME_SIZE, "unknown-block(%u,%u)",
 				MAJOR(dev), MINOR(dev));
 	return buffer;
@@ -68,6 +75,7 @@ EXPORT_SYMBOL(__bdevname);
 static ssize_t part_partition_show(struct device *dev,
 				   struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 
 	return sprintf(buf, "%d\n", p->partno);
@@ -76,6 +84,7 @@ static ssize_t part_partition_show(struct device *dev,
 static ssize_t part_start_show(struct device *dev,
 			       struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 
 	return sprintf(buf, "%llu\n",(unsigned long long)p->start_sect);
@@ -84,6 +93,7 @@ static ssize_t part_start_show(struct device *dev,
 ssize_t part_size_show(struct device *dev,
 		       struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 	return sprintf(buf, "%llu\n",(unsigned long long)part_nr_sects_read(p));
 }
@@ -91,6 +101,7 @@ ssize_t part_size_show(struct device *dev,
 static ssize_t part_ro_show(struct device *dev,
 			    struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 	return sprintf(buf, "%d\n", p->policy ? 1 : 0);
 }
@@ -98,6 +109,7 @@ static ssize_t part_ro_show(struct device *dev,
 static ssize_t part_alignment_offset_show(struct device *dev,
 					  struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 	return sprintf(buf, "%llu\n", (unsigned long long)p->alignment_offset);
 }
@@ -105,6 +117,7 @@ static ssize_t part_alignment_offset_show(struct device *dev,
 static ssize_t part_discard_alignment_show(struct device *dev,
 					   struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 	return sprintf(buf, "%u\n", p->discard_alignment);
 }
@@ -112,6 +125,7 @@ static ssize_t part_discard_alignment_show(struct device *dev,
 ssize_t part_stat_show(struct device *dev,
 		       struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 	struct request_queue *q = part_to_disk(p)->queue;
 	unsigned int inflight[2];
@@ -142,6 +156,7 @@ ssize_t part_stat_show(struct device *dev,
 ssize_t part_inflight_show(struct device *dev,
 			struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 
 	return sprintf(buf, "%8u %8u\n", atomic_read(&p->in_flight[0]),
@@ -214,6 +229,7 @@ static const struct attribute_group *part_attr_groups[] = {
 
 static void part_release(struct device *dev)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *p = dev_to_part(dev);
 	blk_free_devt(dev->devt);
 	hd_free_part(p);
@@ -222,6 +238,7 @@ static void part_release(struct device *dev)
 
 static int part_uevent(struct device *dev, struct kobj_uevent_env *env)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *part = dev_to_part(dev);
 
 	add_uevent_var(env, "PARTN=%u", part->partno);
@@ -239,6 +256,7 @@ struct device_type part_type = {
 
 static void delete_partition_rcu_cb(struct rcu_head *head)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *part = container_of(head, struct hd_struct, rcu_head);
 
 	part->start_sect = 0;
@@ -249,6 +267,7 @@ static void delete_partition_rcu_cb(struct rcu_head *head)
 
 void __delete_partition(struct percpu_ref *ref)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct hd_struct *part = container_of(ref, struct hd_struct, ref);
 	call_rcu(&part->rcu_head, delete_partition_rcu_cb);
 }
@@ -281,6 +300,7 @@ void delete_partition(struct gendisk *disk, int partno)
 static ssize_t whole_disk_show(struct device *dev,
 			       struct device_attribute *attr, char *buf)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return 0;
 }
 static DEVICE_ATTR(whole_disk, S_IRUSR | S_IRGRP | S_IROTH,
@@ -304,7 +324,9 @@ struct hd_struct *add_partition(struct gendisk *disk, int partno,
 
 	err = disk_expand_part_tbl(disk, partno);
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ERR_PTR(err);
+}
 	ptbl = rcu_dereference_protected(disk->part_tbl, 1);
 
 	if (ptbl->part[partno])
@@ -430,10 +452,14 @@ static int drop_partitions(struct gendisk *disk, struct block_device *bdev)
 	int res;
 
 	if (bdev->bd_part_count || bdev->bd_super)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EBUSY;
+}
 	res = invalidate_partition(disk, 0);
 	if (res)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return res;
+}
 
 	disk_part_iter_init(&piter, disk, DISK_PITER_INCL_EMPTY);
 	while ((part = disk_part_iter_next(&piter)))
@@ -447,6 +473,7 @@ static bool part_zone_aligned(struct gendisk *disk,
 			      struct block_device *bdev,
 			      sector_t from, sector_t size)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned int zone_sectors = bdev_zone_sectors(bdev);
 
 	/*
@@ -500,20 +527,28 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 	int p, highest, res;
 rescan:
 	if (state && !IS_ERR(state)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		free_partitions(state);
 		state = NULL;
 	}
 
 	res = drop_partitions(disk, bdev);
 	if (res)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return res;
+}
 
 	if (disk->fops->revalidate_disk)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		disk->fops->revalidate_disk(disk);
+}
 	check_disk_size_change(disk, bdev);
 	bdev->bd_invalidated = 0;
 	if (!get_capacity(disk) || !(state = check_partition(disk, bdev)))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (IS_ERR(state)) {
 		/*
 		 * I/O error reading the partition table.  If any
@@ -521,11 +556,13 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 		 * after unlocking native capacity.
 		 */
 		if (PTR_ERR(state) == -ENOSPC) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			printk(KERN_WARNING "%s: partition table beyond EOD, ",
 			       disk->disk_name);
 			if (disk_unlock_native_capacity(disk))
 				goto rescan;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EIO;
 	}
 	/*
@@ -534,6 +571,7 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 	 * successfully read as we could be missing some partitions.
 	 */
 	if (state->access_beyond_eod) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		printk(KERN_WARNING
 		       "%s: partition table partially beyond EOD, ",
 		       disk->disk_name);
@@ -549,9 +587,14 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 	 * necessary.
 	 */
 	for (p = 1, highest = 0; p < state->limit; p++)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (state->parts[p].size)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			highest = p;
+}
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	disk_expand_part_tbl(disk, highest);
 
 	/* add partitions */
@@ -562,8 +605,10 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 		if (!size)
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		from = state->parts[p].from;
 		if (from >= get_capacity(disk)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			printk(KERN_WARNING
 			       "%s: p%d start %llu is beyond EOD, ",
 			       disk->disk_name, p, (unsigned long long) from);
@@ -572,7 +617,9 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 			continue;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (from + size > get_capacity(disk)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			printk(KERN_WARNING
 			       "%s: p%d size %llu extends beyond EOD, ",
 			       disk->disk_name, p, (unsigned long long) size);
@@ -599,6 +646,7 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 		 */
 		if (bdev_is_zoned(bdev) &&
 		    !part_zone_aligned(disk, bdev, from, size)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			printk(KERN_WARNING
 			       "%s: p%d start %llu+%llu is not zone aligned\n",
 			       disk->disk_name, p, (unsigned long long) from,
@@ -606,6 +654,7 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 			continue;
 		}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		part = add_partition(disk, p, from, size,
 				     state->parts[p].flags,
 				     &state->parts[p].info);
@@ -616,9 +665,12 @@ int rescan_partitions(struct gendisk *disk, struct block_device *bdev)
 		}
 #ifdef CONFIG_BLK_DEV_MD
 		if (state->parts[p].flags & ADDPART_FLAG_RAID)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			md_autodetect_dev(part_to_dev(part)->devt);
+}
 #endif
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	free_partitions(state);
 	return 0;
 }
@@ -628,7 +680,9 @@ int invalidate_partitions(struct gendisk *disk, struct block_device *bdev)
 	int res;
 
 	if (!bdev->bd_invalidated)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	res = drop_partitions(disk, bdev);
 	if (res)
@@ -657,6 +711,7 @@ unsigned char *read_dev_sector(struct block_device *bdev, sector_t n, Sector *p)
 fail:
 		put_page(page);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	p->v = NULL;
 	return NULL;
 }
diff --git a/block/partitions/check.c b/block/partitions/check.c
index 720145c..eb392d02 100644
--- a/block/partitions/check.c
+++ b/block/partitions/check.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  fs/partitions/check.c
@@ -119,11 +121,15 @@ static struct parsed_partitions *allocate_partitions(struct gendisk *hd)
 
 	state = kzalloc(sizeof(*state), GFP_KERNEL);
 	if (!state)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	nr = disk_max_parts(hd);
 	state->parts = vzalloc(nr * sizeof(state->parts[0]));
 	if (!state->parts) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		kfree(state);
 		return NULL;
 	}
@@ -147,9 +153,12 @@ check_partition(struct gendisk *hd, struct block_device *bdev)
 
 	state = allocate_partitions(hd);
 	if (!state)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 	state->pp_buf = (char *)__get_free_page(GFP_KERNEL);
 	if (!state->pp_buf) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		free_partitions(state);
 		return NULL;
 	}
@@ -161,6 +170,7 @@ check_partition(struct gendisk *hd, struct block_device *bdev)
 	if (isdigit(state->name[strlen(state->name)-1]))
 		sprintf(state->name, "p");
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	i = res = err = 0;
 	while (!res && check_part[i]) {
 		memset(state->parts, 0, state->limit * sizeof(state->parts[0]));
@@ -175,20 +185,26 @@ check_partition(struct gendisk *hd, struct block_device *bdev)
 
 	}
 	if (res > 0) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		printk(KERN_INFO "%s", state->pp_buf);
 
 		free_page((unsigned long)state->pp_buf);
 		return state;
 	}
 	if (state->access_beyond_eod)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		err = -ENOSPC;
+}
 	if (err)
 	/* The partition is unrecognized. So report I/O errors if there were any */
 		res = err;
 	if (res) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (warn_no_part)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			strlcat(state->pp_buf,
 				" unable to read partition table\n", PAGE_SIZE);
+}
 		printk(KERN_INFO "%s", state->pp_buf);
 	}
 
diff --git a/block/partitions/check.h b/block/partitions/check.h
index 6042f76..44004f2 100644
--- a/block/partitions/check.h
+++ b/block/partitions/check.h
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /* SPDX-License-Identifier: GPL-2.0 */
 #include <linux/pagemap.h>
 #include <linux/blkdev.h>
diff --git a/block/partitions/cmdline.c b/block/partitions/cmdline.c
index e333583..f69adc1 100644
--- a/block/partitions/cmdline.c
+++ b/block/partitions/cmdline.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2013 HUAWEI
diff --git a/block/partitions/efi.c b/block/partitions/efi.c
index 39f70d9..bd2fff6 100644
--- a/block/partitions/efi.c
+++ b/block/partitions/efi.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /************************************************************
  * EFI GUID Partition Table handling
  *
@@ -112,6 +114,7 @@ static int force_gpt;
 static int __init
 force_gpt_fn(char *str)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	force_gpt = 1;
 	return 1;
 }
@@ -133,6 +136,7 @@ __setup("gpt", force_gpt_fn);
 static inline u32
 efi_crc32(const void *buf, unsigned long len)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (crc32(~0L, buf, len) ^ ~0L);
 }
 
@@ -148,13 +152,16 @@ efi_crc32(const void *buf, unsigned long len)
 static u64 last_lba(struct block_device *bdev)
 {
 	if (!bdev || !bdev->bd_inode)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	return div_u64(bdev->bd_inode->i_size,
 		       bdev_logical_block_size(bdev)) - 1ULL;
 }
 
 static inline int pmbr_part_valid(gpt_mbr_record *part)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (part->os_type != EFI_PMBR_OSTYPE_EFI_GPT)
 		goto invalid;
 
@@ -193,9 +200,12 @@ static int is_pmbr_valid(legacy_mbr *mbr, sector_t total_sectors)
 	if (!mbr || le16_to_cpu(mbr->signature) != MSDOS_MBR_SIGNATURE)
 		goto done;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < 4; i++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		ret = pmbr_part_valid(&mbr->partition_record[i]);
 		if (ret == GPT_MBR_PROTECTIVE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			part = i;
 			/*
 			 * Ok, we at least know that there's a protective MBR,
@@ -206,14 +216,17 @@ static int is_pmbr_valid(legacy_mbr *mbr, sector_t total_sectors)
 		}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (ret != GPT_MBR_PROTECTIVE)
 		goto done;
 check_hybrid:
 	for (i = 0; i < 4; i++)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if ((mbr->partition_record[i].os_type !=
 			EFI_PMBR_OSTYPE_EFI_GPT) &&
 		    (mbr->partition_record[i].os_type != 0x00))
 			ret = GPT_MBR_HYBRID;
+}
 
 	/*
 	 * Protective MBRs take up the lesser of the whole disk
@@ -228,11 +241,15 @@ static int is_pmbr_valid(legacy_mbr *mbr, sector_t total_sectors)
 	 * an image from a smaller disk to a larger disk.
 	 */
 	if (ret == GPT_MBR_PROTECTIVE) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sz = le32_to_cpu(mbr->partition_record[part].size_in_lba);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (sz != (uint32_t) total_sectors - 1 && sz != 0xFFFFFFFF)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			pr_debug("GPT: mbr size in lba (%u) different than whole disk (%u).\n",
 				 sz, min_t(uint32_t,
 					   total_sectors - 1, 0xFFFFFFFF));
+}
 	}
 done:
 	return ret;
@@ -256,7 +273,9 @@ static size_t read_lba(struct parsed_partitions *state,
 	sector_t n = lba * (bdev_logical_block_size(bdev) / 512);
 
 	if (!buffer || lba > last_lba(bdev))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
                 return 0;
+}
 
 	while (count) {
 		int copied = 512;
@@ -265,13 +284,16 @@ static size_t read_lba(struct parsed_partitions *state,
 		if (!data)
 			break;
 		if (copied > count)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			copied = count;
+}
 		memcpy(buffer, data, copied);
 		put_dev_sector(sect);
 		buffer += copied;
 		totalreadcount +=copied;
 		count -= copied;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return totalreadcount;
 }
 
@@ -291,7 +313,9 @@ static gpt_entry *alloc_read_gpt_entries(struct parsed_partitions *state,
 	gpt_entry *pte;
 
 	if (!gpt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	count = (size_t)le32_to_cpu(gpt->num_partition_entries) *
                 le32_to_cpu(gpt->sizeof_partition_entry);
@@ -327,7 +351,9 @@ static gpt_header *alloc_read_gpt_header(struct parsed_partitions *state,
 
 	gpt = kmalloc(ssz, GFP_KERNEL);
 	if (!gpt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return NULL;
+}
 
 	if (read_lba(state, lba, (u8 *) gpt, ssz) < ssz) {
 		kfree(gpt);
@@ -355,7 +381,9 @@ static int is_gpt_valid(struct parsed_partitions *state, u64 lba,
 	u64 lastlba, pt_size;
 
 	if (!ptes)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 	if (!(*gpt = alloc_read_gpt_header(state, lba)))
 		return 0;
 
@@ -476,6 +504,7 @@ static int is_gpt_valid(struct parsed_partitions *state, u64 lba,
 static inline int
 is_pte_valid(const gpt_entry *pte, const u64 lastlba)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if ((!efi_guidcmp(pte->partition_type_guid, NULL_GUID)) ||
 	    le64_to_cpu(pte->starting_lba) > lastlba         ||
 	    le64_to_cpu(pte->ending_lba)   > lastlba)
@@ -497,6 +526,7 @@ static void
 compare_gpts(gpt_header *pgpt, gpt_header *agpt, u64 lastlba)
 {
 	int error_found = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (!pgpt || !agpt)
 		return;
 	if (le64_to_cpu(pgpt->my_lba) != le64_to_cpu(agpt->alternate_lba)) {
@@ -605,7 +635,9 @@ static int find_valid_gpt(struct parsed_partitions *state, gpt_header **gpt,
 	u64 lastlba;
 
 	if (!ptes)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	lastlba = last_lba(state->bdev);
         if (!force_gpt) {
@@ -621,37 +653,50 @@ static int find_valid_gpt(struct parsed_partitions *state, gpt_header **gpt,
 		if (!good_pmbr)
 			goto fail;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		pr_debug("Device has a %s MBR\n",
 			 good_pmbr == GPT_MBR_PROTECTIVE ?
 						"protective" : "hybrid");
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	good_pgpt = is_gpt_valid(state, GPT_PRIMARY_PARTITION_TABLE_LBA,
 				 &pgpt, &pptes);
         if (good_pgpt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		good_agpt = is_gpt_valid(state,
 					 le64_to_cpu(pgpt->alternate_lba),
 					 &agpt, &aptes);
+}
         if (!good_agpt && force_gpt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
                 good_agpt = is_gpt_valid(state, lastlba, &agpt, &aptes);
+}
 
         /* The obviously unsuccessful case */
         if (!good_pgpt && !good_agpt)
                 goto fail;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
         compare_gpts(pgpt, agpt, lastlba);
 
         /* The good cases */
         if (good_pgpt) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
                 *gpt  = pgpt;
                 *ptes = pptes;
                 kfree(agpt);
                 kfree(aptes);
 		if (!good_agpt)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
                         pr_warn("Alternate GPT is invalid, using primary GPT.\n");
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
                 return 1;
         }
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
         else if (good_agpt) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
                 *gpt  = agpt;
                 *ptes = aptes;
                 kfree(pgpt);
@@ -702,8 +747,10 @@ int efi_partition(struct parsed_partitions *state)
 		return 0;
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	pr_debug("GUID Partition Table is valid!  Yea!\n");
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (i = 0; i < le32_to_cpu(gpt->num_partition_entries) && i < state->limit-1; i++) {
 		struct partition_meta_info *info;
 		unsigned label_count = 0;
@@ -715,12 +762,16 @@ int efi_partition(struct parsed_partitions *state)
 		if (!is_pte_valid(&ptes[i], last_lba(state->bdev)))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_partition(state, i+1, start * ssz, size * ssz);
 
 		/* If this is a RAID volume, tell md */
 		if (!efi_guidcmp(ptes[i].partition_type_guid, PARTITION_LINUX_RAID_GUID))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			state->parts[i + 1].flags = ADDPART_FLAG_RAID;
+}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		info = &state->parts[i + 1].info;
 		efi_guid_to_str(&ptes[i].unique_partition_guid, info->uuid);
 
@@ -728,15 +779,22 @@ int efi_partition(struct parsed_partitions *state)
 		label_max = min(ARRAY_SIZE(info->volname) - 1,
 				ARRAY_SIZE(ptes[i].partition_name));
 		info->volname[label_max] = 0;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		while (label_count < label_max) {
 			u8 c = ptes[i].partition_name[label_count] & 0xff;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (c && !isprint(c))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				c = '!';
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			info->volname[label_count] = c;
 			label_count++;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		state->parts[i + 1].has_info = true;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	kfree(ptes);
 	kfree(gpt);
 	strlcat(state->pp_buf, "\n", PAGE_SIZE);
diff --git a/block/partitions/ldm.c b/block/partitions/ldm.c
index 2a365c7..cacc994 100644
--- a/block/partitions/ldm.c
+++ b/block/partitions/ldm.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /**
  * ldm - Support for Windows Logical Disk Manager (Dynamic Disks)
  *
diff --git a/block/partitions/msdos.c b/block/partitions/msdos.c
index 0af3a3d..90e97cd 100644
--- a/block/partitions/msdos.c
+++ b/block/partitions/msdos.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 // SPDX-License-Identifier: GPL-2.0
 /*
  *  fs/partitions/msdos.c
@@ -37,16 +39,19 @@
 
 static inline sector_t nr_sects(struct partition *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (sector_t)get_unaligned_le32(&p->nr_sects);
 }
 
 static inline sector_t start_sect(struct partition *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (sector_t)get_unaligned_le32(&p->start_sect);
 }
 
 static inline int is_extended_partition(struct partition *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return (SYS_IND(p) == DOS_EXTENDED_PARTITION ||
 		SYS_IND(p) == WIN98_EXTENDED_PARTITION ||
 		SYS_IND(p) == LINUX_EXTENDED_PARTITION);
@@ -80,6 +85,7 @@ static int aix_magic_present(struct parsed_partitions *state, unsigned char *p)
 		return 0;
 	/* Assume the partition table is valid if Linux partitions exists */
 	for (slot = 1; slot <= 4; slot++, pt++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (pt->sys_ind == LINUX_SWAP_PARTITION ||
 			pt->sys_ind == LINUX_RAID_PARTITION ||
 			pt->sys_ind == LINUX_DATA_PARTITION ||
@@ -87,12 +93,18 @@ static int aix_magic_present(struct parsed_partitions *state, unsigned char *p)
 			is_extended_partition(pt))
 			return 0;
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	d = read_part_sector(state, 7, &sect);
 	if (d) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (d[0] == '_' && d[1] == 'L' && d[2] == 'V' && d[3] == 'M')
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			ret = 1;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_dev_sector(sect);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return ret;
 }
 
@@ -134,6 +146,7 @@ static void parse_extended(struct parsed_partitions *state,
 	this_sector = first_sector;
 	this_size = first_size;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	while (1) {
 		if (++loopct > 100)
 			return;
@@ -224,7 +237,9 @@ static void parse_solaris_x86(struct parsed_partitions *state,
 
 	v = read_part_sector(state, offset + 1, &sect);
 	if (!v)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (le32_to_cpu(v->v_sanity) != SOLARIS_X86_VTOC_SANE) {
 		put_dev_sector(sect);
 		return;
@@ -281,7 +296,9 @@ static void parse_bsd(struct parsed_partitions *state,
 
 	l = read_part_sector(state, offset + 1, &sect);
 	if (!l)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (le32_to_cpu(l->d_magic) != BSD_DISKMAGIC) {
 		put_dev_sector(sect);
 		return;
@@ -361,7 +378,9 @@ static void parse_unixware(struct parsed_partitions *state,
 
 	l = read_part_sector(state, offset + 29, &sect);
 	if (!l)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 	if (le32_to_cpu(l->d_magic) != UNIXWARE_DISKMAGIC ||
 	    le32_to_cpu(l->vtoc.v_magic) != UNIXWARE_DISKMAGIC2) {
 		put_dev_sector(sect);
@@ -406,7 +425,9 @@ static void parse_minix(struct parsed_partitions *state,
 
 	data = read_part_sector(state, offset, &sect);
 	if (!data)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return;
+}
 
 	p = (struct partition *)(data + 0x1be);
 
@@ -459,13 +480,16 @@ int msdos_partition(struct parsed_partitions *state)
 
 	data = read_part_sector(state, 0, &sect);
 	if (!data)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -1;
+}
 
 	/*
 	 * Note order! (some AIX disks, e.g. unbootable kind,
 	 * have no MSDOS 55aa)
 	 */
 	if (aix_magic_present(state, data)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_dev_sector(sect);
 #ifdef CONFIG_AIX_PARTITION
 		return aix_partition(state);
@@ -476,6 +500,7 @@ int msdos_partition(struct parsed_partitions *state)
 	}
 
 	if (!msdos_magic_present(data + 510)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_dev_sector(sect);
 		return 0;
 	}
@@ -487,7 +512,9 @@ int msdos_partition(struct parsed_partitions *state)
 	 * is not 0 or 0x80.
 	 */
 	p = (struct partition *) (data + 0x1be);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (slot = 1; slot <= 4; slot++, p++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (p->boot_ind != 0 && p->boot_ind != 0x80) {
 			/*
 			 * Even without a valid boot inidicator value
@@ -495,12 +522,15 @@ int msdos_partition(struct parsed_partitions *state)
 			 * without a partition table.
 			 */
 			fb = (struct fat_boot_sector *) data;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			if (slot == 1 && fb->reserved && fb->fats
 				&& fat_valid_media(fb->media)) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				strlcat(state->pp_buf, "\n", PAGE_SIZE);
 				put_dev_sector(sect);
 				return 1;
 			} else {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 				put_dev_sector(sect);
 				return 0;
 			}
@@ -509,9 +539,11 @@ int msdos_partition(struct parsed_partitions *state)
 
 #ifdef CONFIG_EFI_PARTITION
 	p = (struct partition *) (data + 0x1be);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (slot = 1 ; slot <= 4 ; slot++, p++) {
 		/* If this is an EFI GPT disk, msdos should ignore it. */
 		if (SYS_IND(p) == EFI_PMBR_OSTYPE_EFI_GPT) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			put_dev_sector(sect);
 			return 0;
 		}
@@ -528,12 +560,15 @@ int msdos_partition(struct parsed_partitions *state)
 	 */
 
 	state->next = 5;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (slot = 1 ; slot <= 4 ; slot++, p++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		sector_t start = start_sect(p)*sector_size;
 		sector_t size = nr_sects(p)*sector_size;
 
 		if (!size)
 			continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (is_extended_partition(p)) {
 			/*
 			 * prevent someone doing mkfs or mkswap on an
@@ -543,6 +578,7 @@ int msdos_partition(struct parsed_partitions *state)
 			 */
 			sector_t n = 2;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			n = min(size, max(sector_size, n));
 			put_partition(state, slot, start, n);
 
@@ -551,35 +587,51 @@ int msdos_partition(struct parsed_partitions *state)
 			strlcat(state->pp_buf, " >", PAGE_SIZE);
 			continue;
 		}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		put_partition(state, slot, start, size);
 		set_info(state, slot, disksig);
 		if (SYS_IND(p) == LINUX_RAID_PARTITION)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			state->parts[slot].flags = ADDPART_FLAG_RAID;
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (SYS_IND(p) == DM6_PARTITION)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			strlcat(state->pp_buf, "[DM]", PAGE_SIZE);
+}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (SYS_IND(p) == EZD_PARTITION)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 			strlcat(state->pp_buf, "[EZD]", PAGE_SIZE);
+}
 	}
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	strlcat(state->pp_buf, "\n", PAGE_SIZE);
 
 	/* second pass - output for each on a separate line */
 	p = (struct partition *) (0x1be + data);
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	for (slot = 1 ; slot <= 4 ; slot++, p++) {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		unsigned char id = SYS_IND(p);
 		int n;
 
 		if (!nr_sects(p))
 			continue;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		for (n = 0; subtypes[n].parse && id != subtypes[n].id; n++)
 			;
 
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		if (!subtypes[n].parse)
 			continue;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		subtypes[n].parse(state, start_sect(p) * sector_size,
 				  nr_sects(p) * sector_size, slot);
 	}
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	put_dev_sector(sect);
 	return 1;
 }
diff --git a/block/scsi_ioctl.c b/block/scsi_ioctl.c
index 7440de4..22f0b8f 100644
--- a/block/scsi_ioctl.c
+++ b/block/scsi_ioctl.c
@@ -1,3 +1,5 @@
+extern int kernel_init_done;
+int printk(const char *fmt, ...);
 /*
  * Copyright (C) 2001 Jens Axboe <axboe@suse.de>
  *
@@ -54,21 +56,25 @@ EXPORT_SYMBOL(scsi_command_size_tbl);
 static int sg_get_version(int __user *p)
 {
 	static const int sg_version_num = 30527;
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(sg_version_num, p);
 }
 
 static int scsi_get_idlun(struct request_queue *q, int __user *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(0, p);
 }
 
 static int scsi_get_bus(struct request_queue *q, int __user *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(0, p);
 }
 
 static int sg_get_timeout(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return jiffies_to_clock_t(q->sg_timeout);
 }
 
@@ -77,13 +83,16 @@ static int sg_set_timeout(struct request_queue *q, int __user *p)
 	int timeout, err = get_user(timeout, p);
 
 	if (!err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		q->sg_timeout = clock_t_to_jiffies(timeout);
+}
 
 	return err;
 }
 
 static int max_sectors_bytes(struct request_queue *q)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	unsigned int max_sectors = queue_max_sectors(q);
 
 	max_sectors = min_t(unsigned int, max_sectors, INT_MAX >> 9);
@@ -93,6 +102,7 @@ static int max_sectors_bytes(struct request_queue *q)
 
 static int sg_get_reserved_size(struct request_queue *q, int __user *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	int val = min_t(int, q->sg_reserved_size, max_sectors_bytes(q));
 
 	return put_user(val, p);
@@ -103,7 +113,9 @@ static int sg_set_reserved_size(struct request_queue *q, int __user *p)
 	int size, err = get_user(size, p);
 
 	if (err)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return err;
+}
 
 	if (size < 0)
 		return -EINVAL;
@@ -118,6 +130,7 @@ static int sg_set_reserved_size(struct request_queue *q, int __user *p)
  */
 static int sg_emulated_host(struct request_queue *q, int __user *p)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return put_user(1, p);
 }
 
@@ -213,7 +226,9 @@ int blk_verify_command(unsigned char *cmd, fmode_t has_write_perm)
 
 	/* root can do any command. */
 	if (capable(CAP_SYS_RAWIO))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return 0;
+}
 
 	/* Anybody who can open the device can do a read-safe command */
 	if (test_bit(cmd[0], filter->read_ok))
@@ -230,6 +245,7 @@ EXPORT_SYMBOL(blk_verify_command);
 static int blk_fill_sghdr_rq(struct request_queue *q, struct request *rq,
 			     struct sg_io_hdr *hdr, fmode_t mode)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct scsi_request *req = scsi_req(rq);
 
 	if (copy_from_user(req->cmd, hdr->cmdp, hdr->cmd_len))
@@ -256,6 +272,7 @@ static int blk_fill_sghdr_rq(struct request_queue *q, struct request *rq,
 static int blk_complete_sghdr_rq(struct request *rq, struct sg_io_hdr *hdr,
 				 struct bio *bio)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	struct scsi_request *req = scsi_req(rq);
 	int r, ret = 0;
 
@@ -301,7 +318,9 @@ static int sg_io(struct request_queue *q, struct gendisk *bd_disk,
 	struct bio *bio;
 
 	if (hdr->interface_id != 'S')
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	if (hdr->dxfer_len > (queue_max_hw_sectors(q) << 9))
 		return -EIO;
@@ -426,7 +445,9 @@ int sg_scsi_ioctl(struct request_queue *q, struct gendisk *disk, fmode_t mode,
 	char *buffer = NULL;
 
 	if (!sic)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -EINVAL;
+}
 
 	/*
 	 * get in an out lengths, verify they don't exceed a page worth of data
@@ -539,7 +560,9 @@ static int __blk_send_generic(struct request_queue *q, struct gendisk *bd_disk,
 
 	rq = blk_get_request(q, REQ_OP_SCSI_OUT, __GFP_RECLAIM);
 	if (IS_ERR(rq))
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return PTR_ERR(rq);
+}
 	rq->timeout = BLK_DEFAULT_SG_TIMEOUT;
 	scsi_req(rq)->cmd[0] = cmd;
 	scsi_req(rq)->cmd[4] = data;
@@ -554,6 +577,7 @@ static int __blk_send_generic(struct request_queue *q, struct gendisk *bd_disk,
 static inline int blk_send_start_stop(struct request_queue *q,
 				      struct gendisk *bd_disk, int data)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	return __blk_send_generic(q, bd_disk, GPCMD_START_STOP_UNIT, data);
 }
 
@@ -563,7 +587,9 @@ int scsi_cmd_ioctl(struct request_queue *q, struct gendisk *bd_disk, fmode_t mod
 	int err;
 
 	if (!q)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return -ENXIO;
+}
 
 	switch (cmd) {
 		/*
@@ -689,6 +715,7 @@ EXPORT_SYMBOL(scsi_cmd_ioctl);
 
 int scsi_verify_blk_ioctl(struct block_device *bd, unsigned int cmd)
 {
+if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 	if (bd && bd == bd->bd_contains)
 		return 0;
 
@@ -735,7 +762,9 @@ int scsi_cmd_blk_ioctl(struct block_device *bd, fmode_t mode,
 
 	ret = scsi_verify_blk_ioctl(bd, cmd);
 	if (ret < 0)
+{ if (kernel_init_done) printk("We reached unpopular paths: %s:%i\n", __FILE__, __LINE__);
 		return ret;
+}
 
 	return scsi_cmd_ioctl(bd->bd_disk->queue, bd->bd_disk, mode, cmd, arg);
 }
-- 
2.7.4

